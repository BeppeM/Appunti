\documentclass[a4paper,12pt, oneside]{book}

% \usepackage{fullpage}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage[safe,extra]{tipa}
%\usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{fontspec}
\usepackage{enumerate}
\usepackage{braket}
%\usepackage[demo]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage[cache=false]{minted}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}

\usepackage{tikz}\usetikzlibrary{er}\tikzset{multi  attribute /.style={attribute
    ,double  distance =1.5pt}}\tikzset{derived  attribute /.style={attribute
    ,dashed}}\tikzset{total /.style={double  distance =1.5pt}}\tikzset{every
  entity /.style={draw=orange , fill=orange!20}}\tikzset{every  attribute
  /.style={draw=MediumPurple1, fill=MediumPurple1!20}}\tikzset{every
  relationship /.style={draw=Chartreuse2,
    fill=Chartreuse2!20}}\newcommand{\key}[1]{\underline{#1}}
  \usetikzlibrary{arrows.meta}
  \usetikzlibrary{decorations.markings}
  \usetikzlibrary{arrows,shapes,backgrounds,petri}
\tikzset{
  place/.style={
        circle,
        thick,
        draw=black,
        minimum size=6mm,
    },
  transition/.style={
    rectangle,
    thick,
    fill=black,
    minimum width=8mm,
    inner ysep=2pt
  },
  transitionv/.style={
    rectangle,
    thick,
    fill=black,
    minimum height=8mm,
    inner xsep=2pt
    }
  } 
\usetikzlibrary{automata,positioning}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[space]{grffile} % For spaces in paths
\usepackage{etoolbox} % For spaces in paths
\makeatletter % For spaces in paths
\patchcmd\Gread@eps{\@inputcheck#1 }{\@inputcheck"#1"\relax}{}{}
\makeatother

\title{Architetture Dati}
\author{UniShare\\\\Davide Cozzi\\\href{https://t.me/dlcgold}{@dlcgold}}
\date{}

\pgfplotsset{compat=1.13}
\begin{document}
\maketitle

\definecolor{shadecolor}{gray}{0.80}
\setlist{leftmargin = 2cm}
\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}
\tableofcontents
\renewcommand{\chaptermark}[1]{%
  \markboth{\chaptername
    \ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%
\chapter{Introduzione}
\textbf{Questi appunti sono presi a lezione. Per quanto sia stata fatta
  una revisione è altamente probabile (praticamente certo) che possano
  contenere errori, sia di stampa che di vero e proprio contenuto. Per
  eventuali proposte di correzione effettuare una pull request. Link: }
\url{https://github.com/dlcgold/Appunti}.\\
\chapter{Sistemi centralizzati}
\begin{definizione}
  Un \textbf{DBMS (\textit{DataBase Management System})} è un sistema, ovvero un
  software, in grado di gestire collezioni di dati che siano:
  \begin{itemize}
    \item \textit{grandi}, ovvero di dimensioni maggiori della memoria centrale
    dei sistemi di calcolo usati (se ho a che fare con una quantità di dati non
    così grande e con un uso personale posso affidarmi ad una \textit{hashmap}
    piuttosto che ad un db)
    \item \textit{persistenti}, ovvero con un periodo di vita indipendente dalle
    singole esecuzioni dei programmi che le utilizzano e per molto tempo 
    \item \textit{condivise}, ovvero usate da diversi applicativi e diversi
    utenti (fattore che porta anche allo studio del carico di lavoro,
    \textit{workload}). L'accesso può essere sia \textit{in scrittura} che
    \textit{in lettura} (ovviamente anche entrambi) a seconda del caso. Si
    pongono quindi problemi di concorrenza e sicurezza
    \item \textit{affidabili}, sia resistente dal punto di vista hardware (un
    guasto non deve farmi perdere i dati) che dal punto di vista della sicurezza
    informatica. Le transazioni devono essere quindi \textbf{atomiche} (o tutto
    o niente) e \textbf{definitive} (che non verranno più dimenticate). Il
    software può cambiare mentre i dati no
  \end{itemize}
\end{definizione}
A livello di architettura per un \textit{sistema centralizzati} si hanno:
\begin{itemize}
  \item uno o più \textit{storage} per memorizzare i dati, a loro volta su uno o
  più file del \textit{file system}
  \item il \textit{DBMS}, il componente software che funge da componente logico
  \item diverse applicazioni che elaborano i dati provenienti dal db
  (\textit{lettura}) ed eventualmente scrivono dati sullo stesso
  (\textit{scrittura})
  \item il \textbf{DBA (\textit{DataBase Administrator})} che tramite riga di
  comando o GUI si occupa di manutenzione, sicurezza, ottimizzazione etc$\ldots$
  del DBMS 
\end{itemize}
L'\textit{architettura dati} di un DBMS è definita dall'ente \textit{ANSI/SPARC}
e è a tre livelli:
\begin{enumerate}
  \item diversi \textbf{schemi esterni}, porzioni di db messi a disposizione per
  le varie applicazioni
  \item uno \textbf{schema logico (o concettuale)}, che fa riferimento al
  \textit{modello relazionale} dei dati ed è indipendente dalla tecnologia
  usata. Avendo un unico schema logico si ha un'unica semantica (perlomeno a
  livello astratto). Si ha unica base di dati, quindi un unico insieme di record
  interrogati e aggiornati da tutti gli utenti. Non si ha nessuna forma di
  eterogeneità concettuale 
  \item uno \textbf{schema fisico}, che fa riferimento alla tecnologia usata per
  implementare le tabelle per salvare i dati. Si ha un'unica rappresentazione
  fisica dei dati e quindi nessuna distribuzione e nessuna eterogeneità fisica
\end{enumerate}
\textit{Un unico schema fisico è collegato ad un unico schema logico.}\\
Inoltre si hanno:
\begin{itemize}
  \item un \textbf{unico linguaggio di interrogazione} e quindi un'unica
  modalità di accesso ai dati
  \item un unico sistema di gestione per accesso, aggiornamento e gestione per
  la transazioni e le interrogazioni
  \item un'unica modalità di ripristino in caso d'emergenza
  \item un unico amministratore dei dati
  \item \textbf{nessuna autonomia gestionale}
\end{itemize}
Per il discorso della persistenza dei dati si ha necessità di una memoria
secondaria dove il DBMS salva le strutture dati, studiando un modo efficiente di
trasferimento dei dati nel \textit{buffer} in memoria centrale. Il
\textit{buffer} è un'area di memoria (o meglio un componente software) nella
memoria centrale che cerca, tramite una logica di ``vicinanza'', di mettere i
dati della memoria secondaria in quella centrale. Si usa il \textbf{principio di
  località}.\\
A causa degli accessi condivisi al db si hanno problemi di \textbf{concorrenza},
avendo accesso multi-utente alla stessa dei dati condivisa, accesso che
necessità anche di meccanismi di \textbf{autorizzazione}. In merito alla
concorrenza si ha che le transazioni sono corrette se \textbf{seriali} (ordinate
temporalmente) ma questo non è sempre applicabile e quindi si deve stabilire un
\textit{controllo della concorrenza}.\\
Per accedere ai dati di un db si hanno le \textbf{query
  (\textit{interrogazioni})} che fanno parte del modello logico a cui si
interfaccia l'utente. Essendo i dati nelle memorie secondarie bisogna cercare un
modo di rendere gli accessi performanti, in primis tramite opportune strutture
fisiche in quanto e strutture logiche non sarebbero efficienti in memoria
secondaria. Bisogna fare in modo che gli accessi alla memoria secondaria siano
il più limitati possibili e quindi bisogna ottimizzare l'esecuzione delle query.
Ovviamente una scansione lineare delle tabelle sarebbe troppo dispendiosa con
tabelle grosse, ricordando che i file sono ad accesso sequenziale. Inoltre un
ipotetico \textit{join} tra tabelle renderebbe ancora più complesso l'accesso,
soprattutto se \textit{full-join}.\\
Per poter garantire tutto ciò che è stato detto l'architettura del DBMS deve
essere organizzata in termini di \textit{funzionalità cooperanti}:
\begin{itemize}
  \item un \textbf{query compiler} che prende una query in SQL e la traduce con
  un compilatore
  \item un \textbf{gestore di interrogazioni e aggiornamenti} che trasforma le
  query in SQL in algebra relazionale facendo operazioni di ottimizzazione
  \item un \textbf{gestore dei metodi di accesso} per permettere il passaggio
  tra file e tabelle passando dal \textbf{gestore del buffer} e il
  \textbf{gestore della memoria secondaria} dove i dati non sono in forma
  tabellare ma di file e pagine
  \item un \textbf{DDL compiler}, dove DDL sta per Data Description Language,
  che si occupa dei comandi del DBA
  \item un \textbf{gestore della concorrenza}, che garantisce il controllo della
  concorrenza
  \item un \textbf{gestore dell'affidabilità}, che garantisce che un dato non
  vada perso
  \item un \textbf{gestore delle transazioni}
\end{itemize}
Gli ultimi quattro entrano in uso specialmente in fase di scrittura.\\
\textbf{Tutto deve essere veloce!}\\
In un sistema distribuito la parte di query compiler, gestore delle
interrogazioni, gestore delle transazioni e gestore della concorrenza resta
invariato mentre il resto cambia drasticamente (in quanto i dati sono
distribuiti) dovendo gestire diversamente l'accesso ai dati e la sua
sicurezza. Bisogna gestire anche come i vari nodi devono interagire coi dati.
\section{Ottimizzazione delle query}
Ottimizzare le query è tutt'altro che banale.\\
Il primo step è il \textbf{parsing}, che stabilisce se la query è sensata dal
punto di vista sintattico e se i vari nomi di tabelle e attributi sono coerenti
con lo schema. Per questo ultimo aspetta ci si appoggia al \textbf{Data
  Catalog}, un particolare db che contiene informazioni sui vari database, in
primis sui vari nomi delle tabelle e per ciascuna sui nomi di ogni attributo. Si
ha quindi una soluzione per gestire i \textit{metadati}. Il parser effettua
un'analisi lessicale, per la sintattica e la semantica, usando il dizionario e
l a traduzione in algebra relazionale, producendo un \textbf{query tree}. Si
calcola anche un \textbf{query plan logico}, utilizzando regole sintattiche di
buon senso, per capire cosa fare prima (per esempio se fare prima una
\textit{select} o un \textit{join}) per ottenere il risultato corretto nel minor
tempo possibile (prima di fare una \textit{join} magari seleziono prima una
sottotabella con i dati potenzialmente utili, togliendo quelli sicuramente
inutili$\ldots$ magari quel \textit{join} può anche essere evitato). La query
viene quindi rappresentata come un albero dove e foglie corrispondono alle
strutture dati logiche, ovvero le tabelle. I nodi interni sono invece le varie
\textbf{operazioni algebriche (\textit{select, join, proiezione, prodotto
    cartesiano} e \textit{operazioni insiemistiche})}.
\newpage
\begin{esempio}
  Vediamo una query:
  \begin{minted}{sql}
    SELECT Pnumber, Dnum, Lname, Address, Bdate
    FROM Project P, Dept D, Emp E
    Where P.dnum=D.dnumber and E.ssn = D.mgrssn
    and P.location = 'Stafford'
  \end{minted}
  che produce:
  \begin{center}
    \psscalebox{1.0 1.0} % Change this value to rescale the drawing.
    {
      \begin{pspicture}(0,-2.36)(10.7,2.36)
        \rput[bl](4.0,2.1249022){$\pi$ Pnumber, Dnum, Lname, Address, Bdate}
        \rput[bl](5.6,0.9249023){join mgrssn=ssn}
        \rput[bl](2.0,0.124902345){join dnum=dnumber}
        \rput[bl](8.4,0.124902345){emp}
        \rput[bl](0.0,-1.0750977){$\sigma$ plocation='stafford'}
        \rput[bl](6.4,-1.0750977){dept}
        \rput[bl](1.6,-2.2750976){project}
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm
        2.0,arrowlength=1.4,arrowinset=0.0]{<-}(6.8,1.3249023)(6.8,2.1249022)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm
        2.0,arrowlength=1.4,arrowinset=0.0]{<-}(8.8,0.52490234)(8.4,0.9249023)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm
        2.0,arrowlength=1.4,arrowinset=0.0]{<-}(5.2,0.52490234)(6.0,0.9249023)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{<-}(3.6,-0.67509764)(3.6,0.124902345)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{<-}(6.4,-0.67509764)(4.8,0.124902345)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm
        2.0,arrowlength=1.4,arrowinset=0.0]{<-}(2.4,-1.8750976)(2.4,-1.0750977)
      \end{pspicture}
    }
  \end{center}
  ma l'ottimizzatore va oltre (magari invertendo i \textit{where} etc$\ldots$)
  con un query plan più efficiente che permette di cambiare automaticamente le
  query in altre più efficienti.\\
\end{esempio}
Si ha un db chiamato \textbf{Statistics} che contiene statistiche sulla storia
delle query nonché altre informazioni sui dati. L'uso di tale db permette di
ottimizzare le query.\\
Solo dopo questo processo si ha la trasformazione delle tabelle logiche in
strutture fisiche e metodi di accesso alla memoria e la trasformazione delle
operazioni algebriche nelle loro implementazioni sulle strutture fisiche. Per la
trasformazione si usano proprietà algebriche e una stima dei costi delle
operazioni fondamentali per diversi metodi di accesso (in poche parole le regole
della ricerca operativa). L'ottimizzazione ha complessità \textbf{esponenziale}
e quindi si introducono approssimazioni basate su euristiche, usando
un'\textbf{alberatura di costi} usando la tecnica del \textbf{Branch\&Bound}.
\section{Transazioni}
Una \textbf{transazione} è l'insieme di istruzioni di accesso in lettura e
scrittura ai dati, istruzioni eventualmente inserite in un linguaggio di
programmazione. Una transazione gode di proprietà che garantiscono la corretta
esecuzione anche in ambito di concorrenza e sicurezza, tanto che sono
paradigmatiche del modello relazionale. Le transazioni iniziano con un
\textbf{begin-transaction} (a volte finiscono con \textit{end-transaction},
opzionale) e all'interno deve essere eseguito tra:
\begin{itemize}
  \item \textbf{commit work}, per terminare correttamente la lettura e/o
  scrittura 
  \item \textbf{rollback work}, per abortire la transazione
\end{itemize}
Un \textbf{sistema transazionale OLTP (\textit{OnLine Transaction Processing})}
è in grado di definire ed eseguire transazioni per conto di un certo numero di
applicazioni concorrenti anche alto. 
\begin{esempio}
  Vediamo un esempio di transazione (esempio di addebito su un conto corrente e
  accredito su un altro):
  \begin{minted}{sql}
    start transaction;
    update ContoCorrente
      set Saldo = Saldo + 10 where
      NumConto = 12202;
    update ContoCorrente
      set Saldo = Saldo – 10 where
      NumConto = 42177;
    commit work;
  \end{minted}
  oppure, con anche la verifica che ci siano ancora soldi dopo il prelievo (con
  eventuale aborto):
  \begin{minted}{sql}
    start transaction;
    update ContoCorrente
      set Saldo = Saldo + 10 where
      NumConto = 12202;
    update ContoCorrente
      set Saldo = Saldo – 10 where
      NumConto = 42177;
    select Saldo into A
      from ContoCorrente
      where NumConto = 42177;
    if (A >= 0) then commit work
    else rollback work;
  \end{minted}
  Il controllo può essere fatto a posteriori grazie al rollback che permette di
  ``dimenticare'' tutte le operazioni precedenti
\end{esempio}
Le istruzioni commit work e rollback work possono comparire più volte
all'interno del programma ma esattamente una delle due deve essere eseguita. Si
ha un \textbf{approccio binario}.\\
Bisogna approfondire quindi le \textbf{unità di elaborazione} che hanno le
proprietà cosiddette \textit{ACID}:
\begin{itemize}
  \item \textbf{Atomicità}, ovvero una transazione è un'unità atomica di
  elaborazione. Non si può lasciare il db in uno ``stato 
  intermedio''. Un problema prima del commit cancella tutto le operazioni svolte
  (\textit{UNDO}) e un problema dopo il commit non deve avere conseguenze, se
  necessario vanno ripetute le operazioni (\textit{REDO}) 
  \item \textbf{Consistenza}, ovvero la transazione rispetta i vincoli di
  integrità (se lo stato iniziale è corretto lo è anche quello finale). Quindi
  se ci sono violazioni non devono restare alla fine (nel caso
  \textit{rollback})
  \item \textbf{Isolamento}, ovvero la transazione non risente delle altre
  transazioni concorrenti. Una transazione non espone i suoi stati intermedi
  evitando l'\textit{effetto domino} (si evita che il rollback di una
  transazione vada in cascata con le altre). L'esecuzione concorrente di una
  collezione di transazioni deve produrre un risultato che si potrebbe ottenere
  con una esecuzione sequenziale
  \item \textbf{Durabilità} (ovvero persistenza), ovvero gli effetti di una
  transazione andata in commit non vanno persi anche in presenza di guasti (a
  tal fine si sfrutta il \textbf{recovery manager}, che garantisce
  l'affidabilità, del DBMS)
\end{itemize}
\section{Gestore della concorrenza}
Il \textbf{gestore della concorrenza} permette di eseguire in parallelo più
operazioni.\\
Definiamo \textbf{schedule} come una sequenza di esecuzione di un insieme di
transazioni. Uno schedule è \textbf{seriale} se se una transazione  termina
prima che la successiva iniziale, altrimenti è \textbf{non seriale}. Qualora non
sia seriale si potrebbero avere problemi.\\
Si sfrutta quindi la \textbf{proprietà di isolamento} facendo in modo che ogni
transazione esegua come se non ci fosse concorrenza: \textit{un insieme di
  transazioni eseguite concorrentemente produce lo stesso risultato che
  produrrebbe una (qualsiasi) delle possibili esecuzioni sequenziali delle stesse
  transazioni allora si ha la proprietà di isolamento}.\\
Si ha quindi che uno schedule è serializzabile se l'esito della sua esecuzione è
lo stesso che si avrebbe con una qualsiasi sequenza seriale delle transazioni
contenute.\\
Si hanno quindi diversi algoritmi per il controllo della concorrenza secondo
varie tipologie:
\begin{itemize}
  \item controllo basato su \textit{conflict equivalence}
  \item controllo di concorrenza basato su \textit{locks} \textit{(protocollo
    2PL o two phase locking, shared locks e gestione dei deadlock}). Il
  protocollo 2PL è usato nei DBMS dove per costruzione si hanno schedule
  serializzabili usando i lock per  bloccare l'accesso alla risorse da parte di
  una transazione fino a che una risorsa non sia rilasciata. Si hanno quindi i
  concetti di \textit{lock} e \textit{unlock} che garantiscono l'uso esclusivo
  di una risorsa e l'autorizzazione esclusiva dell'uso di una risorsa viene dato
  dal gestore delle transazioni. Si hanno delle \textbf{tabelle di lock}. Si ha
  che, in ogni transazione, tutte le richieste di \textit{lock} precedono tutti
  gli \textit{unlock} (che comunque devono essere fatti dopo l'operazione di
  \textit{commit}) 
  \item controllo di concorrenza basato su \textit{timestamps}
\end{itemize}
\chapter{Sistemi distribuiti relazionali}
Abbiamo visto nei sistemi centralizzati come ci fosse una sola base dati. In un
sistema distribuito abbiamo diversi \textbf{basi dati locali}, diverse
applicazioni su ogni nodo di elaborazione (dove ogni nodo condivide varie
informazioni) con gli utenti che accedono alle varie applicazioni. Questo tipo
di architettura prende il nome di \textbf{architettura shared nothing}, in
quanto i DBMS di ogni singola macchina sono autonome (anche di vendor diversi)
ma che lavorano insieme.\\
Un sistema distribuito permette non solo di avere dati ``distribuiti'' tra vari
nodi ma anche di ``duplicarne'' alcuni per diversi scopi, coi nodi collegati in
rete (addirittura si hanno soluzioni interamente distribuite nel cloud).\\
Confrontando un db distribuito con un multi-database (ovvero vari database
completi da ``unificare'') notiamo come entrambi
abbiano un'alta distribuzione, il primo una bassa eterogeneità (a differenza del
secondo, dove nei vari db potrei avere forte differenza di tipologia dei dati
contenuti). Si ha anche bassa autonomia nel caso si db distribuiti a differenza
del multi-database (dove ogni db è singolarmente autonomo).\\
Bisogna capire cosa distribuire. Si hanno diverse condizioni (che possono essere
presenti simultaneamente):
\begin{itemize}
  \item le applicazioni, fra loro cooperanti, risiedono su più nodi elaborativi
  \textbf{(elaborazione distribuita})
  \item l'archivio informativo è distribuito su più nodi (\textbf{base di dati
  distribuita}) 
\end{itemize}
La distribuzione si dice essere \textbf{ortogonale e trasparente} agli altri.\\
Capire cosa distribuire è una parte consistente dello studio di come costruire
un'architettura distribuita (magari frutto di situazioni particolari come la
``fusione'' di due sistemi a causa di un'acquisizione aziendale etc$\ldots$ dove
diverse logiche applicative e diverse strutture dati possono creare situazioni
molto pericolose).\\
Possiamo classificare i db distribuiti. Si ha innanzitutto che un \textbf{DBMS
Distribuito Eterogeneo Autonomo} è in generale una federazione di DBMS che
collaborano nel fornire servizi di accesso ai dati con livelli di
\textit{trasparenza} definiti (infatti le diversità tra db nei nodi vengono
``nascosti'' a vari \textit{livelli di trasparenza} per distribuzione,
eterogeneità e autonomia). Come abbiamo visto esiste l'esigenza di integrare a
posteriori vari db preesistenti (anche a causa di integrazione di nuovi
applicativi o nuove cooperazioni di processi) e questa situazione è spinta
dallo sviluppo della rete.\\
Possiamo quindi dividere i livello di federazione su tre categorie tra loro
ortogonali (ovvero indipendenti):
\begin{itemize}
  \item autonomia
  \item distribuzione
  \item eterogeneità
\end{itemize}
\subsubsection{Autonomia}
L'\textbf{autonomia} fa riferimento al grado di indipendenza tra i nodi e si
hanno diverse forme:
\begin{itemize}
  \item \textbf{autonomia di progetto}, il livello ``massimo'' dove ogni nodo ha
  un proprio modello dei dati e di gestione delle transazioni
  \item \textbf{autonomia di condivisione}, dove ogni nodo sceglie la porzione
  di dati da condividere ma condividendo con gli altri nodi lo schema comune
  \item \textbf{autonomia di esecuzione}, dove ogni nodo sceglie in che modo
  eseguire le transazioni
\end{itemize}
Si hanno quindi:
\begin{itemize}
  \item \textbf{DBMS Strettamente integrati} con nessuna autonomia, con dati
  logicamente centralizzati, un unico data manager per le transazioni
  applicative e vari data manager locali che non operano in modo autonomo ma
  eseguono le direttive centrali
  \item \textbf{DBMS semi-autonomi}, dove ogni data manager è autonomo ma
  partecipa a transazioni globali, dove una parte dei dati è condivisa e dove
  sono richieste modifiche architetturali  per poter fare parte della
  federazione
  \item \textbf{DBMS Peer to Peer} completamente autonomi, dove ogni DBMS lavora
  in completa autonomia ed è inconsapevole dell'esistenza degli altri  
\end{itemize}
\subsubsection{Distribuzione}
Per la\textbf{ distribuzione} dei dati si hanno 3 livelli classici:
\begin{itemize}
  \item \textbf{distribuzione client/server}, in cui la gestione dei dati è
  concentrata nei server, mentre i client forniscono l'ambiente applicativo e la
  presentazione
  \item \textbf{distribuzione Peer to Peer}, in cui non c'è distinzione tra
  client e server, e tutti i nodi del sistema hanno identiche funzionalità DBMS
  \item \textbf{nessuna distribuzione}
\end{itemize}
Le prime due possono anche non essere distinte.
\subsubsection{Eterogeneità}
L'\textbf{eterogeneità} può invece riguardare vari aspetti:
\begin{itemize}
  \item \textbf{modello dei dati} (relazionale, XML, object oriented (OO), json)
  \item \textbf{linguaggio di query} (diversi dialetti SQL, query by example,
  linguaggi di interrogazione OO o XML)
  \item \textbf{gestione delle transazione} (protocolli diversi per il gestore
  della concorrenza o per il recovery)
  \item \textbf{schema concettuale e logico} (concetti rappresentati in uno
  schema come attributo e in altri come entità)
\end{itemize}
Quindi si hanno vari tipi di DBMS:
\begin{itemize}
  \item \textbf{DBMS distribuito omogeneo (DDBMS)} quando si ha alta
  distribuzione ma non si hanno autonomia ed eterogeneità (gestiti solitamente
  dallo stesso vendor)
  \item \textbf{DBMS eterogeneo logicamente integrato (data warehouse)} quando
  si ha alta eterogeneità ma non si hanno distribuzione e autonomia
  \item \textbf{DBMS distribuiti eterogenei} quando si ha alta eterogeneità e
  distribuzione ma non autonomia 
  \item \textbf{DBMS federati distribuiti} quando si ha alta
  distribuzione, semi autonomia e non eterogeneità
  \item \textbf{DBMS distribuiti federati eterogenei} quando si ha alta
  distribuzione ed eterogeneità e semi autonomia
  \item \textbf{multi db MS}, totalmente autonomi ed eventualmente omogenei o
  eterogenei 
\end{itemize}
\textit{Si hanno molti altri sistemi in base alle 3 categorie}.\\
\section{DDBMS}
Parliamo di \textbf{DBMS distribuito omogeneo (\textit{DDBMS})}.\\
Studiamo uno schema in cui si passa da un sistema centralizzato ad un sistema
distribuito.\\
Si hanno due architetture di riferimento:
\begin{itemize}
  \item l'\textbf{architettura dati}
  \item l'\textbf{architettura funzionale}, ovvero l'insieme di tecnologie a
  supporto dell'architettura dati
\end{itemize}
Non avendo eterogeneità mantengo lo stesso schema di un DBMS centralizzato ma
distribuisco dati bisogna prendere lo schema centralizzato e aggiungere
componenti tra lo schema logico e lo schema fisico. Infatti non si avrà più un
solo schema logico e un unisco schema fisico ma tanti schemi logici e fisici
locali (ad ogni logico corrisponde un fisico). I vari schemi logici inoltre si
interfacciano con uno \textbf{schema logico globale}, i vari schemi logici
locali non sono quindi altro che delle \textit{viste} dello schema logico
globale. Questa organizzazione tra schemi logici locali e schema logico globale
è la cosiddetta \textbf{organizzazione LAV (\textit{Local As View})}. In ogni
caso il progettista interroga lo schema logico globale e saranno varie
tecnologie ad interrogare gli schemi logici locali (si fa una sorta di routing
delle query).\\
Per ciascuna funzione (come query processing, transaction manager etc$\ldots$)
si possono avere vari tipi di gestione:
\begin{itemize}
  \item centralizzata/gerarchica o distribuita
  \item con assegnazione statica o dinamica dei ruoli
\end{itemize}
\textit{Lo schema globale viene progettato prima degli schemi locali.}\\
Ovviamente cambia il \textbf{processo di progettazione} nel caso dei
DDBMS. Normalmente si ha un approccio \textit{top-down} per la progettazione,
con:
\begin{enumerate}
  \item analisi dei requisiti
  \item progettazione concettuale
  \item progettazione logica
  \item progettazione fisica
\end{enumerate}
ma questo tipo di progettazione va cambiato e quindi si introduce una nuova
fase e si cambiano le ultime due:
\begin{enumerate}
  \item analisi dei requisiti
  \item progettazione concettuale
  \item \textbf{progettazione della distribuzione}, per capire dove mettere i
  dati
  \item progettazione logica \textbf{locale}, che traduce dallo schema
  concettuale globale allo schema logico locale solo alcuni concetti
  \item progettazione fisica \textbf{locale}
\end{enumerate}
Si introduce il concetto di \textbf{portabilità}, ovvero la capacità di
eseguire le stesse applicazioni DB su ambienti runtime diversi (anche con SQL
diversi e differenti dallo standard). La portabilità è a \textit{compile-time}\\
Si ha anche il concetto di \textbf{interoperabilità} (tra vendors diversi),
ovvero la capacità di eseguire applicazioni che coinvolgono contemporaneamente
sistemi diversi ed eterogenei (con zero autonomia). A tal fine sono stati
introdotti dei \textit{middleware}, tra cui \textbf{ODBC} che si occupa
dell'accesso a dati di diversi vendor. ODBC, a livello architetturale, si pone
sopra il DBMS e da un'immagine indipendente da ciò che c'è sotto (funziona come
una sorta di \textit{driver}), trasformando tutto in una sorta di SQL
standard. Si hanno anche dei protocolli, come \textbf{X-Open Distributed
  Transaction Processing (\textit{DTP})} (che è una descrizione architetturale
abilitante il protocollo di esecuzione di transazioni distribuite), che
consentono di eseguire delle 
transazioni secondo una logica diversa. Questo protocollo stabilisce una serie
di API che vengono implementate da ogni singolo DBMS per offrire una
connettività standard (approccio molto usato per transazioni con vendor
diversi). Il protocollo funziona sia se si ha che fare con omogeneità che con
eterogeneità. \\
Si hanno altri approcci:
\begin{itemize}
  \item \textbf{basi dati parallele}, con incremento delle prestazione mediante
  parallelismo sia di storage devices che di processore (scalabilità
  orizzontale). Un esempio sono le \textbf{basi dati GRID}
  \item \textbf{basi dati replicate} dove si ha la
  replicazione della stessa informazione su diversi server per motivi di
  performance. Importanti per i temi della consistenza e della sicurezza
  \item \textbf{Data warehouses}, ovvero DBMS centralizzati, risultato
  dell'integrazione di fonti eterogenee, dedicati nel dettaglio alla gestione di
  dati per il supporto alle decisioni. Prevede la \textit{cristallizzazione} dei
  dati, acquisiti da varie sorgenti, creando un nuovo schema con la
  memorizzazione dei dati in formato nuovo (solitamente relazionale). Non usa un
  approccio LAV 
\end{itemize}
\subsection{Caratteristiche dei DDBMS}
Si hanno vari tipi di architetture DDBMS:
\begin{itemize}
  \item \textbf{shared-everything}, ad esempio \textit{SMP server}, dove il db
  management system e il disco sono in un unico nodo
  \item \textbf{shared-disk}, ad esempio \textit{Oracle RAC}, dove diversi db
  management systems 
  agiscono su una stessa \textbf{SAN (\textit{Storage Area Network})}, ovvero
  un'architettura dati di puro storage (con tanti dischi in raid). I vari db
  accedono ai dati secondo una certa regolazione. Viene distribuito il carico
  sui db ma si hanno problemi di concorrenza e hanno grandi problemi di
  scalabilità e costo economico
  \item \textbf{shared-nothing}, sempre più usati, dove ogni db management
  system ha il suo disco. È molto scalabile e, a patto di gestire la
  complessità, posso aggiungere nodi in modo illimitato (\textbf{scalabilità
    orizzontale}). Si presta molto all'ambiente cloud. Sono \textit{architetture
  federate}.
\end{itemize}
Vediamo quindi le proprietà generali di un DDBMS (facendo esplicito riferimento
alle architetture \textit{shared-nothing} per la loro scalabilità):
\begin{itemize}
  \item \textbf{località}, secondo il \textit{principio di località}, che
  garantisce un aumento di performances (nonché di sicurezza) tenendo i dati si
  trovano ``vicino'' 
  alle applicazioni che li utilizzano più frequentemente 
  \item \textbf{modularità}, permettendo di scalare orizzontalmente e
  permettendo modifiche a dati ed applicazioni a basso costo
  \item \textbf{resistenza ai guasti}
  \item \textbf{prestazioni ed efficienza}
\end{itemize}
Concentrandoci sulla \textbf{località} si ha che la partizione dei dati
corrisponde spesso ad una partizione naturale delle applicazioni e degli utenti.
I dati risiedono più vicino a dove vengono più usati ma possono comunque essere
raggiunti anche da lontano (\textit{globalmente}). Si cerca inoltre sempre di
più di spostare i dati verso le applicazioni (paradigma ribaltato nel caso di
\textit{big data}).\\
In merito alla \textbf{modularità} si nota come la distribuzione dinamica dei
dati si adatta meglio alle esigenze delle applicazioni (magari spostando solo
sottotabelle verso alcuni nodi etc$\ldots$, sia in modo trasparente rispetto
all'utente che altrimenti).\\
Parlando di \textbf{resistenza ai guasti} si ha una maggior fragilità a causa
delle unità che aumentano di numero ma si ha \textbf{ridondanza} e quindi
maggiore resistenza ai guasti di dati e applicazioni ridondate (\textit{fail
  soft}).\\
Discorso più interessante è da farsi sulle \textbf{prestazioni}. Ogni nodo in un
sistema shared-nothing gestisce db di dimensioni ridotte. Inoltre ogni nodo può
essere ottimizzato ad hoc ed è più semplice gestire e ottimizzare applicazioni
locali. Si ha inoltre distribuzione del carico totale e parallelismo  tra
transazioni locali che fanno parte di una stessa transazione distribuita (anche
se questo aspetta obbliga soluzioni di coordinamento e appesantisce il carico
sulla rete, che rischia di diventare un ``collo di bottiglia'').\\
I DDBMS hanno ovviamente \textbf{funzionalità} specifiche.\\
Ogni server ha buona capacità di gestire transazioni indipendentemente, anche se
le interazione distribuita tra server rappresentata un carico supplementare. Per
le interrogazioni si ha che le query arrivano dalle applicazioni e i risultati
dai server mentre per le transazioni le richieste transazionali arrivano dalle
applicazioni ma sono richiesti \textbf{dati di controllo} per il
coordinamento.\\’
La gestione della rete deve essere ottimizzata e serve uno studio sulla
distribuzione locale dei dati.\\
Ricapitolando si hanno le seguenti funzionalità specifiche:
\begin{itemize}
  \item \textbf{trasmissione} di query, transizioni, frammenti di db e dati di
  controllo tra i nodi
  \item \textbf{frammentazione, replicazione e trasparenza} (secondo vari
  livelli), fattori legati alla natura distribuita dei dati
  \item un \textbf{query processor} e un \textbf{query plan} per la previsione
  di una strategia globale accanto a strategie per le query locali. Si gestisce
  il passaggio tra schema logico globale e quelli locali. Chi esegue
  la query lo fa senza pensare alla frammentazione dei dati
  \item \textbf{controllo di concorrenza} tramite algoritmi distribuiti,
  fondamentale per gli accessi \textit{in scrittura}
  \item \textbf{strategie di recovery} e \textbf{gestione dei guasti}, sia in
  merito alla rete che all'hardware stesso
\end{itemize}
\subsection{Frammentazione e replicazione}
Si definisce \textbf{frammentazione} come la possibilità di allocare porzioni
(\textit{chunk}) diverse del db su nodi diversi.\\
Si definisce \textbf{replicazione} come la possibilità di allocare stesse
porzioni del db su nodi diversi.\\
Si definisce \textbf{trasparenza} come la possibilità per l'applicazione di
accedere ai dati senza sapere dove sono allocati (serve qualcosa che instradi le
query).
\subsubsection{frammentazione}
Esistono due tipi di frammentazione:
\begin{enumerate}
  \item \textbf{frammentazione orizzontale}, che prevede di prendere una tabella
  e frammentare in base alle righe (le prime $n$ da una parte, le seconde $m$
  dall'altra etc$\ldots$). Si mantiene quindi inalterato lo schema in quanto
  ottengo solamente delle tabelle più piccole in quanto pezzi. Per spezzare uso
  una \textit{select} (per la \textbf{selezione}) che selezioni ogni volta un
  certo ``blocco'' di tabella 
  \item \textbf{frammentazione verticale}, che consente di ridurre la
  dimensionalità della tabelle spezzandola in base alle colonne. In ogni nuova
  tabella però la prima colonna deve essere uguale alla prima della tabella
  originale (ovvero dove si ha la chiave primaria), questo per garantire che si
  possa ricomporre la tabella (e lo schema) originale (con operazioni di
  \textit{join}, o meglio un \textit{natural join}) e garantire la
  trasparenza. Anche in questo caso uso 
  una \textit{select} (per la \textbf{proiezione}) che selezioni ogni volta un
  certo numero di colonne da mettere nella nuova tabella
\end{enumerate}
Bisogna quindi garantire:
\begin{itemize}
  \item \textbf{completezza}, ovvero ogni record della relazione $R$ di partenza
  deve poter essere ritrovato in almeno uno dei frammenti
  \item \textbf{ricostruibilità}, ovvero la relazione $R$ di partenza deve poter
  essere ricostruita senza perdita di informazione a partire dai frammenti
  \item \textbf{disgiunzione}, ovvero ogni record della relazione $R$ deve
  essere rappresentato in uno solo dei frammenti
  \item \textbf{replicazione}, l'opposto della disgiunzione
\end{itemize}
Quindi possiamo definire meglio le proprietà dei due tipi di frammentazione per
la relazione $R$, frammentata in diversi $R_i$:
\begin{enumerate}
  \item \textbf{orizzontale}:
  \begin{itemize}
    \item $schema(R_i)=schema(R),\,\forall i$
    \item ogni $R_i$ contiene un sottoinsieme dei record di $R$
    \item è definita da una proiezione su una condizione $ci$: $\sigma_{ci}(R)$
    \item garantisce la completezza, infatti $R_1\cup R_2\cup\ldots R_n=R$
    \item l'unione garantisce la ricostruibilità
  \end{itemize}
  \item \textbf{verticale}:
  \begin{itemize}
    \item $schema(R)=L=(A_1,\ldots,A_m)$ e $schema(R_i) = L_i =
    (A_{i1},\ldots,A_{ik})$ 
    \item garantisce la completezza, infatti $L_1\cup L_2\cup\ldots L_n=L$, dove
    i vari $L_i$ sono i frammenti verticali ed $L$ è la tabella originale
    \item si garantisce la ricostruibilità in quanto $L_i\cap L_j \supseteq
    chiave\,\,\,primaria(R),\,\forall i\neq j$ (ovvero ogni frammento deve
    contenere la chiave primaria)
  \end{itemize}
\end{enumerate}
\subsubsection{Replicazione}
Approfondiamo ora la \textbf{replicazione}. Si hanno diversi aspetti positivi
per l'accesso \textit{in lettura},
come il miglioramento delle prestazioni in quanto consente la coesistenza di
applicazioni con requisiti operazionali diversi sugli stessi dati e aumenta la
\textit{località dei dati} usati da ogni applicazioni. Nel momento in cui si ha
l'accesso \textit{in scrittura} si hanno però diversi aspetti negativi. Si hanno
diverse complicazioni architetturali, tra cui la gestione della transazioni e
l'updates di copie multiple, che devono essere tutte aggiornate. Inoltre bisogna
studiare dal punto di vista progettuale cosa replicare, quanto replicare (ovvero
capire quante copie mantenere), dove allocare le copie e le politiche per
gestirle.\\
In merito all'allocazione studiamo anche gli \textbf{schemi di
  allocazione}. Ogni frammento può essere allocato su un nodo diverso. Lo schema
globale quindi è solo \textit{virtuale} (in quanto non materializzato in un
solo nodo) e lo \textbf{schema di allocazione} definisce il \textit{mapping} tra
un frammento e un nodo. Si ha quindi una tabella, un \textbf{catalogo}, che ci
da informazioni sul partizionamento, associando ogni frammento al nodo in cui è
allocato.
\subsubsection{Trasparenza}
Con la \textbf{trasparenza} si ha la separazione della semantica di alto livello
dalle modalità di frammentazione e allocazione. Si separa quindi la
\textit{logica applicativa} dalla \textit{logica dei dati} ma per farlo serve
uno strato software che gestisca la traduzione dallo schema unico ai
sottoschemi, comportando un aumento di complessità del sistema e una perdita di
prestazioni (problemi che si riducono con un \textit{mapping} integrato del
DDBMS).\\
Le applicazioni (transazioni, interrogazioni) non devono essere modificate a
seguito di cambiamenti nella definizione e organizzazione dei dati e si hanno
due tipi di trasparenza, che si applicano agli schemi ANSI-SPARC nel modello
distribuito (schema logico globale e schemi logici/fisici locali):
\begin{enumerate}
  \item \textbf{trasparenza logica (o indipendenza logica)}, ovvero in
  dipendenza dell'applicazione da modifiche dello schema logico. Un'applicazione
  che usa un frammento non viene modificata se vengono modificati altri
  frammenti
  \item \textbf{trasparenza fisica (o indipendenza fisica)}, ovvero in
  dipendenza dell'applicazione da modifiche dello schema fisico
\end{enumerate}
Frammentazione e allocazione sono tra lo schema logico globale e ogni schema
logico locale. \\
Si hanno quindi tre livelli di trasparenza:
\begin{itemize}
  \item \textbf{trasparenza di frammentazione}, che permette di ignorare
  l'esistenza dei frammenti ed è lo scenario migliore per la programmazione
  applicativa con un'applicazione scritta in SQL standard. Il sistema si occupa
  di convertire query globali in locali e relazioni in sotto-relazioni. La
  scomposizione delle query per ogni sotto-relazione è detta \textbf{query
    rewriting} 
  \item \textbf{trasparenza di replicazione/allocazione}, dove l'applicazione è
  consapevole dei frammenti ma non dei nodi in cui si trovano. In questo caso la
  query è già spezzata in quanto si sa di avere a che fare con un sistema
  frammentato
  \item \textbf{trasparenza di linguaggio}, dove l'applicazione specifica sia
  i frammenti che i nodi, nodi che possono offrire interfacce che non sono SQL
  standard. Tuttavia l'applicazione sarà scritta in SQL standard a prescindere
  dai linguaggi locali dei nodi. Le query vengono quindi tradotte
  ottimizzatone di query. \textit{Questo è il livello di trasparenza più
    basso}
\end{itemize}
\section{Query distribuite}
Analizzeremo prevalentemente DDBMS distribuiti \textit{shared-nothing} e, in
seguito, \textit{architetture di replica} (con un \textit{replication server}
atto a gestire al replica).\\
Le query sono ovviamente le operazioni più importanti. Possono essere di sola
\textit{lettura} (tramite operazioni come la \textit{select}) o anche di
\textit{scrittura}. Le due tipologie di operazioni vengono gestite in modo
molto differente (la lettura sincrona non è un problema, se non hardware
risolvibile con una distribuzione del carico, a differenza della scrittura
sincrona). Le operazioni devono essere eseguite \textbf{velocemente}.
\subsection{Accesso in lettura}
Studiamo prima le \textbf{query in scrittura}.\\
Esistono, in un sistema relazionale, una serie di attività che convertono la
query in SQL in algebra relazionale e solo dopo si ha la distribuzione. 
L'utente, ignaro dello schema distribuito, interroga lo schema
logico globale e il DDBMS decompone la query secondo una localizzazione
specifica in base ai singoli frammenti (ovvero deve distribuire la query in modo
sensato). Si ha anche un'ottimizzazione globale della query prima della
distribuzione in modo che anche la distribuzione stessa sia ottimizzabile
correttamente, infatti il gestore delle interrogazioni manda ai singoli nodi i
giusti frammenti di query che verranno ottimizzati localmente. Ho quindi nel
complesso 4 fasi che compongono il \textbf{query processor}:
\begin{enumerate}
  \item \textbf{query decomposition}
  \item \textbf{data localization}
  \item \textbf{global query optimization}
  \item \textbf{local optimization}
\end{enumerate}
\subsubsection{Query decomposition}
La \textit{query decomposition} opera sullo schema logico globale non tenendo
conto della distribuzione. In questo caso si hanno tecniche di ottimizzazione
algebrica (usando quindi l'algebra relazionale indipendentemente dalla
distribuzione) analoghe a quelle usati in sistemi centralizzati e si ha come
output un \textbf{query tree} non ottimizzato rispetto ai \textbf{costi di
comunicazione}. Il costo di comunicazione riguarda il costo di uso della
\textbf{rete} e dipende da vari fattori. Il costo di comunicazione è il vero
``collo di bottiglia'' in sistemi distribuiti.
\subsubsection{Data localization}
La \textit{data localization} considera la frammentazione delle tabelle e la
distribuzione, capendo ad esempio dove effettuare le \textit{select}
etc$\ldots$. Si procede quindi all'ottimizzazione delle operazioni rispetto 
alla frammentazione, tramite \textbf{tecniche di riduzione}. Viene quindi
prodotta una query efficiente per la frammentazione ma non
ottimizzata. Supponiamo per esempio di avere una tabella su 3 nodi (distribuita
tramite frammentazione orizzontalmente) e che di base
la query faccia la richiesta a tutti e tre (facendo l'unione dei
risultati). Usando la tecnica di riduzione, qualora, per esempio, effettivamente
sia necessaria solo in un nodo, si avrà che la query sarà distribuita unicamente
nel nodo corretto.
\subsubsection{Global query optimization}
La \textit{global query optimization} si basa sulle statistiche sui frammenti
per effettuare l'ottimizzazione. Viene arricchito il \textbf{query tree}, creato
con gli operatori dell'algebra relazionale, tramite gli \textbf{operatori di
  comunicazione} (ovvero \textit{send} e \textit{receive}), che vengono
effettuati tra nodi. Alcune query, dopo aver tenuto conto dei tempi di
comunicazione, potranno essere eseguite in parallelo (grazie all'indipendenza
data dallo \textit{shared-nothing}). In questo caso le decisione più rilevanti
riguardano le operazioni di \textit{join} (che è uno degli operatori più
complicati) e,in particolare (come vedremo più avanti), l'ordine tra i
\textit{join} n-ari e la scelta tra \textit{join} e \textit{semijoin} (un
operatore particolare per i sistemi distribuiti). L'operatore di \textit{join}
infatti ``ingrandisce'' i dati ``fondendo'' tabelle, che magari sono frammentate
in più tabelle su vari nodi. L'uso di \textit{send} e \textit{receive} permette
la comunicazione dei dati tra i nodi, anche se questo rischia di diventare
troppo esoso in termini di prestazioni. Si hanno quindi degli \textbf{algoritmi
  di calcolo del costo adattivi} e si deve studiare la rete e i suoi
ritardi, che dipendono dalla \textit{topologia} della rete stessa e dal carico
applicativo. Si ha quindi una fase di \textbf{ottimizzazione a runtime}, dove si
riadatta il \textbf{query plan}. Per riadattarlo si fa in primis monitoring
sull'esecuzione della query, si procede adattando il modello di costo
(eventualmente con software automatici) e, eventualmente, riadattando la query
se si calcola uno scarto di costo troppo elevato. È il DBA che stabilisce delle
soglie temporali entro le quali ottenere una risposta. Per questo conta la
trasparenza, in quanto non è l'applicazione che deve interessarsi di questo
aspetto. Si introduce un nuovo \textit{layer} di complessità.\\
In alcuni casi è impossibile ad avere un DDBMS che si occupi di questo tipo di
ottimizzazioni.\\
\textbf{La distribuzione non è predicibile a priori}.
Vediamo un semplice esempio:
\begin{esempio}
   \label{esempio:costi}
  Si supponga di avere il seguente schema:
  \begin{itemize}
    \item Employee (eno, ename, title), di cardinalità 400
    \item AssiGN(eno, projectno, resp, dur), di cardinalità 1000 e dove
    \emph{resp} rappresentata il tipo di responsabilità
  \end{itemize}
  Si ha la seguente query: \textit{trovare i nomi dei dipendenti che sono anche
    manager di progetti:}
  \begin{minted}{sql}
    SELECT ename
    FROM Employee E JOIN AssiGN A on E.eno=A.eno
    WHERE resp=''manager''
  \end{minted}
  Che, in algebra relazionale già abbastanza ottimizzata ipotizzando che ci
  siano pochi manager, sarebbe:
  \[\pi_{ename}(EMP><_{eno}(\sigma_{resp=''manager''}(ASG)))\]
  Supponiamo di avere poi 5 nodi uguali, il quinto per il risultato e i primi 4
  frammentati orizzontalmente secondo questo schema di divisione per nodo:
  \begin{enumerate}
    \item $ASG1=\sigma_{eno}\leq 'E3'(ASG)$
    \item $ASG2=\sigma_{eno}> 'E3'(ASG)$
    \item $EMP1=\sigma_{eno}\leq 'E3'(EMP)$
    \item $EMP2=\sigma_{eno}> 'E3'(EMP)$
  \end{enumerate}
  Vediamo quindi una prima esecuzione:
  \begin{itemize}
    \item chiedo al nodo 1 i manager e sposto i risultati di\\
    $\sigma_{resp=''manager''}(ASG1)$ sul nodo 3 (dove sono 
    descritti in modo più completo) come $(ASG'1)$. Il risultato di
    $EMP1><_{eno}(ASG1)$, calcolato sul nodo 4, lo porto sul nodo 5 $EMP'1$ 
    \item chiedo al nodo 2 i manager e sposto i risultati di\\
    $\sigma_{resp=''manager''}(ASG'2)$ sul nodo 4 (dove sono descritti in modo
    più completo) come $(ASG'2)$. Il risultato di $EMP2><_{eno}(ASG'2)$,
    calcolato sul nodo 4, lo porto sul nodo 5 come $EMP'2$ 
    \item sul nodo 5 il risultato sarà $EMP'1\cup EMP'2$
  \end{itemize}
  Vediamo una seconda soluzione:
  \begin{itemize}
    \item contemporaneamente chiedo al nodo 1 e la nodo 3 di mandare al nodo 5
    tutti i manager. Sempre contemporaneamente a queste due operazioni chiedo al
    nodo 2 e al nodo 4 di mandare al nodo 5 tutte le informazioni. I tempi
    saranno basati sul più lento dei quattro nodi, che determinerà il tempo
    massimo dell'operazione (che sono circa calcolabili a priori tramite la
    tabella delle statistiche)
    \item nel nodo 5 calcolo il risultato:
    \[(EMP1\cup EMP2)><_{eno}\sigma_{resp=''manager''}(ASG1\cup ASG2)\]
  \end{itemize}
  I tempi di trasporto detteranno quale soluzione tra le due è la più
  performante ma, viste le cardinalità esigue di dati, probabilmente vince la
  seconda (dove si ha un solo spostamento globale). Questa seconda strategia
  costringe a pensare a particolari strutture di accesso secondarie dette
  \textbf{indici}, che permettono interrogazioni efficaci ma che \textbf{non
    possono essere ``portati''} in sistemi distribuiti. Quindi nel nodo 5 non ho
  gli \textbf{indici} e quindi devo fare l'intero \textbf{prodotto cartesiano}
  per il \textit{join} (che però in questo caso ha un tempo trascurabile,
  grazie alla bassa cardinalità dei dati, rispetto ai costi di trasferimento,
  generalmente non trascurabili rispetto ai costi delle operazioni interne ad un
  nodo).
\end{esempio}
Riprendendo l'esempio definiamo:
\begin{itemize}
  \item \textbf{costo di messaggio} come il costo fisso di spedizione o
  ricezione di un messaggio (detto \textit{setup})
  \item \textbf{costo di trasmissione} come il costo, fisso rispetto alla
  topologia, di trasmissione dati
  \item \textbf{costo di comunicazione} come la somma tra il costo di messaggio,
  moltiplicato per il numero di messaggi, più il costo di trasmissione,
  moltiplicato per il numero di \textit{bytes} trasmessi
  \item \textbf{costo totale} come la somma dei costi delle operazioni
  (\textit{I/O} e \textit{CPU}) più i costi di comunicazione
  (\textit{comunicazione})
  \item \textbf{response time} come la somma dei costi qualora si tenga conto
  del \textit{parallelismo delle trasmissioni}, quindi come la somma tra il
  costo di messaggio, moltiplicato per il numero di messaggi comunicati in modo
  sequenziale, più il costo di trasmissione, moltiplicato per il numero di
  \textit{bytes} trasmessi in modo sequenziale. In questo conto volendo posso
  usare dei \textbf{pesi} basati sulla cardinalità delle unità da trasferire e
  tenere conto del massimo tempo di risposta che si ottiene
\end{itemize}
Si ha quindi che:
\begin{itemize}
  \item nelle \textbf{grandi reti geografiche} i costi di\textit{ comunicazione}
  sono molto maggiori del costo di \textit{I/O}, circa di 10 volte 
  \item nelle \textbf{reti locali} i costi di \textit{comunicazione} e
  \textit{I/O} sono paragonabili, grazie alle reti \textit{gigabit} in locale
\end{itemize}
Tendenzialmente il costo di comunicazione è ancora il \textbf{fattore critico}
ma sempre meno.\\
Bisogna scegliere cosa \textbf{minimizzare}:
\begin{itemize}
  \item il \textit{response time}, aumentando il parallelismo che però può
  portare ad un aumento del \textit{costo totale}, con un maggior numero di
  trasmissione e un maggior processing locale. Nell'esempio \ref{esempio:costi}
  potrebbe sembrare la seconda soluzione, che effettivamente parallelizza di più
  ma non minimizza i costi di risposta 
  \item il \textit{costo totale}, senza tener conto del parallelismo utilizzando
  meglio le risorse e aumentando il \textit{throughput} ma peggiorando così il
  \textit{response time}. Nell'esempio \ref{esempio:costi} è la prima soluzione
\end{itemize}
\subsubsection{Join e Semijoin}
Il \textbf{join} presenta il problema di portare alla perdita
dell'\textbf{indice}. Bisogna quindi studiare come effettuare l'operazione tra
due tabelle su due nodi diversi. Una prima operazione è data dell'operazione di
\textit{semijoin}.
\begin{definizione}
  Definiamo, in algebra relazionale, l'operazione \textit{semijoin}, tra due
  tabelle $R$ e $S$, sull'attributo $A$, come:
  \[R\,\,\,semijoin_A\,\,\,S\equiv \pi_{R^*}(R\,\,\,join_A \,\,\,A)\]
  dove $R^*$ è l'insieme degli attributi di $R$.\\
  In altre parole scelgo esplicitamente di tenere solo gli attributi di $R$ dopo
  il \textit{semijoin}.\\
  Quindi con $R\,\,\,semijoin_A\,\,\,S$ ho la proiezione sugli attributi di $R$
  operazione di \textit{join} e quindi ho che il \textit{semijoin} non è
  \textbf{commutativo}.\\
  Dalla seconda tabella porto solo la serie di attributi che mi servono
  esplicitamente ($\pi_A(S)$) riducendo il carico di lavoro.\\
 \textbf{ Alla fine il nostro $R'$ con i risultati del \textit{semijoin} sarà
  trasportato nel nodo di $S$}
\end{definizione}
Prese due tabelle allocate su nodi differenti, il \textit{joi}n tra di esse può
quindi essere calcolato tramite operazioni di \textit{semijoin}, valgono infatti
le seguenti equivalenze (che portano a diverse strategie a seconda della stima
dei costi):
\begin{itemize}
  \item
  $R\,\,\,join_{\theta}\,\,\,S \iff (R\,\,\,semijoin_{\theta}\,\,\,S)
  \,\,\,join_\theta\,\,\,S$
  \item $R\,\,\,join_{\theta}\,\,\,S \iff
  R\,\,\,join_\theta\,\,\,(S\,\,\,semijoin_{\theta}\,\,\,R)$ 
  \item $R\,\,\,join_{\theta}\,\,\,S \iff (R\,\,\,semijoin_{\theta}\,\,\,S)
  \,\,\,join_\theta\,\,\,(A\,\,\,semijoin_{\theta}\,\,\,R)$
\end{itemize}
In tutti i casi si riduce lo spostamento dei dati.\\
L'uso del \textit{semijoin} è conveniente sse il costo del suo calcolo e del
trasferimento del risultato è inferiore al costo del trasferimento dell'intera
relazione e del costo dell'intero \textit{join} (e questo dipende dal numero di
attributi coinvolti).\\
\textit{Avere più di un \textit{join} complica la situazione, anche solo per la
  scelta dell'ordine in cui eseguirli.}
\subsubsection{Local optimization}
La \textit{local optimization} si occupa dell'ottimizzazione degli schemi
locali. Ogni nodo riceve una \textit{fragment query} e la ottimizza, con
tecniche analoghe ai sistemi centralizzati, in modo completamente
indipendente. Si hanno comunque operazioni di ottimizzazione locale a priori
sul fatto che il \textit{global query optimization} punti a ridurre i costi di
comunicazione (nel caso di un DDBMS in rete geografica) o ad aumentare il
parallelismo (in caso di DDBMS in rete locale).\\
In ogni caso nella progettazione di sistemi di gestione dati distribuiti bisogna
tener conto di:
\begin{itemize}
  \item tipologie di query distribuite
  \item stime o statistiche sullo storico query distribuite già eseguite (fatto
  periodicamente dal DDBMS)
  \item topologia della rete
  \item carico aspettato e workload previsto
\end{itemize}
\subsection{Accesso in scrittura e controllo di concorrenza}
In questo caso la situazione si complica. Un conto è avere delle \textbf{remote
  requests (read-only)}, che possono essere un numero arbitrario di query SQL in
sola lettura, un altro è avere delle \textbf{remote transactions (read-write)},
ovvero un numero arbitrario di operazioni SQL che prevedono anche
\textit{insert} e \textit{update}. Ragionando in un'ottica in cui si ha un
numero arbitrario di server si parla di \textbf{distribuited requests}, dove
ogni singola operazione SQL si può riferire, grazie ad un \textit{ottimizzatore
distribuito}, a qualunque insieme di server, e di \textbf{distribuited
transactions}, dove ogni operazione è diretta ad un unico server e dove le
transazioni possono modificare più di un db, tutto ciò grazie ad un
\textit{protocollo transazionale di coordinamento distribuito}, detto
\textbf{two-phase commit} (in questo caso si ha spesso a che fare con sistemi
\textit{eterogenei} e \textit{federati}). Deve valore la \textbf{proprietà di
  atomicità} di \textit{ACID}. Sempre riguardo \textit{ACID} si ha che la
\textbf{proprietà di consistenza} non dipende dalla distribuzione, in quanto le
proprietà sono indipendenti dall'allocazione, e si ha che la \textbf{proprietà
  di durabilità} viene garantita localmente. Vanno invece rivisti
architetturalmente la \textbf{proprietà di atomicità}, tramite componenti come
il \textit{reliability control} e il \textit{recovery manager} (in caso di
guasti), e la \textbf{proprietà di isolamento}, tramite il \textit{concurrency
  control} (senza il quale si può incorrere in \textit{update fantasma}, dove un
\textit{update} viene cancellato da uno seguente).\\ 
Rivedendo i \textbf{principi del controllo di concorrenza}.
\begin{definizione}
  Data una transazione $t_i$ si ha che essa viene scomposta in sotto-transazioni
  $t_{ij}$ a seconda del nodo $j$-simo su cui viene eseguita. A sua volta una
  transizione $t_{ij}$ viene nominata $r_{ij}$ per le operazioni di lettura e
  $w_{ij}$ per le operazioni di scrittura. L'uso di una risorsa $x$ viene
  indicata, per esempio, con $r_{ij}(x)$ o $w_{ij}(x)$.
\end{definizione}
Ogni sotto-transazione viene schedulata in modo indipendente di server di
ciascun nodo e quindi la \textbf{schedule globale} (dove \textit{schedules}
indica le sequenze delle transazioni da eseguire) dipende dalle
\textbf{schedules locali} di ogni nodo.\\
Purtroppo \textbf{la serializzabilità locale di ogni schedule non garantisce la
  sua serializzabilità globale}. Si viene a creare un \textbf{grafo globale dei
  conflitti} in quanto i conflitti non sono a livello locale ma a livello
globale (infatti si hanno risorse occupate tra i vari nodi, si arriva, in certi
casi, ad una \textbf{situazione di deadlock}). \\
Si ha quindi che lo \textit{schedule globale} è serializzabile sse \textbf{gli
  ordini di serializzazione sono gli stessi per tutti i nodi coinvolti} (nel
caso in cui il db non sia replicato).\\
Qualora si abbia un db replicato si aggiunge un altro problema qualora le
scritture riguardino due repliche diverse. In tal caso si può violare la
\textbf{mutua consistenza} (che dice che al termine della transazione tutte le
copie devono avere lo stesso valore) dei due db locali, anche con due schedule
localmente seriali. Si introduce quindi un \textbf{protocollo di controllo delle
  repliche}.
Il \textbf{protocollo di controllo delle repliche} viene chiamato \textbf{ROWA
  (\textit{Read Once Write All})}. In base a questo protocollo, dato un item
logico $X$ (con $x_1\ldots x_n$ items fisici), si ha che le transazioni vedono
solamente $X$ ed è il protocollo che si occupa di mappare \textit{read(X)} su
una copia qualunque e \textit{write(X)} su tutte le copie. Questo meccanismo
torna utile nell'uso standard delle repliche, permettendo al client di leggere
dal nodo più vicino ma imponendo, eventualmente, la scrittura su tutte le
copie e bloccando le operazioni fino a quando l'ultima scrittura non è
avvenuta. Si hanno purtroppo problemi di perdita di prestazioni a causa di
questa scrittura ``di massa''. Per lo stesso problema si hanno anche condizioni
di rilascio tramite \textit{protocolli asincroni}.
\subsection{2 phase locking}
Anche l'\textbf{algoritmo 2PL (\textit{2 Phase Locking})} viene esteso al caso
di schemi distribuiti. Si hanno due possibili strategie:
\begin{enumerate}
  \item \textbf{primary site}, \textit{centralized}, in quanto
  basata sui siti
  \item \textbf{primary copy}, in quanto basata sulle copie
\end{enumerate}
2PL prevede che prima di rilasciare un \textit{lock} debba averli richiesti
tutti, e nello \textit{stricted 2PL} solo dopo che sia anche stato effettuato il
\textit{commit}.\\
Nel caso di \textit{centralized 2PL} si ha un \textbf{lock manager (LM)} per
ogni nodo, è un'architettura ``master-slave''. Il ``master'' è appunto il
\textit{lock manager} che gestisce i \textit{lock} per l'intero db
distribuito. Gli ``slave'' sono invece i \textit{data processor}, che seguono
quanto fa il \textit{lock manager coordinatore} (che se non è disponibile per
problemi tecnici del nodo comporta seri problemi in quanto la scelta di un nuovo
\textit{lock manager} tra quelli di ogni nodo è parecchio complicata).\\
Si ha inoltre che il \textbf{transaction manager (TM)} del nodo in cui inizia la
transazione sarà ritenuto il \textit{TM coordinatore} dei transaction
manager. La transazione anche in questo caso sarà eseguita dai \textit{data
  processor} nei vari nodi. Il TM coordinatore formula all'LM coordinatore le
richieste di \textit{lock}, che vengono concesse tramite l'\textit{algoritmo
  2PL}. Una volta concesse il TM coordinatore le comunica ai vari \textit{data
  processor}, assegnando ad essi i vari lock e l'accesso ai dati. AL termine
delle operazioni i \textit{data processor} comunicheranno il termine al TM
coordinatore che a sua volta lo comunicherà all'LM coordinatore, che rilascerà i
lock. Si ha però un effetto ``collo di bottiglia'' sul nodo del LM che deve
gestire moltissime richieste e fino a che non risponde sistema va in
\textit{wait}. Una soluzione a questo problema è individuata nella tecnica della
\textbf{copia primaria}. Prima dell'assegnazione del \textit{lock}, viene
individuata per ogni risorsa una \textit{copia primaria}. Inoltre si ha che i
diversi nodi hanno diversi \textit{lock manager} attivi, ognuno che gestisce una
partizione dei lock complessivi, relativi alle risorse primarie residenti nel
nodo. Inoltre, per ogni risorsa nella transazione, il TM comunica le richieste
di lock al LM responsabile della copia primaria, che assegna i \textit{lock}. Si
evita quindi il ``collo di bottiglia'' ma è necessario determinare a priori il
LM per ogni risorsa. Inoltre si necessità di una \textbf{directory globale} dove
tutti i nodi ``vedono tutto''.
\subsection{Gestione dei deadlock}
Indipendentemente da quanto appena discusso si può creare un'\textbf{attesa
  circolare} tra transazioni di due o più nodi. Bisogna quindi applicare un
algoritmo distribuito. Per costruire l'algoritmo dobbiamo ragionare che siamo in
una ``rete tra pari'' \textit{Peer-to-Peer} (e non ``master-slave'') e quindi
bisogna definire un protocollo su cui costruire l'algoritmo \textit{asincrono e
  distribuito}. L'algoritmo potrebbe partire su uno qualsiasi dei nodi.\\
Si ipotizza innanzitutto che tutte le sotto-transazioni siano attivate in modo
sincrono, tramite \textit{Remote Procedure Call (RPC)} bloccante, ovvero una
transazione chiede di fare un'operazione su un certo nodo facendo una RPC ad un
altro nodo, mettendosi in attesa fino a che non finisce. Si possono generare due
tipi di attesa:
\begin{enumerate}
  \item \textbf{attesa da RPC}, una sotto-transazione su un nodo attende
  un'altra sotto-transazione, della stessa transazione, su un altro nodo
  \item \textbf{attesa da rilascio di risorsa}, una sotto-transazione su un nodo
  attende un'altra sotto-transazione, della stessa transazione, sullo stesso
  nodo a causa del rilascio di una risorsa (che normalmente è la tipica
  situazione che porta ad un deadlock in un sistema centralizzato)
\end{enumerate}
La composizione dei due tipi di attesa può generare un \textbf{deadlock
  globale}. \\
E’ possibile caratterizzare le condizioni di attesa su ciascun nodo tramite
condizioni di precedenza e serve quindi specificare qualche notazione, per
rappresentare il fatto che ogni nodo deve capire quali sono le transazioni sono
in attesa per una chiamata esterna o per l'accesso ad una risorsa interna:
\begin{itemize}
  \item $EXT_i$ per una chiamata all'esecuzione di una transazione sul nodo $i$
  \item $x < y$ per indicare che $x$ sta aspettando il rilascio di una risorsa
  da parte di $y$ (che può essere anche $EXT$)
  \item indichiamo quindi la \textbf{sequenza di attesa generale} al nodo $k$
  come: 
  \[EXT<T_{ik}<T_{jk}<EXT\]
\end{itemize}
\begin{esempio}
  Sul DBMS1 si ha:
  \[EXT2<T_{21}<T_{11}<EXT2\]
  e sul DBMS2:
  \[EXT1<T_{12}<T_{22}<EXT1\]
  ovvero sul nodo 2 c'è la transazione 1 che sta aspettando che finisca. La
  transazione 1 sul nodo 1 sta aspettando che la transazione 2 sul nodo 1
  finisca, che a sua volta sta aspettando che la transazione sul nodo 2
  finisca. Però sul DBMS2 scopriamo che la transazione 1 sul nodo 1 è in
  attesa della transazione 2 sul nodo 2 finisca. Quest'ultima sta aspettando
  che finisca la transazione 1 sul nodo 2 che a sua volta attende la
  transazione sul nodo 1. Si ha quindi un \textbf{deadlock distribuito},
  rappresentato nell'immagine \ref{esempio:1}.
  \begin{figure}
    \centering
    \psscalebox{1.0 1.0} % Change this value to rescale the drawing.
    {
      \begin{pspicture}(0,-3.50458)(9.6,3.50458)
        \definecolor{colour0}{rgb}{0.03529412,0.6392157,0.91764706}
        \definecolor{colour1}{rgb}{0.98039216,0.27450982,0.27450982}
        \psframe[linecolor=black, linewidth=0.002, shadow=true,
        shadowsize=0.10583334,shadowcolor=black,fillstyle=solid,
        fillcolor=colour0, dimen=outer](4.0,2.8954198)(0.0,-3.50458)
        \psframe[linecolor=black, linewidth=0.002, shadow=true,
        shadowsize=0.10583334,shadowcolor=black,fillstyle=solid,
        fillcolor=colour0, dimen=outer](9.6,2.8954198)(5.6,-3.50458)
        \psellipse[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour1, dimen=outer](2.0,1.49542)(0.8,0.6)
        \psellipse[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour1,
        dimen=outer](2.0,-1.3045801)(0.8,0.6)
        \psellipse[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=colour1, dimen=outer](7.6,-1.3045801)(0.8,0.6)
        \psellipse[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=colour1, dimen=outer](7.6,1.49542)(0.8,0.6)
        \rput[bl](1.2,3.29542){$DBMS1$}
        \rput[bl](6.8,3.29542){$DBMS2$}
        \rput[bl](1.8,1.2954199){$t_{11}$}
        \rput[bl](1.8,-1.50458){$t_{21}$}
        \rput[bl](7.4,1.2954199){$t_{12}$}
        \rput[bl](7.4,-1.50458){$t_{22}$}
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(2.0,-0.70458007)(2.0,0.8954199)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(2.8,1.6954199)(6.8,1.6954199)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(7.6,0.8954199)(7.6,-0.70458007)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(6.8,-1.50458)(2.8,-1.50458)
        \rput[bl](4.4,1.2954199){RPC}
        \rput[bl](4.4,-1.9045801){RPC}
        \rput[bl](2.4,0.09541992){lock}
        \rput[bl](8.0,0.09541992){lock}
      \end{pspicture}
    }
    \caption{Grafico dell'esempio di deadlock distribuito}
    \label{esempio:1}
  \end{figure}
\end{esempio}
Per risolvere il problema del \textbf{deadlock distribuito} ogni nodo, ad un
certo punto con un suo ordine temporale, prende la sua sequenza di attesa e la
aggiunge ad alle condizioni di attesa locale degli altri nodi legati da
$EXT$. Dopodiché analizza la situazione, rilevando potenziali \textbf{deadlock
  locali}, e comunica le sequenze di attesa alle altre istanze dell'algoritmo,
ovvero agli altri nodi. Qualora si abbia un \textit{deadlock locale} si crea un
grafo dedicato allo stesso dove sarà possibile notare un ciclo, che rappresenta
il deadlock. Ovviamente è possibile evitare che due nodi scoprano
lo stesso deadlock, rendendo così quindi più efficiente l'algoritmo che invia le
sequenze di attesa solo in alcuni modi:
\begin{enumerate}
  \item \textbf{in avanti}, verso il nodo dove è attiva la sotto-transizione
  attesa (nodo nel quale vede che non ci siano deadlock (???)). Nei sistemi
  \textit{Peer-to-Peer} questi sono meccanismi \textit{coreografati}, decisi a
  priori.\\
  \textit{Esempio alla slide 68 del quarto PDF della costruzione di un grafo
    delle transazioni con presenza di ciclo}
  \item solamente quando l'identificatore del secondo nodo attende il
  rilascio della risorsa identificatore del primo nodo
\end{enumerate}
\subsection{Recovery management}
Approfondiamo quindi come garantire la \textbf{proprietà di atomicità}.\\
Abbiamo visto come uno dei guasti possibili in un sistema distribuito sia quello
legato alla perdita di messaggi sulla rete, nonché al partizionamento delle
stessa (comportando magari l'isolamento dei nodi). Ricapitolando abbiamo diversi
tipi di guasti: 
\begin{itemize}
  \item \textbf{guasti nei nodi}, sia \textit{soft} che hard
  \item \textbf{perdita di messaggi}, che lascia l'esecuzione di un protocollo
  in uno stato di \textit{indecisione}, in quanto ogni messaggio del protocollo
  è seguito da un \textit{ack} e la perdita o del messaggio o dell'\textit{ack}
  stesso genera incertezza (non potendo decidere se il messaggio sia arrivato o
  meno)
  \item \textbf{partizionamento della rete}, dove una transazione distribuita
  può essere attiva contemporaneamente su più sotto-reti temporaneamente
  isolate. In questa situazione i singoli nodi non riescono a capire bene chi
  sia isolato e la cosa può portare i nodi a fare scelte contraddittorie
\end{itemize}
Si è quindi studiato il \textbf{protocollo two phase commit (\textit{2PC})} che
cerca di funzionare in presenza di guasti di rete. Questo tipo di protocollo
consente ad una transizione di giungere ad un eventuale \textit{commit} o
\textit{abort} su ciascuno dei nodi che partecipano alla transazione. In questo
protocollo la decisione di \textit{commit} o \textit{abort} tra due o più
\textbf{resource managers (RM)} (i server) viene certificata da un
\textbf{transaction manager (TM)} (il coordinatore). Lo scambio dei messaggi (e
il salvataggio di un log per ciascuno) tra TM e RMs è ciò su cui si basa il
protocollo 2PC. Si ha quindi sempre un'architettura ``master-slave'' (in modo
metaforico si può rappresentare come il prete che unisce in matrimonio i due
sposi). Si ha quindi il TM che interroga i RMs riguardo allo stato della loro
esecuzione.
\subsubsection{2PC in assenza di guasti}
Si hanno diverse fasi:
\begin{enumerate}
  \item durante la prima fase il TM interroga (con un \textit{prepare} o
  \textit{ready\_to\_commit}) tutti i nodi per capire come 
  ciascun nodo intenda terminare la transazione, autonomamente o
  irrevocabilmente \textit{commit} o \textit{abort} (magari per violazione della
  concorrenza locale o per violazione di qualche vincolo di consistenza
  etc$\ldots$). I nodi risponderanno quindi o con \textit{ready\_to\_commit} o
  con \\
  \textit{not\_ready\_to\_commit}
  \item nella seconda fase il TM prende la decisione globale. Si ha che se anche
  solo un nodo richiede un \textit{abort} allora si avrà \textit{abort} per
  tutti i nodi, altrimenti \textit{commit}, chiudendo la transazione. Il TM si
  occupa anche di comunicare ai RMs la decisione finale per poter procedere con
  le azioni locali 
\end{enumerate}
Le fasi sono schematizzate in figura \ref{fig:2pc}.\\
Come abbiamo detto precedentemente si ha la raccolta di \textit{log} nei quali
compaiono due tipi di record:
\begin{enumerate}
  \item \textbf{record di transazione}, con le informazioni sulle operazioni
  effettuate
  \item \textbf{record di sistema}, con l'evento di \textit{checkpoint} e di
  \textit{dump} (ovvero la copia esatta del db in un certo stato)
\end{enumerate}
Le scritture sui log avvengono prima della decisione delle operazioni, che a
loro volta si suddividono in \textit{prepare} e \textit{global decision}.
\begin{figure}[h]
  \centering
  \includegraphics[scale = 0.5]{img/2pc.png}
  \caption{Diagramma del protocollo 2PC, dove negli ovali abbiamo le\emph{
      decisioni} e nei rettangoli le \emph{azioni sui log}. Entrambi i tipi di
    frecce indicano un \emph{messaggio}. La prima fase è fino alle decisioni
    globali incluse.}
  \label{fig:2pc}
\end{figure}
Ai log del TM vengono aggiunti ulteriori dettagli:
\begin{itemize}
  \item \textbf{prepare record} (in figura \ref{fig:2pc} \textit{begin\_commit})
  che contiene l'identità (nodi e transazioni) di tutti i RMs
  \item \textbf{global commit} o \textbf{global abort} che descrive la decisione
  globale. La decisione del TM diventa esecutiva quando scrive nel proprio log
  \textbf{global commit} o \textbf{global abort} 
  \item \textbf{complete record} (in figura \ref{fig:2pc}
  \textit{begin\_of\_transaction}), che viene scritto alla fine del protocollo
\end{itemize}
Al log dei RMs vengono aggiunti ulteriori dettagli:
\begin{itemize}
  \item \textbf{ready record}, per segnalare la disponibilità irrevocabile a
  partecipare alla fase di commit. Si hanno diverse politiche sul
  \textbf{protocollo 2PL} (\textit{recoverable, 2PL, ACR, strict 2PL}). Inoltre
  questo log contiene l'indicazione del TM e i records (come nel caso
  centralizzato), ovvero \textit{begin, insert, delete, update, commit}
  \item \textbf{not ready record} (in figura \ref{fig:2pc} \textit{abort}) per
  segnalare l'indisponibilità del RM al commit
\end{itemize}
In entrambe le fasi del 2PC possono avvenire guasti e in entrambe tutte le
componenti devono poter decidere in base al loro stato. Viene quindi introdotto
l'uso di \textbf{timers}, stabilendo un \textbf{timeout} entro il quale il TM
aspetta una risposta (in entrambe le fasi, anche se nella prima è più
importante). Se il timeout viene superato si lancia un \textit{abort}. Vediamo
distinte le due fasi
\begin{enumerate}
  \item il TM scrive \textit{prepare} nel suo log e invia \textit{prepare} ai
  RMs, fissando un timeout massimo per le riposte. Gli RMs \textit{recoverable},
  ovvero pronti al \textit{commit}, scrivono \textit{ready} nel loro log e
  inviano \textit{ready} al TM. Gli RMs non \textit{recoverable},
  ovvero non pronti al \textit{commit} a causa di un deadlock, scrivono
  \textit{not-ready} nel loro log e terminano il protocollo, con un \textit{log
    unilaterale}. Il TM, come detto scrive nel suo log  \textbf{global commit} o
  il \textbf{global abort}, il secondo nel caso in cui ci sia anche solo un
  \textit{not-ready} o che scatti il timeout (assumendo che i nodi che non hanno
  risposto siano in \textit{failure}).
  \item il TM trasmette la decisione globale e fissa un secondo
  \textit{timeout}. Gli RMs \textit{ready} agiscono di conseguenza scrivendo
  \textit{commit} o \textit{abort} nei loro log e inviando un \textit{ack} al
  TM. Solo dopo effettuano in locale \textit{commit} o \textit{abort}. Il TM
  raccoglie gli \textit{ack} e, in assenza di qualche risposta, fissa un nuovo
  \textit{timeout} ripetendo la trasmissione per gli RMs problematici. Questo
  fino a che non avrà ricevuto da tutti un \textit{ack} e in quel momento scrive
  \textit{complete} nel suo log
\end{enumerate}
Abbiamo quindi appena visto un \textbf{paradigma centralizzato} ``master-salve''
dove i vari RMs non comunicano tra loro. Il TM è quindi il collo di bottiglia''.
Si hanno altri paradigmi:
\begin{itemize}
  \item \textbf{lineare}, dove gli RMs hanno un ordine prestabilito e comunicano
  sempre secondo un ordine prestabilito. Il TM
  è solo il primo di tale ordine. Questo paradigma è utile per reti senza
  possibilità di \textit{broadcast}
  \item \textbf{distribuito}. In questo caso nella prima fase il TM comunica coi
  vari RMs, che però rispondono a tutti. I vari RMs decidono in base alle
  informazioni ricevute dagli altri RMs (comunicano tra loro in
  \textit{broadcast}) e quindi non è più necessaria la seconda fase di
  \textit{2PC}. Si genera una gran quantità di messaggi tra i vari RMs, avendo
  una situazione ``tra pari'' \textit{Peer-to-Peer}, creando un problema di
  prestazioni, dovendo garantire la comunicazione tra tutti i nodi (anche
  tramite \textit{ack})
\end{itemize}
\subsubsection{2PC in caso di guasto}
In uno stato di guasto, nel caso \textbf{centralizzato}, un RM nello stato
\textit{ready} perde la sua autonomia e attende la decisione del TM. Nel caso di
guasto del TM i vari RMs sono lasciati in \textit{stato di incertezza} e le
risorse allocate alla transazione restano bloccate.
\begin{definizione}
  Definiamo \textbf{finestra di incertezza} come la finestra temporale fra la
  scrittura di ready nel log dei RMs e la scrittura di \emph{commit} o
  \emph{abort}. Questo intervallo è ridotto al minimo da \textbf{2PC}.
\end{definizione}
Durante la finestra di incertezza tutte le risorse acquisite tramite meccanismi
di \textit{lock} restano bloccate e, in caso di guasto durante la
\textit{finestra di incertezza}, TM e RMs usano i \textbf{protocolli di
  recovery}.\\
Si possono avere diversi guasti:
\begin{itemize}
  \item \textbf{guasti di componenti}, ovvero guasti al TM o ai RMs.
  \item \textbf{perdita di messaggi o partizionamento della rete}
\end{itemize}
\paragraph{Guasti di componenti}
In questo caso devono essere usati protocolli con due compiti:
\begin{enumerate}
  \item assicurare la terminazione delle procedure. Sono i \textbf{protocolli di
  terminazione}
\item assicurare il ripristino. Sono i \textbf{protocolli di recovery}
\end{enumerate}
\textbf{In entrambi i casi questi protocolli funzionano sia che il guasto
  interessi un solo componente che più di uno}.\\
Partiamo dal caso in cui cada un RM. Può cadere prima di iniziare il protocollo,
non rispondendo al \textit{prepare} e portando all'\textit{abort}. Può cadere
dopo il \textit{ready\_to\_commit} e se quando si riprende vede nei log lo stato
\textit{ready} si mette in \textit{wait} non sapendo bene come fare (nel caso di
\textit{abort} semplicemente chiuderà il protocollo). Il TM
provvederà a inviare a tale RM \textit{commit} o \textit{abort}. Non si hanno
quindi problemi al protocollo, o si va diretti all'\textit{abort} o si
aspetta. Il TM capisce che un RM è caduto grazie al \textit{timeout}.\\
Più complesso è il caso in cui cada il TM. Se cade prima di ricevere le risposte
al \textit{prepare}, quando si sarà ripreso, guarderà lo stato delle risposte al
\textit{prepare} ricevute, altrimenti si avrà un \textit{abort unilaterale}. \\
Vengono introdotti gli \textbf{algoritmi bizantini} nel caso in cui il TM cada e
il RMs debbano decidere insieme cosa fare. Per decidere serve visibilità.\\
Quindi, ricapitolando per le cadute del TM:
\begin{itemize}
  \item cade quanto l'ultimo record del log è \textit{prepare}, magari bloccando
  alcuni RMs. In tal caso si hanno 2 opzioni di recovery:
  \begin{enumerate}
    \item decidere \textit{global abort}, e procedere con la seconda fase di 2PC
    \item ripetere la prima fase, sperando di giungere a un \textit{global
      commit}, richiedendo nuovamente lo stato dei RM
  \end{enumerate}
  \item cade quanto l'ultimo record del log è \textit{global-commit} o
  \textit{global-abort} il TM deve ripete la seconda fase in quanto alcuni RMs
  potrebbero essere bloccati o comunque ignari della decisione presa prima della
  caduta
  \item l’ultimo record nel log è una \textit{complete}, in tal caso non si
  hanno problemi
\end{itemize}
Ricapitoliamo anche le cadute dell'RM:
\begin{itemize}
  \item l'ultimo record nei log è di \textit{azione}, \textit{abort} o
  \textit{commit}, come nel caso centralizzato e in tal caso di procede con un
  \textbf{warm restart}:
  \begin{itemize}
    \item nel caso di \textit{abort} o \textit{azione} si procede con
    l'\textit{undo} dell'operazione
    \item nel caso di \textit{commit} di effettua nuovamente la transazione
  \end{itemize}
  \item l'ultimo record nei log è \textit{ready} e in tal caso l'RM si blocca
  non conoscendo la decisione del TM e si inseriscono, durante \textit{warm
    restart}, nel \textit{ready set} le transazioni dubbie. Si hanno quindi due
  alternative:
  \begin{enumerate}
    \item \textbf{remote recovery}, ovvero l'RM chiede al TM cosa è accaduto
    \item il TM riesegue la seconda fase del protocollo
  \end{enumerate}
\end{itemize}
\paragraph{Perdita di messaggi e partizionamento della rete}
In questo caso il TM non riesce a distinguere tra perdita di messaggi
\textit{prepare} o \textit{ready} nella prima fase e procede, scattando i
\textit{timeout}, con un \textbf{global abort}. \\
Durante la seconda fase la non distinzione tra perdita di \textit{ack} o di
decisioni dei gli RM porta alla ripetizione della seconda fase dopo il
\textit{timeout}.\\
% Un partizionamento della rete non causa problemi ulteriori, dato che una
% transazione può avere successo solo se il TM e tutti i RM appartengono alla
% stessa partizione.
Se durante lo svolgimento del protocollo 2pc si partiziona la rete avendo due
sottoreti una con TM e RM1 che e la seconda sottorete con RM2, RM3. In questo
caso il TM continua a mandare il messaggio della global decision a RM2 RM3 dopo
lo scadere del timeout. Si concluderà comunque, con alte probabilità, con un
\textit{abort} da parte del TM.
\subsubsection{Ottimizzazioni di 2PC}
2PC può essere ottimizzato per:
\begin{itemize}
  \item ridurre il numero di messaggi tra TM e RMs
  \item ridurre le scritture nei log
\end{itemize}
Si hanno due tipi di ottimizzazione:
\begin{enumerate}
  \item \textbf{ottimizzazione read-only}, quando un RM sa che se la propria
  transazione è \textit{read-only} allora non influenza l'esito finale della
  transazione. Al \textit{prepare} risponde \textit{read-only} e termina il
  protocollo. Il TM ignora i partecipanti \textit{read-only} dalla seconda fase
  e, qualora si sapesse a priori, possono anche essere direttamente esclusi dal
  protocollo
  \item \textbf{ottimizzazione presumed abort} che si basa sulla regola
  ``scordarsi gli \textit{abort} e ricordasi i \textit{commit}''. In questo caso
  il TM abbandona la transazione dopo la decisione di abort senza scrivere
  \textit{global abort} nel log e senza aspettare risposta dai vari RMs. Se il
  TM riceve richiesta di un \textit{remote recovery} il TM decide per il
  \textit{global abort}. Non sarà più necessario quindi scriver e\textit{global
    abort} o \textit{prepare} nei log ma solo \textit{global commit, ready} e
  \textit{commit}
\end{enumerate}
\subsubsection{Protocollo X/Open}
Il protocollo 2PC è stato adottato nel protocollo \textbf{X/Open DTP
  (\textit{Distributed Transaction Processing})}, che è un 
consorzio di vendors che vogliono rendere portabile lo standard dell'ambiente
UNIX. In questo protocollo si ha il \textit{TX interface} per la comunicazione
tra il TM e l'applicazione e si ha l'\textit{XA interface} per la comunicazione
tra TM e RMs. Ogni vendor ha la sua implementazione del modello.
\section{Repliche}
Parliamo ora delle tecnologie per la \textbf{replicazione di dati}.\\
Tra le principali soluzioni architetturale troviamo \textbf{IBM replication
  technologies} e \textbf{Microsoft SQL Server replication technologies}
(\textit{i dettagli delle implementazioni non saranno oggetto d'esame}).\\
\begin{definizione}
  La \textbf{replica} è il processo di cerare e mantenere istanze dello stesso
  db allineate tra loro, consentendo la condivisione di dati ma anche
  comportando cambiamenti architetturali. Si ha che le eventuali modifiche
  devono essere viste da tutti i nodi.
\end{definizione}
\begin{definizione}
  Definiamo \textbf{sincronizzazione} è il processo che m i consente di
  \textit{allineare} le copie, prima o poi.
\end{definizione}
In base all'ultima definizione si capisce che spesso le copie non sono
aggiornate istantaneamente. Si ha quindi la \textbf{replica sincrona} o la
\textbf{replica asincrona} (non avendo allineamento \textit{realtime}). IBM
preferisce un approccio \textit{based and mode based} mentre Microsoft uno
basato su \textit{snapshot, transactional \textnormal{e} merge}.\\
Vediamo le differenze tra le due repliche:
\begin{itemize}
  \item le \textbf{repliche sincrone} cercano di far si che tutte le repliche
  vengano aggiornate contemporaneamente (ad esempio si ha il protocollo
  ROWA). Scrivo in modo sincrono su tutti nodi e solo quando tutte le repliche
  confermano la scrittura avanzo con le transazioni (che fallisce se ho nodi non
  disponibili). Nelle repliche sincrone si necessitano molti scambi di messaggi.
  Di fatto si obbliga due o più \textit{storage} ad aggiornarsi e a fare
  \textit{rollback} in caso di fallimento. Si hanno quindi alte disponibilità,
  un auto \textit{fail-over} (bloccando la transazioni in caso di guasti sui
  nodi) e un \textit{data loss} minimo. Le repliche sincrone vengono soprattutto
  usate nei \textit{disaster recovery}, ovvero in situazioni \textit{mission
    critical} (ad esempio sistemi bancari dove i db di backup, almeno 3, devono
  stare a centinaia di kilometri di distanza, in zone sismiche tra loro
  diverse). Gli svantaggi delle repliche sincrone sono la necessità di una rete
  valida, si hanno problemi di scalabilità, di costi e minor flessibilità
  \item nelle \textbf{repliche asincrone} prima si aggiorna il db
  \textit{target} e poi le repliche (normalmente dopo pochi secondi ma anche
  dopo giorni). Si hanno evidenti vantaggi di costo, scalabilità e flessibilità
  (perché in caso di problema lavoro in primis sul db principale)
  ma a rischio di \textit{data loss} (nell'intervallo di tempo tra la scrittura
  del db principale e delle repliche). Normalmente si usano soluzioni asincrone
  per accessi online e la loro efficienza, per bilanciamento del calcolo
  etc$\ldots$ 
\end{itemize}
\textit{In caso di perdita di dati bisogna analizzare i singoli contratti per
  capire legalmente come rispondere di dati che non verranno recuperati
  probabilmente.}\\
Ci sono vari contesti in cui pensare alla replica dei dati:
\begin{itemize}
  \item condivisione di dati da utenti tra loro scollegati. Un esempio è una
  copia su un portatile con una replica usata da un commerciale. Si possono
  avere conflitti nel momento in cui più utenti con db replicati lavorano
  offline. Si ha il \textit{merge conflict}
  \item \textit{data consolidation}, ovvero quando un'azienda vuole tenere più
  copie dei dati in vari punti e alla fine bisogna riportare i dati a livello
  centrale a cadenza periodica. Può servire per fare data warehousing o anche
  solo semplicemente per monitorare le vendite delle varie filiali o per
  aggiornare il catalogo
  \item \textit{data distribution} che è il caso degli \textit{e-commerce}. È un
  caso tipicamente \textit{mission-critical} e bisogna aumentare l'accesso ai
  dati e si ha una costante sincronizzazione realtime bidirezionale per evitare
  problemi. Un altro caso è la distribuzione tra diversi uffici, dove le
  repliche locali hanno magari dati non presenti nel db globale
  \item prestazioni, accesso efficiente, \textit{load balancing} e accesso
  offline. Se non si hanno necessità di update immediati (tipo un sito vetrina)
  allora la replica garantisce la disponibilità e l'accessibilità a basso
  costo. Con il load balancing scarico gli utenti su diverse macchine replicate,
  in primis per le molte realtà di sola lettura o comunque con pochissime
  scritture (un esempio può anche essere un social network dove anche eventuali
  ritardi di qualche secondo non sono problematici). Per la disponibilità
  bisogna fare un forte testing dell'intera architettura, se cade un server
  bisogna puntare ad una replica e bisogna essere sicuri e spesso i costi sono
  troppo alti (\textit{RIVEDERE QUESTA PARTE})
  \item separazione tra \textit{data entry \textnormal{e} reporting}, se si
  usa lo stesso server per entrambi i compiti (che in pratica sono scrittura
  costante e lettura costante) può essere utile separare in due server. Si
  evitano così i rallentamenti dati dai \textit{lock}. Bisogna studiare i tempi
  di sincronizzazione
  \item coesistenza di applicazioni, questo è un caso particolare. Qualora
  sia necessario cambiare applicazione devo, eventualmente, cambiare anche i
  sistemi. Bisogna quindi travasare i dati vecchi e durante il trasferimento
  bisogna comunque mantenere funzionanti le applicazioni. Quindi bisogna far
  coesistere i due database durante il trasferimento (facendo il travaso di
  notte bloccando le transazioni). I costi sono incredibili e si può avere anche
  coesistenza delle applicazioni e non solo dei dati, con migrazioni parziali
  (magari per area geografica) etc$\ldots$ questo comporta che magari due
  filiali devono collaborare con due applicazioni e due db diversi
\end{itemize}
Ci sono anche casi in cui non si dovrebbe replicare:
\begin{itemize}
  \item quando ci sono frequenti update su più copie, portando le copie a
  possibili conflitti che devono essere scoperti e gestiti ``manualmente'' 
  \item quando la consistenza è \textit{critical} magari in contesti di
  trasferimento di fondi etc$\ldots$. In questo caso solitamente si impone un
  protocollo ROWA (con transazioni \textit{ACID compliant}), riducendo le
  prestazioni per avere l'autorizzazione dei \textit{commit} da parte delle
  repliche 
\end{itemize}
Ma spesso bisogna comunque replicare ``scegliendo il male minore'' (ad esempio
nelle banche) e bisogna quindi analizzare il singolo caso, anche in base al
budget. Inoltre non bastano le tecnologie serve un'ottima organizzazione.\\ 
Concludendo si ha che i benefici della replica sono:
\begin{itemize}
  \item disponibilità
  \item affidabilità
  \item prestazioni
  \item riduzione di carico
  \item lavoro offline
  \item supporto a molti utenti
\end{itemize}
Distinguiamo anche delle classi di tipologie di replica:
\begin{itemize}
  \item \textbf{data distribution}, di tipo \textit{1:many}, con un
  \textit{source} che distribuisce, in modo sincrono o asincrono, le varie copie
  passive ai \textit{target}
  \item \textbf{Peer-to-Peer}, dove i vari nodi sono interconnessi e si
  aggiornano tra di loro. Si usa un approccio ROWA
  \item \textbf{data consolidation}, di tipo \textit{many:1}, dove ho più
  \textit{source} che aggiornano un \textit{target} a livello centrale
  \item \textbf{bi-directional} (per il \textit{conflict detention resolution}),
  dove una copia primaria e uno secondaria 
  possono leggere e scrivere a vicenda tra loro (è quindi una versione
  semplificata del \textit{Peer-to-Peer} con due Peer)
  \item \textbf{multi-Tier staging}, in cui si hanno meccanismi intermedi tra
  \textit{source} e \textit{target} con ``aree di deposito'' dette aree di
  \textit{staging}
\end{itemize}
Per realizzare una replica posso fare in diversi modi:
\begin{itemize}
  \item faccio letteralmente il backup del disco con una persona che stacca il
  disco dal server e lo copia, riattaccando infine copia e disco originale
  \item posso prima fare il backup attaccando un altro disco e poi mettere nella
  nuova macchina il disco copia
  \item posso fare una \textit{replica incrementale}, ovvero faccio un
  \textit{full backup} e sposto solo il file di \textit{log} delle transazioni
  nel nuovo server, rieseguendo quanto fatto (essendo contenuto nel
  \textit{log}). Quindi prima faccio un \textit{full
    backup} e poi un \textit{backup \textnormal{del} log}, questo per ogni
  replica. Un'alternativa è l'\textbf{event publish}.
  Un \textbf{event publish} è una replica senza \textit{apply} e
  leggo i file di log. Analizzando gli eventi tramite particolari meccanismi
  riscrivo quindi sull'architettura target. Da un \textit{publisher} si passa ad
  un \textit{distributor} e infine ai \textit{subscriber}
  % MANCA PARTE DB2
\end{itemize}
\textbf{Sulle slide \textit{repliche} si ha un approfondimento delle
  architetture IBM e Microsoft opzionali per il corso}.
\begin{shaded}
  \noindent
  \textit{Vengono qui aggiunte le cose dette in live.}\\
  Un \textbf{db parallelo} è studiato per le prestazioni. Si ha accesso
  parallelo ai dati, parallelismo \textit{intra-query} (stessa query su
  frammenti diversi), parallelismo \textit{inter-query} (tante query diverse) e
  sono fatti da elementi hardware posti vicini tra loro.\\
  Parliamo di \textbf{persistenza dei dati}. Bisogna garantire la durabilità dei
  dati, bisogna quindi usare un db. Si hanno anche problemi per creare oggetti
  persistenti a partire da un linguaggio OOP, si ha il cosiddetto
  l'\textbf{object-relational paradigm mismatch}. Si separa quindi l'aspetto OOP
  e l'aspetto relazionale per risolvere il problema per poi ``mappare'' l'uno
  nell'altro, tramite i \textbf{data mapper}. Si hanno operazioni \textbf{CRUD}:
  \begin{itemize}
    \item Create
    \item Read
    \item Update
    \item Delete
  \end{itemize}
  Si ha quindi il cosiddetto \textbf{Object–relational mapping}, ovvero questa
  tecnica di mapping. Si hanno vari framework che lo implementano.\\
  Storicamente si è provato a fare db ad oggetti ma con scarsi risultati.\\
  Parliamo ora del \textit{caso Gitlab}. Questo è un esempio paradigmatico di
  cosa succede in caso di mala gestione delle repliche. Il 31 Gennaio 2017 dei
  malintenzionati erano entrati nel db PosgreSQL (con due repliche) e stavano
  scrivendo in modo significativo sul nodo primario DB1 e sul secondario DB2. I
  tantissimi lock si bloccavano a vicenda rendendo non disponile il db. Alle 21
  prendono il db secondario e cercano di riorganizzare tutto ma il db primario
  aveva fatto chiamate al secondario che però non aveva risposto. I tecnici
  iniziano a fare danni extra, svuotando la directory dei dati del secondario ma
  per errore ha ``droppato'' il database primario, cancellando 300GB di dati dal
  primario. C'erano ancora i backup, fatti a \textit{snapshot} ogni 24h. C'era
  un backup manuale di 6 ore prima per altri motivi. Il problema è che non sono
  stati trovati i backup, una volta trovati c'erano problemi a causa di
  incompatibilità tra PosgreSQL 9.2 e 9.6. I dati su Azure non erano
  backuppati per scelta di Azure, che non faceva i backup dei database. Nel
  complesso 5 sistemi di backup in totale non funzionavo (alcuni, tipo S3, per 
  errori di codice erano vuoti). Fortunatamente c'era il backup manuale di 6 ore
  prima e si è risolto il problema ma perdendo 5000 progetti, 700 user e
  migliaia di righe di codice.\\ 
  Bisogna premiare la trasparenza di Gitlab che ha documentato tutto quello che
  è accaduto (senza specificare i nomi dei DBA).
\end{shaded}
\section{Prima esercitazione}
\begin{esercizio}
  Si consideri un db con le seguenti relazioni (e quindi tabelle):
  \begin{itemize}
    \item PRODUCTION (\underline{SerialNumber}, PartType, Model, Quantity,
    Machine) 
    \item PICKUP (\underline{SerialNumber}, \underline{Lot}, \textbf{Client,
      SalesPerson},  Amount) 
    \item CLIENT (\underline{Name}, City, Address)
    \item SALESPERSON (\underline{Name}, City, Address)
  \end{itemize}
  \textit{(con sottolineate le chiavi primarie e in grassetto le chiavi di
    integrità referenziale)}\\
  
  e ci poniamo l'obiettivo di partizionare il db secondo determinate
  specifiche.\\
  Si assume che si abbiano le seguenti specifiche organizzative:
  \begin{itemize}
    \item si hanno 4 centri di produzione (\textit{Dublino, San Jose, Zurigo
      \textnormal{e} Taiwan}, ciascuno responsabile, rispettivamente, di
    \textit{cpu, keyboard, screen \textnormal{e} cable}) e 3 centri di vendita
    (\textit{San Jose, Zurigo  \textnormal{e} Taiwan})
    \item le vendite sono distribuite secondo le località geografiche, i clienti
    a Zurigo sono serviti solo dai venditori di Zurigo etc$\ldots$. Si ha però
    che i venditori di  Zurigo servono anche Dublino
    \item ogni area geografica ha il proprio db (avremo quindi 4 db)
  \end{itemize}
  Vogliamo studiare una \textbf{frammentazione orizzontale} delle 4 tabelle.\\
  Ricordiamo quindi che abbiamo 4 centri di produzione (ciascuno responsabile di
  un prodotto e con un db ciascuno) e 3 punti di vendita.\\
  Partiamo con la frammentazione della relazione \textnormal{PRODUCTION},
  ottenendo 4 tabelle, una per componente prodotto, ottenendo (con $\sigma$
  abbiamo l'operazione di selezione nell'algebra relazionale):
  \begin{itemize}
    \item $PRODUCTION_1=\sigma_{partType=cpu}(PRODUCTION)$
    \item $PRODUCTION_2=\sigma_{partType=keyboard}(PRODUCTION)$
    \item $PRODUCTION_3=\sigma_{partType=screen}(PRODUCTION)$
    \item $PRODUCTION_4=\sigma_{partType=cable}(PRODUCTION)$    
  \end{itemize}
  Passiamo alla relazione \textnormal{PICKUP}. Anche in questo caso si frammenta
  per il prodotto facendo il join con la tabella
  \textnormal{PRODUCTION}(ricordando che $\pi$ è l'operazione di proiezione/join 
  nell'algebra relazionale). Per comodità indichiamo tutti gli attributi di
  \textnormal{PICKUP} con $pick$, avendo quindi:
  \[pick=SerialNumber, Lot, Client, SalesPerson, Amount\]
  indico anche con $SN$ $SerialNumber$
  \begin{itemize}
    \item \scriptsize{$PICKUP_1=\pi_{pick}(\sigma_{partType=cpu}(PICKUP\,\, SN
      =SN(PRODUCTION)))$}   
    \item \scriptsize{$PICKUP_2=\pi_{pick}(\sigma_{partType=keyboard}(PICKUP\,\, SN
      =SN(PRODUCTION)))$}   
    \item \scriptsize{$PICKUP_3=\pi_{pick}(\sigma_{partType=screen}(PICKUP\,\, SN
      =SN(PRODUCTION)))$}  
    \item \scriptsize{$PICKUP_4=\pi_{pick}(\sigma_{partType=cable}(PICKUP\,\, SN
     =SN(PRODUCTION)))$}  
 \end{itemize}
 Prendo quindi una proiezione di tutti gli elementi di \textit{PICKUP} separando
 nei vari \textit{PICKUP} in base al prodotto.\\
 Passo a alle tabelle \textnormal{SALESPERSON}. Abbiamo 3
 punti di vendita, quindi, circa come per \textnormal{PRODUCTION}, frammento in
 base alle città di vendita:
 \begin{itemize}
   \item $SALESPERSON_1=\sigma_{City="San Jose"}(SALESPERSON)$
   \item $SALESPERSON_2=\sigma_{City="Zurigo"}(SALESPERSON)$
   \item $SALESPERSON_3=\sigma_{City="Taiwan"}(SALESPERSON)$
 \end{itemize}
 Manca solamente \textnormal{CLIENT}. Anche in questo caso divido in base alle
 città, ricordando che Zurigo e Dublino sono clienti entrambi di Zurigo:
  \begin{itemize}
   \item $CLIENT_1=\sigma_{City="San Jose"}(CLIENT)$
   \item $CLIENT_2=\sigma_{City="Zurigo"\mbox{ or }City="Dublino"}(CLIENT)$
   \item $CLIENT_3=\sigma_{City="Taiwan"}(CLIENT)$
 \end{itemize}
 Abbiamo finito la frammentazione e quindi dobbiamo solo distribuire tali
 tabelle: 
 \begin{itemize}
   \item le quattro tabelle con indice 1 andranno a San Jose, in
   \texttt{company.sanjose.com}
   \item le quattro tabelle con indice 2 andranno a Zurigo, in
   \texttt{company.zurigo.com} 
   \item le quattro tabelle con indice 3 andranno a Taiwan, in
   \texttt{company.taiwan.com} 
   \item le due tabelle con indice 4 andranno a Dublino (che quindi avrà solo
   parte di \textnormal{PRODUCTION} e parte di \textnormal{PICKUP} in quanto a
   Dublino non si ha un punto vendita), in \texttt{company.dublino.com}
 \end{itemize}
\end{esercizio}
\begin{esercizio}
  Vediamo un esercizio in merito alla trasparenza, che ricordiamo essere a tre
  livelli:
  \begin{enumerate}
    \item di frammentazione
    \item di replicazione/allocazione
    \item di linguaggio
  \end{enumerate}
  Si chiede di fare delle interrogazioni, tenendo conto dei livelli di
  trasparenza, sul db costruito nell'esercizio precedente.\\
  La prima query ci chiede di determinare la quantità dei prodotti che hanno
  valore ``77y6878'' (abbiamo quindi a che fare con la trasparenza di
  frammentazione, infatti interroghiamo come se avessimo a che fare con un solo
  db): 
  \begin{minted}{sql}
    Procedure Query1(:Quan):
     Select Quantity in :Quan
     From PRODUCTION
     Where SerialNumber="77y6878"
    End Procedure
  \end{minted}
  (con \texttt{:Quan} indichiamo il nome della tabella).\\
  Vediamo ora come fare nel caso di trasparenza di allocazione, quindi si sa di
  avere a che fare con un db distribuito. La query quindi si ``sposterà'' alla
  ricerca del giusto frammento:
  \begin{minted}{sql}
    Procedure Query2(:Quan):
     Select Quantity in :Quan
     From PRODUCTION_1
     Where SerialNumber="77y6878"
     
     if :empty then
     Select Quantity in :Quan
     From PRODUCTION_2
     Where SerialNumber="77y6878"
     
     if :empty then
     Select Quantity in :Quan
     From PRODUCTION_3
     Where SerialNumber="77y6878"
     
     if :empty then
     Select Quantity in :Quan
     From PRODUCTION_4
     Where SerialNumber="77y6878"
    End Procedure
  \end{minted}
  Vediamo ora come funziona per la trasparenza di linguaggio. In tal caso
  dobbiamo considerare sia le frammentazioni che i vari indirizzi di
  allocazione, ovvero i \texttt{company.città.com}:
  \begin{minted}{sql}
    Procedure Query3(:Quan):
     Select Quantity in :Quan
     From PRODUCTION_1@company.sanjose.com
     Where SerialNumber="77y6878"
     
     if :empty then
     Select Quantity in :Quan
     From PRODUCTION_2@company.zurigo.com
     Where SerialNumber="77y6878"
     
     if :empty then
     Select Quantity in :Quan
     From PRODUCTION_3@company.taiwan.com
     Where SerialNumber="77y6878"
     
     if :empty then
     Select Quantity in :Quan
     From PRODUCTION_4@company.dublino.com
     Where SerialNumber="77y6878"
    End Procedure
  \end{minted}
  \textit{Nelle slide usa \texttt{union} al posto di \texttt{if :empty then}}.
\end{esercizio}
\begin{esercizio}
  Sempre sul db del primo esercizio effettuiamo la seguente query:
  \textit{determinare le macchine che utilizzano come componente ``keyboard'' e
    sono vendute al cliente ``Brown''}.\\
  Per praticità vediamo solo la trasparenza di frammentazione e quella di
  allocazione.\\
  Partiamo con la trasparenza di frammentazione:
  \begin{minted}{sql}
    Procedure Query1(:Machine)
     Select Machine in :Machine
     From PRODUCTION join PICKUP on
     PRODUCTION.SerialNumber=PICKUP.SerialNumber
     Where PartType = "keyboard" AND Client="Brown"
    End Procedure
  \end{minted}
  Vediamo il caso di trasparenza di allocazione (e sappiamo che ``keyboard'' è
  solo in $PRODUCTION_2$ quindi interroghiamo un solo frammento e senza chiedere
  la specifica del $partType$):
  \begin{minted}{sql}
    Procedure Query2(:Machine)
     Select Machine in :Machine
     From PRODUCTION_2 join PICKUP on
     PRODUCTION_2.SerialNumber=PICKUP_2.SerialNumber
     Where Client="Brown"
    End Procedure
  \end{minted}
  Se avessimo voluto fare anche la trasparenza di linguaggio non sarebbe
  cambiato nulla dato che $PRODUCTION_2$ è solo a Zurigo.
\end{esercizio}
\begin{esercizio}
  Sempre sul db del primo esercizio effettuiamo il cambiamento di indirizzo del
  cliente ``Brown'' che si sposta da ``27 Church St.'', Dublino, a ``43 Park Hoi
  St.'', Taiwan. Abbiamo quindi un cambio di allocazione nel db.\\
  Partiamo con la trasparenza di frammentazione:
  \begin{minted}{sql}
    Procedure Update1
     Update Client
     Set Address  = "43 Park Hoi St.", City="Taiwan"
     Where Name="Brown"
    End Procedure
  \end{minted}
  La cosa si complica nel caso di trasparenza di allocazione, tenendo conto
  delle due città e dei loro db:
  \begin{minted}{sql}
    Procedure Update2
     Delete CLIENT_2
     Where Name="Brown"
     
     Insert into CLIENT_3 (Name, Address, City)
     values ("Brown", "43 Park Hoi St.", "Taiwan")
    End Procedure
  \end{minted}
  Se avessimo voluto fare anche la trasparenza di linguaggio non sarebbe
  cambiato nulla poiché le frammentazioni sono in locazioni diverse
\end{esercizio}
\begin{esercizio}
  Sempre sul db del primo esercizio effettuiamo la seguente query:\\
  \textit{calcolare la somma di tutti gli ordini ricevuti a SanJose, Zurigo e
    Taiwan}.\\
  Partiamo con la trasparenza di frammentazione:
  \begin{minted}{sql}
    Procedure Query1
     Select City, sum(Amount)
     From PICKUP join SALESPERSON on
     SalesPerson = Name
     Groub by city
    End Procedure
  \end{minted}
  Passiamo alla trasparenza di allocazione. Mi serviranno tutti i frammenti di
  \texttt{SALESPERSON}. Inoltre devo considerare i vari \texttt{PICKUP}, tutti e
  quattro per il discorso delle vendite su Dublino:
  \begin{minted}{sql}
    Procedure Query2
     Create view PICKUP as
     PICKUP_1 union PICKUP_2 union PICKUP_3 union PICKUP_4
     
     Select City, sum(Amount)
     From SALESPERSON_1 join PICKUP on
     SalesPerson=Name
     union
     From SALESPERSON_2 join PICKUP on
     SalesPerson=Name
     union
     From SALESPERSON_3 join PICKUP on
     SalesPerson=Name
    End Procedure
  \end{minted}
\end{esercizio}
\begin{esercizio}
   Sempre sul db del primo esercizio cerchiamo di massimizzare il parallelismo
   delle inter-query.\\
   Prendiamo la seguente query:
   \textit{estrarre la somma delle quantità di produzione che sono raggruppate
     secondo i tipi e i modelli delle componenti}.\\
   \begin{minted}{sql}
    Procedure Query1
     Select sum(Quantity), Model, PartType
     from PRODUCTION
     Group by (Model, PartType)
    End Procedure
  \end{minted}
  Vogliamo però massimizzare il parallelismo, divido quindi tra le varie
  frammentazioni:
  \begin{minted}{sql}
    Procedure Query1
     Select sum(Quantity), Model, PartType
     from PRODUCTION_1
     Group by (Model, PartType)

     Select sum(Quantity), Model, PartType
     from PRODUCTION_2
     Group by (Model, PartType)

     Select sum(Quantity), Model, PartType
     from PRODUCTION_3
     Group by (Model, PartType)

     Select sum(Quantity), Model, PartType
     from PRODUCTION_4
     Group by (Model, PartType)
    End Procedure
  \end{minted}
  massimizziamo il parallelismo inter-query se si consideriamo che ogni
  partizione ha un DBMS diverso. Volendo potrei dividere la query ancora a
  seconda del modello, evitando il \texttt{group by} (cosa utile nel caso in cui
  si abbia a che fare con un sistema fortemente multicore)
\end{esercizio}
\begin{esercizio}
  Vediamo un esempio di db replicato che può produrre inconsistenza.\\
  Prendiamo l'esempio del db prodotto nel primo esercizio. Supponiamo che ogni
  frammento di \textnormal{PRODUCTION} sia allocato a tutti i DBMS. Però ogni
  DBMS utilizza un frammento e trasmette i cambiamenti del frammento agli altri
  DBMS, permettendo di avere copie del db. In caso di fallimento di un DBMS il
  db sarebbe comunque accessibile dagli altri sistemi e non so avrebbe problemi
  con le query, avendo che il fallimento è trasparente sia ai client che al db
  stesso. Purtroppo quando si ha un fallimento si può generare un
  partizionamento di rete e questo può comportare delle inconsistenze. Se, per
  esempio, due transazioni tolgono 800 ad una certa quantità la seconda in
  ordine temporale fallirà ma se avvengono in due DBMS non connessi non
  falliranno, producendo inconsistenza.
\end{esercizio}
\begin{esercizio}
  Data una replicazione simmetrica dire quando produce inconsistenza.
  Un esempio è un db senza il \textit{concurrency control} e quindi due
  transazioni come quelle dell'esercizio precedente possono causare
  inconsistenza anche senza fallimento della rete.
\end{esercizio}
\chapter{Blockchains}
\textbf{\textit{Questa lezione è stata tenuta dal prof. Leporati}}.\\
\textbf{Nonostante qualche ambiguità ed errore di scrittura, nel capitolo si
  userà il termine ``nodo'' 
  quando si parla di rete P2P e il termine ``blocco'' quando ci si riferisce ai
  blocchi interni alla blockchain.}
\begin{definizione}
  Una \textbf{blockchain}, in poche parole, è un registro pubblico, condiviso e
  decentralizzato che memorizza la proprietà di beni digitali. 
\end{definizione}
È quindi un registro in cui vengono memorizzate informazioni relative alla
proprietà di qualcosa che può essere rappresentato tramite sequenze di bit
(quindi qualcosa di digitale come i \textit{bitcoin} o altre criptovalute).\\
Vengono memorizzate anche le \textbf{transazioni}, ovvero i cambi di
proprietà.\\
Essendo pubblico tutti possono vedere cosa è stato registrato e in particolare
``chi possiede cosa'' e la storia di una certa proprietà.\\
Questo registro è condiviso in quanto gestito da più persone ed è
decentralizzato in quanto non esiste un nucleo che abbia di poteri da
amministratore rispetto agli altri, tutti sono allo stesso livello.
\\
Questo registro è organizzato in blocchi. Si ha un primo blocco detto
\textbf{genesis block} che da il via a tutto. Si hanno poi altri blocchi, in
nero, collegati a questo e che formano una catena. Un blocco si lega al
successivo tramite particolari funzioni crittografiche, dette \textbf{funzioni
  di hash crittografico}. Nel dettaglio il collegamento tra un blocco e il
successivo è dato dal fatto che il valore di hash del blocco è contenuta
all'interno del blocco successivo, facendo in modo che si estremamente
difficile alterare il contenuto di un blocco, ovvero diverse transazioni, che
sono organizzate secondo una precisa struttura dati che consente di verificare
la validità in modo veloce ed efficiente. Per ogni blocco si calcola quindi
l'hash e lo si salva nel blocco successivo. Per modificare una transazione
quindi dovrei calcolare l'hash e dovrei fare il check con il blocco
successivo. Quindi tutti possono verificare la validità della
blockchain. Modificare i dati in modo tale da ottenere lo stesso hash però è
davvero difficile.\\
\begin{figure}
  \centering
  
  \psscalebox{1.3 1.3} % Change this value to rescale the drawing.
  {
    \begin{pspicture}(0,-1.0)(6.8,1.0)
      \definecolor{colour0}{rgb}{0.08235294,0.61960787,0.22352941}
      \definecolor{colour1}{rgb}{0.9529412,0.25882354,0.25882354}
      \psframe[linecolor=colour0, linewidth=0.02, dimen=outer,
      framearc=0.35](0.4,0.6)(0.0,0.2)
      \psframe[linecolor=colour1, linewidth=0.02, dimen=outer,
      framearc=0.35](2.0,1.0)(1.6,0.6)
      \psframe[linecolor=black, linewidth=0.02, dimen=outer,
      framearc=0.35](2.0,0.2)(1.6,-0.2)
      \psframe[linecolor=black, linewidth=0.02, dimen=outer,
      framearc=0.35](1.2,0.6)(0.8,0.2)
      \psframe[linecolor=black, linewidth=0.02, dimen=outer,
      framearc=0.35](2.8,0.2)(2.4,-0.2)
      \psframe[linecolor=black, linewidth=0.02, dimen=outer,
      framearc=0.35](3.6,0.2)(3.2,-0.2)
      \psframe[linecolor=black, linewidth=0.02, dimen=outer,
      framearc=0.35](4.4,0.2)(4.0,-0.2)
      \psframe[linecolor=black, linewidth=0.02, dimen=outer,
      framearc=0.35](5.2,0.6)(4.8,0.2)
      \psframe[linecolor=black, linewidth=0.02, dimen=outer,
      framearc=0.35](6.0,0.6)(5.6,0.2)
      \psframe[linecolor=black, linewidth=0.02, dimen=outer,
      framearc=0.35](6.8,0.6)(6.4,0.2)
      \psline[linecolor=black, linewidth=0.02](0.4,0.4)(0.8,0.4)
      \psline[linecolor=black, linewidth=0.02](1.1627907,0.3860465)
      (1.6279069,0.8511628)
      \psline[linecolor=black, linewidth=0.02](1.1627907,0.3860465)
      (1.6279069,0.013953488)
      \psline[linecolor=black, linewidth=0.02](2.0,0.013953488)
      (2.372093,0.013953488)
      \psline[linecolor=black, linewidth=0.02](2.7441862,0.013953488)
      (3.2093024,0.013953488)
      \psline[linecolor=black, linewidth=0.02](3.5813954,0.013953488)
      (4.0465117,0.013953488)
      \psline[linecolor=black, linewidth=0.02](4.418605,0.013953488)
      (4.7906976,-0.35813954)
      \psline[linecolor=black, linewidth=0.02](5.162791,-0.35813954)
      (5.627907,-0.35813954)
      \psline[linecolor=black, linewidth=0.02](5.162791,0.3860465)
      (5.627907,0.3860465)
      \psline[linecolor=black, linewidth=0.02](6.0,0.3860465)
      (6.372093,0.3860465)
      \psline[linecolor=black, linewidth=0.02](4.418605,0.013953488)
      (4.7906976,0.3860465)
      \psline[linecolor=black, linewidth=0.02](3.3953488,-0.17209302)
      (3.3953488,-0.6372093)
      \psline[linecolor=black, linewidth=0.02](3.5813954,-0.82325584)
      (4.0465117,-0.82325584)
      \psframe[linecolor=colour1, linewidth=0.02, dimen=outer,
      framearc=0.35](3.6,-0.6)(3.2,-1.0)
      \psframe[linecolor=colour1, linewidth=0.02, dimen=outer,
      framearc=0.35](4.4,-0.6)(4.0,-1.0)
      \psframe[linecolor=colour1, linewidth=0.02, dimen=outer,
      framearc=0.35](5.2,-0.2)(4.8,-0.6)
      \psframe[linecolor=colour1, linewidth=0.02, dimen=outer,
      framearc=0.35](6.0,-0.2)(5.6,-0.6)
    \end{pspicture}
  }
  \caption{Esempio di schema di blockchain. In verde a sinistra troviamo il
    \textbf{genesis block}. In nero i blocchi che formano la catena. In rosso
    abbiamo i nodi \textit{orfani}. Per comodità è stata rappresentata in
    orizzontale con la parte ``alta'' a destra}
  \label{fig:bc}
\end{figure}
Si hanno vari nodi per l'uso della blockchain.
I vari nodi sono nodi di una \textbf{rete Peer-to-Peer (\textit{P2P})}
(come quelle per la condivisione di file come \textit{torrent}, dove quindi
ogni computer fa sia da \textit{client} che da \textit{server}). I vari nodi
osservavano le proposte di transazione che vengono fatte dagli utenti (anche
utenti che solo usano la blockchain), verificano che siano valide (ad esempio
verificando che non avvenga il \textit{double-spending}, ovvero, per esempio,
una doppia concessione dello stesso bene), eseguono un \textbf{protocollo di
  consenso} (in quanto potrebbero esserci nodi ``disonesti'', con per esempio
utenti che vorrebbero appropriarsi di beni altrui come bitcoin, si procede
quindi a maggioranza secondo il \textit{proof-of-work}) e infine
procedono validando la transazione aggiungendo il blocco alla fine della
blockchain (``in cima''). Una copia della blockchain viene memorizzata in ogni
nodo della rete P2P alla fine della transazione, quindi quando i nodi sono
d'accordo sull'aggiunta del blocco allora ciascuno lo aggiunge alla propria
copia della blockchain (altrimenti qualsiasi proposta futura verrebbe
bocciata).\\
In molte blockchain se si stabilisce che due blocchi possono essere attaccati
ad un certo blocco e si inizia a lavorare su entrambi formando quindi due
nuove catene. Alla fine ``vince'' la catena più lunga, fermando la
continuazione dell'altro ramo. I blocchi del ramo ``perdente'' vengono resi
\textit{orfani} lasciando quindi una sola catena attiva. Le transazioni nei
nodi \textit{orfani} vengono dimenticate e bisognerà reinserirle. Questa cosa
è molto inefficiente.\\
Ci sono reti, come quella \textit{bitcoin}, in cui una transazione è valida
sse vengono aggiunti 6 blocchi al di sopra di quello che contiene la
transazione.\\
Le transazioni sono \textbf{pseudo-anonime} (comodo ambito bitcoin dove,
come per i contanti, non si sa per quali mani sono passati quei soldi e per
cosa sono stati usati prima). Ci sono vari meccanismi per rendere anonime le
cose, alcuni migliori (come quelli di \textit{monero} o \textit{zcash}) e
alcuni peggiori (come quelli di \textit{bitcoin} ed \textit{ethereum}). \\
Si ha un ramo della \textit{computer foreniscs} che si occupa di capire chi ha
fatto certe transazioni, esplicitamente di criptovalute, sulla
blockchain. Quindi si prova a raggiungere l'anonimato ma non sempre si riesce
(basta un utente che paghi in \textit{bitcoin} su un sito rivelando la propria
identità).

\section{Bitcoin}
\textbf{Bitcoin} è una criptovaluta proposta da Satoshi Nakamoto
(probabilmente un nome falso in quanto in molti stati creare valuta, anche
digitale, è reato)
nel 2008. Non è nato all'improvviso ma molte idee all'interno di
\textit{bitcoin} erano già presenti i diversi paper di crittografia precedenti
(ad esempio la \textit{proof-of-work} era già presente all'interno della
gestione dello spam delle mail, dove veniva imposto un certo sforzo
computazionale per mandare una mail in modo che l'invio non fosse completamente
``gratuito'', comportando che si possano mandare al massimo circa 2000 mail al
giorno, al più di attrezzarsi con dei super computer). Ci sono stati tanti
precedenti di tentativi di creazione di un sostituto digitale del denaro, con
diverse difficoltà a causa di anonimato e \textit{double-spending}. Inoltre
rappresentare una moneta con una sequenza di bit consente la copia illimitata di
tale sequenza, creando copie perfettamente identiche. Una soluzione per evitare
il \textit{double-spending} è quella di usare una \textit{trust third party
  (TTP)}, ovvero tipicamente una banca che
segna le transazioni monetarie, diventando però un ``collo di bottiglia'',
venendo interpellata in tutte le transazioni. Inoltre la banca è conscia delle
transazioni di denaro, che perdono così l'anonimato. L'idea di Satoshi Nakamoto
e stata quella di unire vari protocolli crittografici usando al posto della
banca un registro condiviso, una blockchain appunto, in cui vengono salvate le
transazioni e dove vengono anche controllate, permettendo di evitare il
\textit{double-spending}. Si usa la blockchain quindi per creare un
\textit{trust}, sostituendo la funzione delle banche. Nel \textbf{genesis block}
di \textit{bitcoin} c'è infatti un messaggio (nel posto del blocco dedicato alla
``causale'') che fa riferimento al potere eccessivo delle banche.\\
Le proprietà memorizzate nella blockchain \textit{Bitcoin} sono chiamate
direttamente, come abbiamo già scritto, \textbf{bitcoins (\textit{BTC})} o
frazioni di essi. Le transazioni sono parecchio complicate, con una struttura
dati complessa, con diverse transazioni in ingresso (infatti una transazione può
contenere transazioni), campi per generare nuovi bitcoin,0diversi \textit{script
  di transazione} in output (scritti in un linguaggio ``particolare''). \\
\textit{Non verrà trattata nel dettaglio la conformazione delle transazioni}.\\
Se un utente $A$ vuole mandare un \textit{bitcoin} ad un utente $B$, usando il
proprio client, che spesso viene chiamato \textbf{wallet}, specifica la quantità
che vuole mandare e l'indirizzo di $B$. Ogni utente ha associato quindi un
indirizzo, in qualità di lunga sequenza di numeri. Per ottenere l'indirizzo
viene usata una \textbf{chiave pubblica}, usando quindi la \textit{crittografia
  a chiave pubblica} in cui si usano algoritmi di cifratura e
de-cifratura. Ciascuno crea con questi algoritmi una copia \textit{chiave
  pubblica/chiave privata}, rende pubblica la \textit{chiave pubblica} e
comunica di usare quella chiave pubblica per risalire all'indirizzo a cui farsi
spedire i \textit{bitcoin}, infatti data la chiave pubblica all'utente che deve
inviare i bitcoin la userà per cifrare il messaggio in modo che, tramite la
chiave privata, solo il legittimo destinatario possa decifrare il messaggio.\\
Dalla chiave pubblica quindi ottiene l'indirizzo al quale $A$ deve mandare i
soldi per farli ricevere a $B$. Quindi l'indirizzo è una sorta di identità per
$B$ che però può generarsi tutte le coppie di chiavi che vuole, combinando poi i
vari bitcoin ricevuti ai vari indirizzi che ha generato tramite la complessa
struttura della transazione. Questa possibilità di generare infinite chiavi però
è solo uno \textit{pseudo-anonimato}. \\
Bisogna però anche dimostrare che $A$ è proprietario dei \textit{bitcoins} che
vuole inviare a $B$. Per farlo $A$ ``firma'' digitalmente la transazione tramite
la sua \textit{chiave segreta}. I nodi della rete P2P, che sono detti
\textbf{miners}, verificano che la firma di $A$ sia valida, verificano che non
ci sia il \textit{double-spending} e infine validano la transazione mettendola
in un nuovo blocco della blockchain. Se $B$, che ha ricevuto i
\textit{bitcoins}, vuole a sua volta mandarli a $C$ avvia una transazione
esattamente come descritto sopra, firmando con a chiave privata che era
accoppiata alla chiave pubblica con la quale $A$ gli aveva mandato i
\textit{bitcoins}, permettendo che la firma sia verificata e validata. Quindi
sulla blockchain il possedimento di un \textit{bitcoin} è rappresentato dal
fatto che si può inviare una transazione per cui quel \textit{bitcoin} può
essere dato a qualcun altro (ovviamente si è usato \textit{bitcoin} ma si poteva
parlare anche di più \textit{bitcoins} o, vedremo in seguito, frazioni, che
molto piccole, di 
esso). Non è segnato da nessuna parte quanti \textit{bitcoins} possiede un certo
utente ma solo le catene di transazioni che sono state fatte da ciascun
\textit{bitcoin}, vedendo quindi chi è l'ultimo proprietario. È quindi
essenziale memorizzare le chiavi in quanto possedere equivale a conoscere una
chiave segreta.\\ 
La blockchain registra ogni singola transazione (e sono circa un migliaio ogni
10 minuti, con un blocco che riesce a contenere circa un migliaio di
transazioni e i blocchi vengono aggiunti uno ogni 10 minuti circa) e
attualmente, in data 19 Ottobre 2020, si è arrivati a 290GB di blockchain.
\subsection{Miners}
Abbiamo parlato prima dei membri della rete P2P della blockchain
\textit{Bitcoin}, detti appunto \textbf{miners}.\\
I \textit{miners} lavorano su un \textit{pool} di transazioni proposte dai vari
\textit{wallet}. I miners scelgono circa un migliaio di queste transazioni alla
volta e cercano di formare il nuovo blocco, validando le transazioni. Effettuano
quindi i calcoli computazionali del \textit{proof-of-work} per cui dimostrano di
aver fatto un certo sforzo per avere il \textbf{diritto} di essere quelli che
aggiungono il prossimo blocco alla catena. Avendo un'aggiunta ogni 10 minuti si
ha una fortissima concorrenza tra i \textit{miners}. Inoltre il carico di lavoro
richiesto diventa sempre più difficile in quanto, grazie alla \textbf{legge
  di Moore}, diventa sempre più facile risolvere il ``puzzle'' crittografico,
per il \textit{proof-of-work,} che
serve a risolvere il nuovo blocco e quindi il ``puzzle'' viene reso sempre più
difficile (ovvero se un blocco arriva in meno di 10 minuti il blocco successivo
sarà più difficile da produrre, in modo che nuovamente servano almeno 10 minuti,
anche se equivalentemente verrà reso più facile se ci vogliono troppi minuti in
più di 10, avendo così \textbf{autoregolazione}). Bisogna quindi parlare di
questo ``problema'' crittografico da risolvere e per farlo bisogna un attimo
specificare meglio le \textit{funzioni di hash}.
\begin{definizione}
  Le \textbf{funzioni di hash} sono funzioni crittografiche che prendono in
  input una sequenza di bit, in teoria arbitrariamente lunga, anche se ogni
  funzione ha un limite teorico (ma praticamente irraggiungibile), e produce una
  sequenza che vorrebbe essere univoca (ma che non lo è) di poche centinaia di
  bit. Per esempio \textit{SHA1} produce in output una sequenza di 160 bit,
  \textit{MD5} di 128 bit, \textit{SHA256} di 256 bit (una di
  quelle usate in \textit{Bitcoin}), \textit{RIPEMD} di 160 bit (anch'essa usata
  in \textit{Bitcoin}) etc$\ldots$\\
  Le funzioni di hash devono essere sufficientemente facili da calcolare a
  partire da un certo input
\end{definizione}
L'idea è quindi è che prendo un file e, usando ad esempio SHA256, mi esce una
sequenza di 256 bit, univoca per quel file. Purtroppo il dominio della funzione
(ovvero i bit del file)
è molto più grande del codominio (ovvero tutte le possibili sequenze di 256 bit)
e quindi non si può avere davvero una \textbf{funzione iniettiva} e quindi si
avranno sempre due sequenze in input che producono lo stesso output e quando
questo avviene si ha una \textbf{collisione}. Le collisioni sono inevitabili ma
le funzioni di hash sono fatte in modo tale che sia estremamente difficile
trovare due input diversi che producano lo stesso output, motivo per cui si può
anche pensare che le hash siano univoche. Si ha anche che è estremamente
difficile cercare esplicitamente un input che abbai lo stesso hash di un altro,
rendendo quindi molto difficile sostituire un input dato con un altro ottenendo
comunque lo stesso output, imbrogliando. Quest'ultimo problema è più difficile
di quello della \textit{collisione} (dove ho, nella pratica, un grado di liberà
in più avendo due input).\\
Si hanno altre due proprietà:
\begin{enumerate}
  \item dato un hash è estremamente difficile trovare un input, per questo si
  dice che la funzione di hash è \textit{one-way} (essendo molto facile da
  calcolare ma difficilissimo da invertire)
  \item anche se cambio un solo bit dell'input ottengo un output completamente
  diverso a quello ottenuto prima del cambiamento, rendendo impossibile capire
  la regola di calcolo o anche solo fare indagini statistiche. Infatti hash
  significa anche ``polpettone'', cosa che rappresenta bene l'azione di
  spezzettamento, calcolo e rimescolamento (con anche valori semi casuali)
  dell'input fatte dalle funzioni per calcolare l'output  
\end{enumerate}
Quindi nel \textit{proof-of-work}, per dimostrare di aver svolto una certa
quantità di lavoro viene preso il dato di cui devo calcolare l'hash, gli viene
aggiunta una quantità casuale detta \textbf{nonce (si pronuncia ''nons'')} e
calcolo l'hash del dato concatenato al \textit{nonce} e vado a vedere se il
risultato ha un certo numero prefissato di bit più significativi uguali a 0,
riduco quindi il codominio, ovvero il numero di possibili output validi, dicendo
che devono cominciare con un certo numero di zeri. Aumentando il numero di zeri
riduco la probabilità di ottenere un risultato valido scegliendo un
\textit{nonce} a caso (e diminuendo ottengo l'opposto). Variando gli zeri
ottengo quanto detto sopra in merito al variare del carico computazionale per
restare sui 10 minuti.\\
Quindi, ricapitolando:
\begin{itemize}
  \item il miner sceglie un migliaio di transazioni più o meno a caso
  \item il miner spara a caso un valore del \textit{nonce} 
  \item calcola l'hash di tutto il blocco:
  \begin{itemize}
    \item  se ottiene un valore con un numero di bit più significativi uguali a
    zero accettabile, prima degli altri, manda il blocco nella rete P2P per far
    validare il nuovo blocco e, se il controllo viene superato, il blocco viene
    accettato e messo in cima alla blockchain (quindi ogni nodo della P2P lo
    attacca in cima alla propria copia)
    \item se non ottiene tale valore spara a caso un altro \textit{nonce} e ci
    riprova
  \end{itemize}
\end{itemize}
Se mentre si sta lavorando un nuovo blocco viene validato (anche dal nodo che
stava lavorando al nuovo blocco) un altro blocco bisogna ``buttare'' quanto
costruito anche se non tutto, butto infatti le transazioni che già compaiono nel
nuovo blocco convalidato. Inoltre cambio l'hash del nuovo blocco a cui sto
lavorando mettendo quello di quello appena convalidato (per il discorso spiegato
all'inizio del capitolo).\\
L'importanza di essere colui che crea il blocco è data dal fatto che l'unico
momento in cui si possono creare \textit{bitcoins} è durante la creazione del
nuovo blocco, chi aggiunge il blocco ha dei nuovi \textit{bitcoins} che vengono
creati insieme al blocco.\\
Il mining può essere fatto da macchine sempre più performanti (all'inizio
bastava un portatile, poi si è passati ad usare i pc di interi uffici di notte
(o qualche furbo anche, illegalmente, dell'università), poi si è passati a
cluster ``home-made'' tramite hardware spesso dedicato solo al mining, mentre
ora servono cluster estremamente potenti ma che comunque facciano ritornare le
spese). Si hanno anche soluzioni di sub-affitto di hardware o di gente che
condivide l'hardware per poi dividere i guadagni. A causa di queste
\textit{farm} professionali enormi, spesso collocate in paesi freddi per il
risparmio del raffreddamento e dove la corrente costa meno (ovvero in zone
disabitate di paesi spesso poveri e poco democratici), rischia di danneggiare
l'idea di decentralizzazione della blockchain. \textbf{Bitcoin} ad un certo
punto era in mano di pochissime persone (cinesi) con farm sparse nei monti
asiatici. Si rischiano anche manipolazioni truffaldine del mercato, ad esempio
proposte di \textit{trading coi bitcoins} (cose comunque vietate nei mercati
regolamentati ma non su quello delle criptovalute, non essendo regolamentato, e
quindi se il broker di imbroglia sei fregato).\\
Un altro problema dell'\textit{accentramento} è il cosiddetto \textbf{attacco
  del 51\%}. Dato che ogni miner deve vincere contro tutti gli altri per poter
essere quello che ha diritto di mettere il nuovo blocco allora se uno riesce a
possedere il 51\% dell potenza di calcolo dei miners, controllando il 51\% dei
nodi della rete P2P, praticamente vince sempre, validando sempre le transazioni,
anche se non valide (ovviamente almeno il 51\%).\\
``Tutti contro tutti'' è anche molto inefficiente dal punto di vista energetico
(la rete P2P che gestisce \textit{bitcoin} consuma più di tutta l'Argentina).
Per questo si cercano sempre nuovi algoritmi/alternative per il
\textit{proof-of-work}, come ad esempio \textit{proof-of-stake}, dove stake sta
per ``quantità di soldi'', ovvero l'idea in cui solo pochi nodi della P2P fanno
il mining. Tali nodi vengono scelti in base alla quantità di soldi che ogni nodo
ha deciso di rendere disponibili agli altri (su un conto speciale). Quindi chi
mette più soldi ha più possibilità di essere scelto.\\
Al massimo si ha che si potranno costruire 21 milioni di \textit{bitcoins} e
questi sono sempre più difficili da creare (è quindi paragonabile all'oro da un
certo punto di vista).\\
Torniamo a parlare delle transazioni.\\
Nel caso in cui $A$ voglia dare a $B$ una certa porzione di \textit{bitcoins}
deve creare due transazioni:
\begin{itemize}
  \item una in cui dal totale produce la frazione complementare a quella che
  deve dare a $B$, questa transazione è verso se stessi
  stessi
  \item una in cui da a $B$ la frazione voluta
\end{itemize}
In poche parole se $A$ ha 10 \textit{bitcoins} deve fare una transazione a se
stessa di 9 e una di 1 a $B$.\\
Questo può essere fatto anche avendo più indirizzi di partenza in cui si hanno i
\textit{bitcoins}, grazie al fatto che una transazione ha più input, e verso più
destinatari, contemporaneamente, in quanto la transazione ammette anche più
output nella sua struttura. \textbf{La somma totale degli input non deve essere
  minore a quella in output} (infatti la transazione non verrà validata). Può
comunque essere maggiore in quanto la differenza, detta \textit{fee}, può essere
un compenso per il miner per far scegliere quella transazione da validare (che
quindi non sceglie proprio a caso il migliaio di transazioni in quanto ordina le
transazioni in base alle \textit{fee}). Queste \textit{fee} saranno l'unica
fonte di guadagno per i miner quando non si potrà più produrre \textit{bitcoins}
per il limite visto precedentemente (in quel momento le \textit{fee}
aumenteranno di valore probabilmente).\\
Due blocchi possono essere aggiunti contemporaneamente perché magari due miner
in luoghi distanti propagano insieme l'avviso di aver trovato un nuovo blocco
valido. Può succedere quindi che vengano aggiunti entrambi anche se uno è
destinato a diventare \textit{orfano}.\\
Proporre una transazione con \textit{fee} pari a 0 può portare al fatto che
nessun miner la prenda in carico, comportando la creazione di
\textit{transazioni zombie} (che sono parecchie).\\
Un altro problema di \textit{bitcoin} è che per essere sicuri che una
transazione si andata a buon fine devo aspettare 6 blocchi, quindi un'ora di
tempo, e questo limita i casi d'uso della moneta (di certo non va bene per
prendere il caffè). Il numero di transazioni supportate è comunque minimo
rispetto a quelle che si possono effettuare con la moneta fisica.
\section{Ethereum}
Il futuro delle blockchains non ha potenzialmente limiti. Ci sono molte
applicazioni e molte varianti, una di queste è \textbf{Ethereum}.\\
\textit{Ethereum} è un altro tipo di blockchain in cui si pone l'attenzione non
tanto sulle transazioni quanto sulle \textbf{computazioni}. Si ha un modello
diverso di blockchain in quanto vengono memorizzate i cosiddetti
\textit{ethereum account}. Ogni utente che si registra quindi viene quindi
associato ad un account con una certa quantità di criptovaluta, che in questo
caso è chiamata \textbf{ether (\textit{ETH})}. Le transazioni avvengono più o
meno come in un conto in banca, togliendo \textit{ether} ad uno e dandoli
all'altro mentre la validazione delle transazioni avviene quasi come in
\textit{bitcoin} anche se si sta cercando di passare alla
\textit{proof-of-stake}. La novità di \textit{ethereum} è la possibilità di
scrivere/programmare i cosiddetti \textbf{smart contract}, ovvero il
corrispettivo dei contratti tra persone, dove, per esempio, un utente, per un
tot guadagno al mese, concede l'uso di una sua proprietà ad un altro
utente. Si usa un linguaggio di programmazione chiamato \textbf{Solidity},
simile ad un linguaggio OOP dove al posto degli oggetti si hanno i
\textbf{contratti} (ma si hanno comunque \textup{struct, function} etc$\ldots$).
\begin{listing}
  \begin{minted}{solidity}
    pragma solidity >=0.4.16 <0.8.0;

    contract SimpleStorage {
      uint storedData;

      function set(uint x) public {
        storedData = x;
      }

      function get() public view returns (uint) {
        return storedData;
      }
    }
  \end{minted}
  \caption{Esempio di contratto in Solidity tratto dalla documentazione di
    Solidity \url{https://solidity.readthedocs.io/en/v0.7.4/}}
\end{listing}
I contratti scritti con Solidity vengono compilati in un bytecode che viene
memorizzato sulla blockchain di ethereum (e quindi una copia viene salvata su
tutti i nodi della rete P2P). Si possono fare transazioni che trasferiscono
soldi oppure posso fare transazioni in cui si invocano funzioni poste
all'interno di un certo contratto, in questo caso un miner raccoglierà la
richiesta ed eseguirà sulla sua macchina il codice del contratto. Ogni
esecuzione di ogni singola operazione del bytecode (che viene eseguito sulla
\textit{ethereum virtual machine}) costa una certa quantità di \textbf{gas} e
quindi bisogna dare una certa quantità di soldi, rappresentati il costo del
\textbf{gas}, per far eseguire il contratto e, 
qualora non siano sufficienti, viene sollevata l'eccezione \texttt{outOfGas}, il
contratto non va a buon fine e i soldi finora spesi vanno persi.\\
Gli \textit{smart contract} sono codice di cui chiunque può vedere il bytecode e
sono una sorta di codice lato server e posso quindi fare dei client che
gestiscono le parti ``delicate'' dei trasferimenti di soldi facendo chiamate ai
contratti, che sono elaborati dai miner. La scrittura dei contratti è rischiosa
quindi si hanno quindi delle comunità (come \textit{OpenZeppelin}) che
controllano i contratti stessi e forniscono delle linee guida e delle librerie
da cui attingere, ma nel momento in cui vengono modificati non sono più testati,
ovviamente. Un esempio è stato di una startup che ha lasciato una
moltiplicazione non protetta da overflow modificando un contratto, preso da
\textit{Openzeppelin}, per permettere pagamenti simultanei (banalmente se si
doveva dare 10 ETH a due utenti si faceva 10$\cdot$2 e si controllava che ci
fossero effettivamente 20 ETH nel conto prima di effettuare il doppio
pagamento). Essendo il bytecode 
pubblico se ne sono accorti e qualcuno si è fatto un pagamento a due suoi altri
account dando una cifra esorbitante, in modo da scatenare l'overflow, il
prodotto per 2 dava 0 a causa dell'overflow e quindi si autorizzava la
transazione. Questa cosa non sarebbe stata possibile su \textit{bitcoin} ma la
diversa implementazione della blockchain di ethereum rendevano possibile la cosa
(e irriconoscibile al miner). Creare quindi \textit{smart contract} è
estremamente difficile, dovendo essere estremamente sicuri. Inoltre una volta
che il contratto è sulla blockchain ci resta, anche se il creatore può chiedere
al contratto stesso di autodistruggersi (nel senso che vengono rese invalide le
chiamate alle funzioni di quel contratto).\\
Ci sono vari strumenti per scrivere contratti, l'ide \textit{Remix} e
\textit{MetaMask}, che permette di avere un \textbf{wallet} su ethereum e di
collegarsi sia alla rete principale che a 
diverse reti, anche locali sul proprio computer, per testing. Tra le reti di
testing principali abbiamo \textit{Rapsten} che cerca di emulare al meglio
possibile quella principale (dove gli ETH che trasferisco o pago come
\textbf{gas} sono finti). In altre reti di test si hanno
altri algoritmi di consenso, come per esempio, nel caso della rete
\textit{Rinkeby} si ha il \textbf{proof-of-autority} (dove chi ha diritto a
scrivere una certa informazione lo può fare senza il consenso della rete P2P).\\
L'iter normale di sviluppo di contratti non prevede in primis l'uso delle reti
di test in quanto servirebbe troppo tempo ma si usano strumenti come
\textit{Ganache} o \textit{Truffle} che sono strumenti di sviluppo in locale che
simulano una rete in locale dove si può testare senza \textbf{gas} in modo
veloce. Solo dopo si passa alla rete di test e poi alla \textit{main net}, la
rete principale.\\
Esiste anche una libreria chiamata \textit{web3}, scritta in \textit{js}, per
permettere alle webapp di connettersi a MetaMask.\\
\section{Altre blockchains}
Si hanno comunque davvero tante diverse blockchains.\\
Si ha, ad esempio, \textit{Quorum}, una variante di \textit{Ethereum} in cui si
possono usare altri algoritmi di consenso e in cui si possono anche creare dei
canali di comunicazione privati tra i nodi della rete P2P.\\
Si hanno quindi diversi tipi di blockchain e in particolare ci sono le
blockchain di tipo \textbf{permissioned} e non solo quelle di tipo
\textbf{permissionless} o \textbf{pubbliche}.\\
per blockchain di tipo \textit{permissioned} si intende che i nodi della rete
P2P, che tipicamente sono anche quelli che scrivono informazioni e quindi
avviare le transazioni, si conoscono ma eventualmente non si fidano, formando
così un \textbf{consorzio} in cui i partecipanti sono potenzialmente in
conflitto (dove magari il fallimento di uno è il successo dell'altro). Se le
dichiarazione pubbliche siano vere o meno viene deciso con un servizio di
\textit{audit} esterno che valuta le dichiarazioni pubbliche. Non c'è quindi ne
mining ne \textit{proof-of-work}, magari nemmeno una criptovaluta. La blockchain
diventa quindi un punto in cui implementare politiche di trust tra i
partecipanti, obbligandosi a vicenda a dichiarare in modo pubblico (e se
imbrogli esci dal consorzio perdendone i vantaggi), fattore che
viene aiutato dal fatto che è estremamente difficile alterare la blockchain
(dovendo rompere una catena e rifarla, imbrogliando sugli hash etc$\ldots$)
rendendo le dichiarazioni praticamente eterne.\\
Si hanno quindi:
\begin{itemize}
  \item \textbf{permissionless blockchain}, come \textit{bitcoin} o
  \textit{ethereum}, dove chiunque può scaricarsi il software e diventare miner
  \item \textbf{permissioned blockchain pubblica}, dove possono vedere tutti,
  come \textit{Quorum}, \textit{EOS} (con una virtual machine basata su
  \textit{webassembly} e \textit{smart contract} scritti in \textit{C++}),
  \textit{Hyperledger} (sviluppata da IBM e ospitata dalla \textit{Linux
    Foundation}, essendo completamente \textit{Open Source}, di tipo modulare,
  molto complessa ma efficiente, nata 
  per il business con algoritmi di consenso basato sul \textbf{problema dei
    generali bizantini} e che scrive sulla blockchain solo se necessario
  etc$\ldots$) etc$\ldots$ Con queste blockchain si perde il discorso della
  decentralizzazione, non permettendo che partecipi chiunque ma si ha una
  \textit{certificate authority} che stabiliscono se uno ha il diritto di
  scrivere o leggere sula blockchain (e si hanno spesso più amministratori che
  si controllano a vicenda atti a stabilire chi può fare cosa)  
  \item \textbf{permissioned blockchain privata}, dove possono vedere solo
  quelli che scrivono
\end{itemize}
Grandi aziende, come IBM, Microsoft, SAP etc$\ldots$ che stanno concentrando
sullo sviluppo di blockchain ma anche piccole applicazioni, come
\textit{CryptoKitties}, basata su Ethereum, che permette di collezionare
e crescere ``gattini digitali'' collezzionabili.\\
Bisognerebbe inoltre fare un discorso sul rapporto che si ha tra \textit{smart
  contract} e dispositivi fisici, nonché sull'interpretazione legale degli
stessi (e gli avvocati dicono che non sono ne legali ne ``smart'' per i problemi
detti sopra).
\begin{figure}
  \centering
  \includegraphics[scale = 0.33]{img/blockchain.png}
  \caption{Diagramma dei casi di scelta di una blockchain}
  \label{fig:bcd}
\end{figure}
\begin{shaded}
  \noindent
  \textit{Vengono qui aggiunte le cose dette in live.}\\
  La blockchain potrebbe essere usata per la tracciabilità di prodotti, per
  evitare \textit{falsi veri} o \textit{veri falsi}. Infatti non è un problema
  lontano dal \textbf{double-spending} (un prodotto viene fatto in numero
  limitato e quindi non posso avere più di quel numero sul mercato). Per il
  tracciamento del prodotto associo un hash-code univoco. Metodi come il
  bar-code non funzionano bene in quanto facilmente duplicabili. Un metodo più
  moderno prevede l'uso proprio delle blockchain, introducendo anche la
  tracciabilità dei documenti, permettendo anche di verificare tutta la filiera
  per arrivare al prodotto finale. Ogni step della filiera è come se avesse un
  file di log, dove viene segnato tutto quello che viene fatto in quello
  step. Si deve anche garantire che nessuno step della filiera ``bari''. Inoltre
  non si può avere una TTP online (i vari consorzi, gli unici che potrebbero
  fare da TTP, potrebbero anche loro ``barare'', non essendo un ente che farebbe
  solo da TTP) e si ha che gli scrittori ``non si fidano'' tra loro. Abbiamo
  quindi tutti i requisiti per una blockchain, come spiegato immagine
  \ref{fig:bcd}. Mantenere una blockchain comunque costa (più di un ipotetico
  timbro univoco da apporre al prodotto, tramite i concetti di \textit{lotto di
    produzione}) ma ha più senso in un contesto multi-aziendale.\\
  Ovviamente si ha anche la difficoltà culturale di far interagire piccole
  imprese con concetti complessi come la \textit{proof-of-work}. Un ragionamento
  simile si applica sia al mondo del cibo che della moda, che ad altri ambiti.\\
  Il tracciamento dei clienti non è fattibile causa GDPR, in quanto non si può
  facilmente cancellare i dati (cosa garantita dalla GDPR) nella blockchain, che
  infatti non è \textit{GDPR-compatible} (e potrei non inserire i dati in
  chiaro ma cifrati, dimenticando la chiave nel caso il cliente voglia essere
  dimenticato, in quanto potenzialmente prima o poi sarebbe facile ``bucare'' i
  dati).
\end{shaded}
\chapter{NoSQL}
Dopo aver trattato la gestione dei dati distribuita in \textbf{ambiente
  relazionale}, dopo aver trattato le blockchain anche da punto di vista di
gestione dati in \textbf{ambiente sicuro}, affrontiamo i sistemi
\textbf{NoSQL}.\\
Nel 1970 un ricercatore di IBM chiamato Codd pubblica un articolo intitolato
\textit{A relational model of data for large shared data banks} in cui presenta
un modello di rappresentazione dati indipendente dall'implementazione. Si era
passati dal \textbf{modello gerarchico} (in cui bisognava usare i puntatori per
accedere ai dati) al \textbf{modello relazionale}. Questo passaggio è stato
fondamentale per lo sviluppo del mondo dell'informatica, un mondo dove
l'hardware dedicato allo storage era estremamente costoso, estremamente poco
capiente (nel range di pochissimi megabyte) e logisticamente parecchio
ingombrante. Il modello relazionale quindi era studiato in primis per questo
contesto, proponendo strategie per la rappresentazione compatta dei dati.\\
Gli aspetti positivi del modello relazionale si possono comunque riassumere in:
\begin{itemize}
  \item è un modello molto ben definito, con il \textbf{principio della
    minimizzazione}, in merito allo spazio, e il \textbf{principio della closed
    world assumption}, ovvero tutto quello che l'applicazione deve sapere è nel
  database e a tal fine si ha anche l'uso del \texttt{NULL} \textit{value} per:
  \begin{itemize}
    \item assenza di informazione sul valore
    \item assenza di senso di un certo dato
    \item assenza di applicabilità di un certo dato
  \end{itemize}
  Quindi il modello relazionale è una sorta di \textit{modello chiuso} che
  contiene tutto il necessario ed era un modello ragionevole in un contesto come
  quello ``pre anni 2000'', dove un'azienda tipicamente necessitava solo di
  informazioni interne all'azienda stessa (non c'erano le informazioni dei
  \textit{social network} etc$\ldots$ infatti, di fatto, non esiste più la
  \textit{closed world assumption})
  \item si hanno circa 35 e più anni di sviluppo su \textit{sicurezza,
    ottimizzazione \textnormal{e} standardizzazione}. Da questo punto di vista
  si ricorda l'uso delle proprietà ACID che comunque rendono ancora valido il
  modello anche nel 2020. Inoltre ormai si hanno talmente tante informazioni
  memorizzate tramite il \textit{modello relazionale} che le aziende sono
  restie a cambiare modello (a causa dei rischi di perdita dei dati durante un
  porting, anche dati dal fatto che i db preesistenti sono enormi, considerando
  che i dati persi non sono recuperabili potenzialmente, a meno di repliche)
  \item è ben conosciuto, è studiato anche nelle scuole e nelle università
  \item è comunque la scelta migliore per diversi casi d'uso
\end{itemize}
Si hanno però anche delle limitazioni per il modello relazione, come ad esempio:
\begin{itemize}
  \item il fatto stesso di essere un \textbf{sistema chiuso}, quasi per gli
  stessi motivi descritti nei vantaggi, può anche essere un aspetto negativo,
  infatti:
  \begin{itemize}
    \item il \textbf{principio di minimizzazione}, ai giorni nostri, va a
    discapito, inutilmente, delle performance 
    \item il \textbf{principio della closed world assumption}, come già detto,
    non è praticamente più valido nell'era dei social network etc$\ldots$
    \item il modello relazionale prevede per un attributo uno e un solo valore e
    non sono quindi ammissibili array o comunque altre strutture per dati con
    multipli valori e non si possono fare interrogazioni in
    modo strutturato all'interno dei valori stessi, dovendo ricorrere a diversi
    \textit{workaround} e, secondo ``il manuale'', il metodo standard è quello
    di fare una tabella di join, detta \textit{tabella di part-of}, ovvero una
    tabella a parte coi possibili multi valori (questa soluzione porta ad
    operazioni di join molto dispendiose)
    \item non è compatibile con i linguaggi di programmazione moderni. Molti
    linguaggi moderni sono infatti OOP, con la \textit{object persistency},
    paradigma non supportato dal modello relazionale. Si hanno quindi framework
    (come \textit{Spring}) per far comunicare, tramite l'\textbf{object
      relational mapping} il modello relazionale e gli
    oggetti della OOP (aggiungendo complicazioni e perdite di
    performance). Database ad oggetti sono stati creati e implementati nei
    principali vendors ma non sono usati nelle aziende per varie ragioni, anche
    di performance e di impatto rispetto ai db relazionali
    \item non supportano i \textbf{comportamenti ciclici}, comportando limiti
    modellistici
  \end{itemize}
  \item i database relazionali sono difficili da modificare dal punto di vista
  schematico tramite \texttt{alter table}. Cambiare gli schemi è complesso e
  comporta il fermo del database
  \item per garantire le proprietà ACID nei sistemi transazionali e per poter
  garantire il 2PC si ha un limite fisico alla \textbf{scalabilità}. Dopo un
  certo punto di crescita i costi di organizzazione rendono impossibile
  l'esecuzione dei protocolli che garantiscono ACID, in ambiente
  distribuito. Oggigiorno ci sono varie applicazioni, legate in primis ai social
  network, per le quali è impossibile mantenere certi protocolli. Il problema
  della scalabilità è uno degli aspetti più limitanti, specialmente in ottica
  \textbf{big data} (anche se vedremo che le alternative non relazionali coprono
  casi d'uso anche oltre il solo ambito dei \textit{big data}). I database
  relazionali si prestano bene alle \textbf{scale up (\textit{scalabilità
      verticale})} (semplicemente aumentando
  l'hardware a disposizione, arrivando a costi esorbitanti e fino al punto in
  cui si raggiunge il limite tecnologico) ma pochissimo allo \textbf{scale out
    (\textit{scalabilità orizzontale})} (che comporterebbe non l'acquisto di un
  nuovo hardware intero ma solo di una parte di esso, a prezzo più basso con
  hardware detto in gergo \textbf{comodity},
  magari per avere più dischi etc$\ldots$, ma hardware dedicato storicamente ai
  db relazionali non prevede questa cosa). Oggigiorno si hanno sempre più spesso
  le \textbf{architetture a microservizi} (che ``implementano'' bene lo
  \textit{scale out})
  \item il \textbf{time in market}, ovvero il tempo in cui un applicativo resta
  in commercio, è sempre più ridotto (spesso, per esempio, un videogioco ha una
  vita anche inferiore ad un anno) portando a rendere obsoleto il processo in
  cui normalmente si integrano i db relazionali. Anche il \textit{time to
    market}, ovvero il tempo di preparazione si è ridotto
\end{itemize}
Oggigiorno lo standard dal punto di vista hardware è drasticamente cambiato con
dischi di capienza enorme (nell'ordine dei terabyte per qualche decina di euro).
Il rapporto costo-gigabyte è crollato rispetto a pochi anni fa. Il contesto
storico è quindi cambiato e in tal senso anche il modello per la
rappresentazione dei dati sta cambiando e sta cambiando anche la scelta tra
\textit{spazio} e \textit{tempo}, non essendo più la prima una grossa
problematica ed essendo la seconda di grande interesse, dovendo rispondere il
più in fretta possibile alle interrogazioni.\\
\textbf{Sulle slide un elenco dei vari step storici.}
\section{I modelli NoSQL}
Il termine \textbf{NoSQL} è stato coniato da Carlo Strozzi nel 1998 quando si
inventa una sorta di API per Linux per accedere ai dati relazionali senza usare
SQL. Il termine è stato ripreso nel 2009 con la ``logica'' \textbf{Not Only
  SQL} dopo che per circa 9 anni, a partire dal 2000, erano nati nuovi modelli,
a grafi, a documenti etc$\ldots$ (da parte di Amazon, Google, Facebook
etc$\ldots$).\\
Quindi NoSQL è un'insieme di modelli di rappresentazione dati, con relativi
software di gestione. Si hanno 3 caratteristiche fondamentali:
\begin{enumerate}
  \item tutti i modello sono \textbf{schema free} o \textbf{schemaless}
  \item tutti i \textbf{DBMS NoSQL} usano il \textbf{CAP theorem}
  \item si passa da ACID a BASE (da ``acido'' a ``basico'') ovvero:
  \begin{itemize}
    \item \textit{Basic Available}
    \item \textit{Soft state}
    \item \textit{Eventual consistency}
  \end{itemize}
\end{enumerate}
\subsubsection{Schema free}
Nel modello relazionale prima veniva definito il modello, ovvero l'insieme degli
attributi che descrivevano i dati e le varie relazioni, e poi veniva popolato
con i dati veri e propri. Questo iter comportava che se durante lo sviluppo ci
si accorgeva che bisognava aggiungere, ad esempio, un attributo bisognava
modificare l'intero schema (nonché i dati già caricati). \\
Con NoSQL si è quindi passati quindi ad un'architettura dove il modello è basato
sui dati che vengono inseriti quindi di fatto per ogni ``riga'' ho
potenzialmente uno schema diverso. Questo concetto, detto \textbf{schema free}
concede una grandissima libertà di sviluppo, al costo di un'alta probabilità di
avere inconsistenza tra i dati. Si ha il cosiddetto \textbf{open word
  assumption}, dove i valori \texttt{NULL} vengono eventualmente messi solo per
motivi applicativi in quanto se un dato non c'è semplicemente non c'è e non
bisogna specificare non \texttt{NULL} il fatto che non ci sia, garantendo una
grande flessibilità (anche se questo può generare problemi). Tale flessibilità
ha comunque un costo anche dal punto di vista delle query, in quanto non si può
partire dallo schema ma si deve partire da frammenti del modello in studio. Si
ha comunque il guadagno che in caso di aggiunta di attributi etc$\ldots$ non
bisogna modificare l'intero schema, aggiungo dove serve e basta, anche se
ovviamente serve una \textit{governance} del dato più forte (in quanto può anche
succedere che non si abbia il tipo di dato). 
\subsubsection{CAP theorem}
Vediamo questo teorema essenziale\footnote{così L.T. è contento}, proposto da
Gilber e Lynch nel 2002 dopo che, qualche anno prima, nel 2000, Brewer aveva
proposto una sua \textit{congettura sulla consistenza, disponibilità e
  partizionamento dei webserver}. Il teorema dimostra la congettura di Brewer:
\begin{teorema}[CAP theorem]
  Preso un \textbf{sistema distribuito di nodi} si considerino tre aspetti:
  \begin{itemize}
    \item \textbf{Consistency (\textit{consistenza})}, ovvero che tutti i nodi
    abbaino gli stessi i dati nello stesso istante (che è diverso dalla
    consistenza di ACID)
    \item \textbf{Availability (\textit{disponibilità})}, ovvero la garanzia che
    ogni richiesta ricevuta da un sistema riceva una risposta in
    ogni caso, sia in caso fallimento che successo (non si deve avere quindi
    un'attesa infinita)
    \item \textbf{Partition tolerance (\textit{partizionamento})}, ovvero il
    sistema deve continuare ad operare comunque anche se si hanno perdite di
    messaggi o fallimenti di parti del sistema
  \end{itemize}
  Il teorema garantisce che tra queste tre caratteristiche, in un sistema
  distribuito, se ne possano avere contemporaneamente due soddisfatte e mai tre.
\end{teorema}
Si nota quindi come, prendendo ad esempio i DDBMS, si rinunci alla
\textit{partition tolerance}, garantendo le prime due caratteristiche (anche se
con un numero limitato di nodi si può arrivare ad una buona resa anche dal
punto di vista della \textit{partition tolerance}).\\
I sistemi NoSQL tipicamente divisi tra:
\begin{itemize}
  \item sistemi che garantiscono \textit{consistency} e \textit{partition
    tolerance} (CP), non potendo garantire che il DBMS lavori 24/7. Tra questi
  troviamo:
  \textit{BigTable, Hypertable, HBase, MongoDB, Terrastore, Scalaris, Berkeley
    DB, MemcacheDB, Redis $\ldots$}
  \item sistemi che garantiscono \textit{availability} e \textit{partition
    tolerance} (AP), non potendo garantire che i dati siano sempre
  consistenti. Tra questi troviamo: \textit{Dynamo, Voldemort, Tokyo Cabinet,
    KAI, Cassandra, SimpleDB, CouchDB Riak $\ldots$}
\end{itemize}
Non è detto che due db NoSQL che implementano lo stesso modello, ad esempio
documentale, garantiscano le stesse due caratteristiche CAP.\\
Quindi in fase progettuale la scelta tra due delle caratteristiche CAP è un
punto chiave per poi scegliere eventualmente su che DBMS NoSQL appoggiarsi (o
anche partire da un DBMS NoSQL che già si conosce ma essendo consci della
caratteristica non garantita).\\
Ci sono contesti applicativi in cui ognuna delle tre caratteristiche può, a
seconda, non essere fondamentale. In ogni caso si ha che:
\begin{itemize}
  \item \textit{consistency} mi garantisce che tutti i client hanno sempre la
  stessa vista sui dati
  \item \textit{availability} mi garantisce che ogni client può sempre leggere
  e/o scrivere
  \item \textit{partition tolerance} mi garantisce che il sistema continui a
  funzionare nonostante partizioni fisiche della rete
\end{itemize}
In fase progettuale quindi ho 3 step:
\begin{itemize}
  \item scelta del modello
  \item scelta del DBMS
  \item scelta delle caratteristiche del CAP theorem
\end{itemize}
\subsubsection{BASE}
Analizziamo meglio l'acronimo.
\begin{itemize}
  \item \textit{Basic Available}, ovvero, comunque vada, la risposta viene
  sempre data anche nel caso di consistenza parziale
  \item \textit{Soft state}, ovvero si rinuncia al concetto di consistenza di
  ACID
  \item \textit{Eventual consistency}, ovvero, ad un certo punto prima o poi
  nel futuro (magari anche dopo pochissimi secondi),
  i dati ``convergeranno'' ad uno stato di consistenza (in opposizione alla
  consistenza immediata di ACID)
\end{itemize}
Vediamo quindi i vari modelli NoSQL:
\begin{itemize}
  \item \textbf{key-value stores} (come \textit{Dynamo, Voldemort, Redis, Riak
    $ldots$}) che è il modello più semplice. È infatti il semplice modello
  \textit{chiave-valore}, semplice da memorizzare, e utile per il caching. Il
  valore è un \textit{blob} quindi non si ha modo di interrogare internamente il
  valore me deve essere l'applicazione a manipolarlo. La chiave garantisce
  tipicamente meccanismi di hash, con la chiave che punta ad un certo
  valore, massimizzando le performance. Viene garantito l'\textit{hashing
    distribuito}, ovvero la \textit{distributed hashed table}
  \item \textbf{column family stores}, (come
  \textit{BigTable, Cassandra, HBase $\ldots$}) dove la chiave punta a colonne
  multiple di valori, ottenendo un formato \textit{pseudo-relazione}. Ogni riga,
  in questo schema \textbf{wide column}, ha una sua struttura interna diversa a
  quella di altre righe, che sono comunque indipendenti. La chiave ovviamente
  può essere hashata. Non posso fare \textit{ranged query}
  \item \textbf{document databases} (come \textit{CouchDB, MongoDB $\ldots$}) è
  formato da documenti con coppie chiave-valore o chiave-oggetto, dove l'oggetto
  può anche essere un'altra chiave-valore, un array di valori, un array di
  oggetti etc$\ldots$. Un esempio pratico di documento è il tipico file json,
  che è ottimo per dire che nel modello documentale si ha un elemento più
  importante, detto \textit{root} che ha al di sotto tutti gli altri
  \item \textbf{graph databases} (come \textit{Neo4J, FlockDB, GraphBase,
    InfoGrip $\ldots$}) con nodi e relazioni tra gli stessi rappresentate dagli
  archi. I nodi possono rappresentare entità a cui sono associate varie
  proprietà (e si chiamano \textit{property graph}). le proprietà sono quindi
  pertinenti ai nodi. I graph db non scalano bene
  \item \textbf{RDF databases}
\end{itemize}
(\textit{Spesso l'azienda stessa non è attenta al tipo di modello usato quindi
  si hanno spesso confusioni}).\\
A livello di complessità (anche dei dati che rappresentano) possiamo dire che,
seguendo l'ordine elencato, si passa dal più semplice al più complesso. Anche
dal punto di vista del volume abbiamo lo stesso ordine, dal più voluminoso
(quindi il meno complesso è anche il più voluminoso ma più scalabile) a quello
meno voluminoso e scalabile.\\
Il problema fondamentale in tutti questi modelli è come connettere le
informazioni e questo diventa un problema fondamentale in termini di prestazioni
e analisi da effettuare. Si ha che le relazioni sono implicitamente incluse nei
dati. La scelta del modello dipende dall'applicativo e ci sono applicativi che
possono obbligare ad avere due o più modelli.\\
Ovviamente ogni vendor ha il suo linguaggio di query per il suo DBMS NoSQL.
\section{Document based system}
È uno dei modelli più ricchi nonché più vicini al modello relazionale. Nella
sezione approfondiremo anche il più ``famoso'' DBMS documentale, ovvero
\textbf{MongoDB}.\\
Si ha un'evoluzione del modello chiave-valore in quanto si ha una chiave, ovvero
un \textit{object identifier}, che può essere indicizzata tramite un
\textit{meccanismo di hashing}. I documenti solitamente vengo invece memorizzati
in file \textbf{json} o \textbf{XML} e possono essere cercati a qualsiasi
livello.\\
A differenza del modello relazionale il modello documentale è alla fine un
albero e quindi l'accesso ai dati deve comunque partire dal nodo radice, a
scendere verso le foglie (anche se si può avere un accesso \textbf{a indici} per
accedere alle informazioni necessarie). Si ha quindi il concetto di
\textbf{elemento più importante degli altri}, concetto assente nel modello
relazionale, definibile ``piatto'' da questo punto di vista (a meno delle
\textit{data warehouse} dove si ha effettivamente una tabella principale).\\
La rappresentazione documentale è più vicina al pensiero umano di divisione
delle cose in ``cartelle'' con varie tipologie di informazioni. \\
Si hanno quindi le seguenti caratteristiche per il modello documentale:
\begin{itemize}
  \item è multidimensionale, ad albero
  \item ogni campo può contenere zero valori, un valore, multipli valori o altri
  documenti
  \item si possono effettuare query ad ogni campo e in ogni livello
  \item lo schema è flessibile, infatti due documenti della stessa
  \textbf{collezione} possono avere uno schema diverso di rappresentazione dei
  dati
  \item posso aggiungere ``in linea'' nuove informazioni
  \item avendo ``annegato'' uno dentro l'altro le relazioni che sussistono tra
  gli elementi, sono richiesti molti meno indici per effettuare le query
  migliorando le performance in quanto le informazioni sono legate tra loro nel
  documento (e non dovendo, come nel caso relazionale, prendere $n$ tabelle in
  $n$ file diversi nel \textit{filesystem} facendo poi il join)
\end{itemize}
Il modello documentale permette due tecniche principali:
\begin{itemize}
  \item \textbf{referencing}, ovvero, come per il modello relazione, si
  relazionano due documenti tramite un certo attributo, ad esempio:
  \begin{figure}[H]
    \centering
    \begin{subfigure}{.4\textwidth}
      \centering
      \begin{minted}{python}
        contact
        {
          "id": 2,
          "name": "Carlo",
          "address_id": 1
        }
      \end{minted}
    \end{subfigure}
    \begin{subfigure}{.4\textwidth}
      \centering
      \begin{minted}{python}
        address
        {
          "_id": 1,
          "city": "Milano",
          ...
        }
      \end{minted}
    \end{subfigure}
  \end{figure}
  
  \item \textbf{embedding}, ovvero, aggiungere quello che nel
  \textit{referencing} è un secondo documento come specifica di un certo valore,
  aggiungendo quindi un livello di profondità (quindi, per esempio, posso avere
  un documento relativo ad un contatto telefonico contenente anche indirizzo
  di casa e tale informazione, con i vari attributi, sarà specificata non in un
  altro documento ma nello stesso), Aumentando lo spazio che però non è più un
  problema. Ad esempio si ha:
  \begin{minted}{python}
    contact
    {
      "id": 2,
      "name": "Carlo",
      "address": {
        "city": "Milano",
        ...
      }
    }
  \end{minted}
\end{itemize}
A causa del\textit{ time to/in market} ormai sempre più ogni applicazione ha il
suo db e questo obbliga la flessibilità dello schema, che garantisce che una
modifica allo schema, con aggiunta o modifica degli attributi, non è un problema
(mentre con il modello relazionale sarebbe stato un problema). Questa
flessibilità però comporta che in fase di query non ho uno schema prefissato
tramite il quale interrogare (e non posso nemmeno vedere i primi $n$ documenti
in quanto l'$n+1$ potrebbe essere comunque diverso). Questa problematica è
relativa all'uso di database costruiti da terzi.\\
Si hanno diversi linguaggi di query, tra cui \textit{jsoniq, jmespath, jaql,
  JLINQ, etc$\ldots$}, non avendo più quindi un unico linguaggio come era SQL per i
modelli relazionali, ma avendone uno per vendor.\\
Dal punto di vista delle terminologia si hanno le seguenti ``relazioni'' con il
modello relazione:
\begin{table}[H]
  \centering
  \begin{tabular}{c|c}
    SQL & NoSQL\\
    \hline
    database & database \\
    tabella & collezione\\
    riga & documento\\
    colonna & chiave
  \end{tabular}
\end{table}
\subsection{MongoDB}
MongoDB è un DBMS non relazionale documentale con i dati memorizzati in un
modello chiamato \textbf{binary json (\textit{Bson})}, che consente maggior
efficienza tramite una rappresentazione binaria. Tramite l'embedding dei dati
consente di non eseguire i \textit{join} e permette anche l'accesso tramite
indici. Supporta comunque il referencing anche se con cali di performances.\\
Dal punto di vista architetturale si hanno una serie di \textit{engine} diversi
fra loro (attualmente si usa \textbf{WiredTiger (\textit{WT})}, dopo acquisto
di una società che si occupava di db relazionali distribuiti con transazioni,
mentre prima si usava \textit{MMAP V1}). Grazie a WT si è introdotto, nella v4,
il supporto alle transazioni. Si ha poi il \textit{data model}, il
\textit{MongoDB query language (MQL)} (proprietario), i componenti di controllo
degli accessi, il meccanismo di gestione etc$\ldots$. Le repliche sono
organizzate in \textbf{replicaSet}.\\
Nel caso di MongoDB si ha:
\begin{table}[H]
  \centering
  \begin{tabular}{c|c}
    RDBMS & MongoDB\\
    \hline
    database & database \\
    tabella/view & collezione\\
    riga & documento (Bson)\\
    colonna & campo\\
    indice & indice\\
    join & documento con embedding\\
    chiave esterna & referencing\\
    partizione & shard
  \end{tabular}
\end{table}
Vediamo quindi un esempio di schema in Mongodb:
\begin{listing}[H]
  \begin{minted}{js}
    var p = {
      '_id': '342',
      'author': DBRef('User',2),
      'timestamp': Date('26-10-20'),
      'tags': ['MongoDB','NoSQL'],
      'comments':['author': DBRef('User',4),
                  'date': Date('26-10-20'),
                  'text': 'hello'.
                  'upvotes': 7,...]
    }
    >db.posts.save(p);            
  \end{minted}
  \caption{Esempio di aggiunta di un documento $p$ nella collezione $posts$}
\end{listing}
Analizzando il codice abbiamo:
\begin{itemize}
  \item la definizione di $p$ come un oggetto json
  \item tra gli elementi abbiamo $tags$ che contiene un array di valori
  \item tra gli elementi abbiamo $comments$ che è un array di documenti
  \item l'ultima riga è tra le chiavi di successo di Mongodb in quanto ha un
  linguaggio simile a quelli di programmazione. Nel dettaglio si ha il
  caricamento del documento $p$ nella collezione $posts$. Si ha un meccanismo
  simile alla OOP e l'integrazione coi linguaggi di programmazione è facilitata
  dall'ormai molto diffuso supporto ai json (come esistono già meccanismi per il
  passaggio da oggetti a json e viceversa)
\end{itemize}
Si possono inoltre fare indici, tramite \texttt{ensureindex} (che prende come
argomento un json), a ogni campo del
documento, per esempio: 
\begin{minted}{js}
  >db.posts.ensureIndex({'author': 1});
  >db.posts.ensureIndex({'comments.author': 1});
  >db.posts.ensureIndex({'tags': 1});
  >db.posts.ensureIndex({'author.location': '2d'});
\end{minted}
Si nota che posso creare indici a livello che voglio, a documenti o array, o
anche, come nell'ultimo esempio, usare indici geo-spaziali.\\
MongoDB, rispetto al modello relazionale, si avvicina tanto alla
programmazione.\\
\subsubsection{Repliche in MongoDB}
Innanzitutto si ha che MongoDB rientra nella categoria CP rispetto al
\textit{CAP theorem}, non garantendo disponibilità in caso di guasto.\\
La replica viene effettuata in modalità master-slave anche se, nel gergo di
MongoDB si ha \textbf{primary-secondary}, avendo scrittura sul primary e replica
sui secondary. Si ha in realtà che uno dei nodi replica viene definito a
posteriori \textit{primary}. La replica avviene tramite il file di log del
\textit{primary}. Si ha un sistema che procede per repliche parziali
incrementali, all'inizio il nodo fa un sync, prendendo i dati dal primary e poi
prende dal file di log del primary solo gli ultimi $n$ movimenti.\\
La garanzia della tolleranza si ha tramite il meccanismo di elezione dei nodi,
nel caso il nodo primary vada down.\\
Si ha che un nodo può assumere uno dei seguenti stati:
\begin{itemize}
  \item STARTUP: un nodo che non è membro effettivo di nessun replica set
  \item PRIMARY: l’unico nodo nel replica set che accetterà le operazioni di
  lettura
  \item SECONDARY: un nodo che di effettuare la sola replicazione dei dati (può
  essere promosso a PRIMARY in fase di elezione)
  \item RECOVERING: un nodo che sta effettuando una qualche operazione di
  recupero dei dati, magari dopo un \textit{rollback} (può essere promosso a
  PRIMARY in fase di elezione) 
  \item STARTUP2: un nodo che è appena entrato nel set (può essere eletto a
  PRIMARY)
  \item ARBITER: un nodo non atto alla replicazione dei dati con lo scopo di
  prendere parte alle elezioni per promuovere i nodi a PRIMARY (può essere
  eletto a PRIMARY in fase di elezione)
  \item DOWN/OFFLINE: un nodo irraggiungibile
  \item ROLLBACK: un nodo che sta svolgendo un rollbackper cui sarà
  inutilizzabile per le letture (può essere promosso a PRIMARY in fase di
  elezione)  
\end{itemize}
L'elezione elegge il nodo con la versione più aggiornata dei dati e in questa
fase le scritture sono interrotte (motivo per cui non è garantita la A di
CAP). L'elezione dura comunque pochissimi.\\
Durante il processo di elezione ogni \textit{replicaSet} manda, con timeout per
assumere che un nodo sia down, un
``hearbeat'' (ovvero un bit ogni due secondi) agli altri nodi. Se il nodo
primario risulta down si procede con l'elezione, che in termini umani è di tipo
\textit{maggioritario con liste bloccate} nel replicaSet. Ogni nodo ha un
identificativo con un punteggio rappresentante la priorità nelle elezioni
(più alto più probabile che venga eletto). Quindi appena il primary va down il
secondario con valore più alto chiamerà le elezioni. Si ha che un replica set
può avere al massimo 50 membri di cui solo 7 votanti, che esprimono un solo
voto. I nodi SECONDARY non votanti hanno priorità 0 e accettano solo letture ma
sono comunque in grado di votare. Nel caso di partizionamento
to della rete si rischia un problema di riconciliazione tra vecchio primary
e nuovo, dovendo quindi bloccare la scrittura.
\subsubsection{Scrittura}
La scrittura avviene sul nodo primario e si hanno diverse politiche in base
tramite l'opzione \textit{writeConcern} che dice cosa fare per le replicaSet.
Si hanno tre parametri:
\begin{enumerate}
  \item $w$, indicante il numero di nodi in cui il dato deve essere replicato
  prima di essere considerata conclusa l’operazione. Ovvero è il numero di nodi
  sui quali deve essere confermata la scrittura per essere confermata a livello
  globale
  \item $j$, ovvero l'intenzione di scrivere sul log di \textit{Wiredtiger},
  tramite la logica \textit{write ahead logging} (ovvero prima scrivo sul file
  di log e poi sul disco), prima che l'operazione sia stata
  eseguita. Questo per garantire la persistenza del dato, avendolo comunque nel
  file di log. Senza l'opzione lo scriverà in un secondo momento
  \item $wtimeout$, ovvero il tempo limite, espresso in ms, da aspettare nel
  caso in cui $w>0$ (se $0$ non devo aspettare risposta da nessuno)
\end{enumerate}
Aumentare $w$ comporta aumentare il tempo di esecuzione. Nel dettaglio:
\begin{itemize}
  \item $w=0$ non da alcuna certezza di inserimento, utile per operazioni veloci
  \item $w=1$ implica la scrittura sul nodo primary
  \item $w=n$ implica la scrittura su $n$ nodi
  \item $w=majority$ dove almeno la metà dei nodi più 1 deve aver scritto prima
  della conferma dell'operazione (nella realtà si tenta di scrivere su più nodi
  ma appena si ha un ack dalla metà più 1 si conferma la scrittura anche se
  magari si sta ancora scrivendo su altri nodi)
\end{itemize}
Qualora si debba caricare una gran quantità di dati, di cardinalità conosciuta,
per motivi di efficienza non conviene aspettare che il nodo dia l'ack ma si
caricano tutti subito sul primary, controllando poi con \texttt{count} se tutto
è stato caricato.
\subsubsection{Frammentazione}
Parliamo ora di frammentazione e scalabilità.\\
Si hanno 3 tipi di sharding per la scalabilità orizzontale:
\begin{itemize}
  \item \textit{hash-based}, quindi sulla chiave
  \item \textit{range-based}, quindi in base ad un certo range di elementi
  \item \textit{tag-aware}, quindi in base a certi attributi
\end{itemize}
Si ha un meccanismo dinamico basato su \textit{pay as you go}, ovvero nel
momento in cui ho lo sharding sarà MongoDB ad adattarlo nel momento in cui
cambia il volume di dati, facendo un \textit{bilanciamento automatico}. Si ha
uno ``spazio'' della mia chiave di shard che posso dividere in $n$ chunks per
$n+1$ chiavi di shard. Il numero massimo di chunk che è possibile definire su
una sharded key può definire il numero massimo di shard in un sistema.\\
Si hanno 3 ruoli fondamentali nella frammentazione:
\begin{enumerate}
  \item \textbf{mongos}, che è il punto di accesso software per le query, è
  l'entry point del cluster, tutte le query verranno eseguite su questo
  processo. Appena lanciato mongos va a leggere il config server
  
  \item \textbf{config server}, ovvero un file che conosce la struttura degli
  shard, conoscendo frammentazioni e repliche. Dalla versione 4.0 ogni nodo
  distribuito è anche replicato
  \item i vari \textbf{shard}, ovvero istanze di MongoDB
\end{enumerate}
Quindi il router quando riceve una query capisce dove sono i frammenti, grazie
al config server, e quindi procede con la query di tipo:
\begin{itemize}
  \item \textbf{target query} se deve interrogare solo un nodo
  \item \textbf{broadcast query} se deve interrogare più nodi
\end{itemize}
Sia gli shard che il config server sono replicati su replicaSet per evitare che
diventino \textit{single point of failure} (anche per questo dalla 4.0 ogni
volta che frammento replico).\\
Si ha un \textbf{primary shard}, che contiene l'intero db (ovvero tutte le
collezioni) e i \textbf{secondary shard} che contengono frammenti e repliche di
alcune collezioni. Solitamente si ha quindi una configurazione a 3 nodi:
\begin{enumerate}
  \item uno primary
  \item due nodi secondari con frammenti diversi e le repliche del primary
\end{enumerate}
\subsubsection{Letture}
Ho una politica di \textit{readConcern} (che deve essere consistente come del
caso del \textit{writeConcern}, facendo scegliere se è più dispendioso scrivere
o leggere):
\begin{itemize}
  \item \textit{local}, che è il valore di default t per leggere i dati dai nodi
  in un replicaSet. La query è fatta secondo la località spaziale e quindi se
  legge un nodo non primary rischio di leggere dati non ancora replicati
  \item \textit{available}, di default per lettura su nodi secondari, che fa
  vedere che il dato c'è ed è consistente
  \item \textit{majority}, quando almeno metà più 1 repliche ha ricevuto un ack
  in scrittura sul dato allora posso leggere
\end{itemize}
Vediamo quindi un esempio di interrogazione, tramite il metodo \texttt{find}:
\begin{center}
  \textit{db.user.find({age:{\$gt:18}},{name:1,address:1}).limits(5)}
\end{center}
che equivale a:
\begin{minted}{sql}
  SELECT name, address
  FROM user
  WHERE age>18
  LIMIT 5
\end{minted}
\textit{\$gt} significa \textit{greater than} e si hanno vari identificativi per
i confronti (per non avere i simboli >, < etc$\ldots$ che darebbero fastidio
nelle stringhe).\\
Vediamo anche una \textit{insert}:
\begin{minted}{js}
  db.user.insertOne({
    name: "Andrea" ,
    age: 26,
    Teach[«datawarehouse», «architecture»]
  })
\end{minted}
con \texttt{insertOne} che specifica che voglio inserire un solo documento
mentre avrei \texttt{insertAll} per tutti i documenti.\\
Un \textit{update} avviene con la stessa logica con \texttt{updateAll}:
\begin{minted}{js}
  Db.user.updateAll({
    name:{$eq: «Andrea»},
    {$set: {Role: «PA»}},
    {$upsert:true}
  })
\end{minted}
%$
\textit{\$upsert} sarebbe un \textit{insert if not exist}, per permettere
l'inserimento e non la modifica in caso di assenza. Tale cosa è assente in SQL
ma servirebbe un doppio comando.\\
Si hanno varie utility per importare i dati, da specificare come opzioni al
comando \texttt{mongoimport}, ad esempio:
\begin{center}
  \textit{mongoimport –d database –c collection –type csv -ignore blanks –file
    PATH} 
\end{center}
per importare un csv al path PATH ignorando gli spazi bianchi.\\
\subsubsection{Transazioni}
Dalla versione 4.0 MongoDB ha introdotto nel modello documentale le transazione
tramite l'engine \textit{Wiredtiger}. Introduce quindi il concetto di
\textit{log} che indica che tutto è nel nodo primary. Si opera sempre sul nodo
facendo un \textit{lock} che può essere di tipo:
\begin{itemize}
  \item \textbf{sharded (\textit{S})}, per le letture
  \item \textbf{exclusive (\textit{X})}, per le scritture
  \item \textbf{intent sharded (\textit{IS})}, per esprimere l'intenzione di
  bloccare in modo condiviso uno dei nodi che discende dal nodo corrente
  \item \textbf{intent exclusive (\textit{IX})}, per esprimere l'intenzione di
  bloccare in modo esclusivo uno dei nodi che discende da quello corrente
\end{itemize}
Il lock garantisce transazioni a livello di singola collezione mentre prima
l'atomicità era garantita a livello della singola operazione. Usando
l'embedding, anche prima della v4.0, potevo gestire un sacco di modiche della
singola collezioni. Nel caso del referencing invece serviva una gestione più
approfondita delle transazioni. Essendo WT creato originariamente per un sistema
distribuito funziona solo per sistemi distribuiti, infatti avendo un solo nodo
si presuppone che si abbia una sola applicazione che accede, cosa che renderebbe
a priori più semplice la gestione.
\subsubsection{Modello contro efficienza}
\textbf{esempio a partire dalla slide 37}\\
Spesso il cambiamento del modello può comportare un miglioramento delle
prestazioni non indifferente, più del cambiamento dell'hardware (\textbf{verrà
  approfondito in live}).\\
Avere più piccoli documenti separati migliora il tempo, rendendolo lineare
(mentre un grosso documento embedding aveva tempi esponenziali nell'inserimento
delle righe).\\
Con questa soluzione poi devo sistemare le query.\\
\textbf{Quindi l'importanza del modello è decisiva dal punto di vista delle
  prestazioni}.\\
\textbf{Nelle slide brevissima parte su CouchDB inutile}.
\begin{shaded}
  \noindent
  \textit{Vengono qui aggiunte le cose dette in live.}\\
  Nella storia di NoSQL compaiono sempre grandi aziende, come Google, Amazon o
  Facebook che creano il software, fanno il paper e rendono il software open. Il
  rilascio dei codice in modo open source viene fatto in quanto in primis la
  community permette continui miglioramenti ``a costo zero'', inoltre le aziende
  in questione guadagno in primis su pubblicità (Google e Facebook in primis) e
  cloud (vedisi Amazon che fa il 90\% degli utili dal cloud). Nessuno di questi
  vende direttamente tecnologia (anche perché, esempio, Android ha un valore
  economico basso) ma la usano. Si ha quindi esigenza applicativa, non
  affidandosi a società esterne ma facendosi le cose internamente, rilasciando
  poi le cose open source per far abituare gli utenti alle proprie tecnologie,
  facendo così in modo che la gente voglia usare quelle tecnologie in ambito
  cloud. In tutto ciò si contribuisce a creare pubblicità mirate etc$\ldots$ in
  quanto il vero ``bene'' sono i dati non l'hardware o le tecnologie (e quelli
  non sono open). Si ha la \textbf{data-driven economy}, i dati sono il nuovo
  petrolio. IBM, Oracle etc$\ldots$ che vendevano invece la tecnologia ora sono
  più in crisi e hanno dovuto cambiare \textit{business}.\\
  Per progettare una review di prodotti servono informazioni in merito a chi fa
  la review (utente verificato, numero di vecchie review etc$\ldots$). In ogni
  elemento del documento salvo queste informazioni. Non è poi possibile avere
  tutte le review nello stesso documento, quindi le primissime review (che
  vengono visualizzate subito) le lascio
  nel documento principale e per le altre lascio una reference ad un altro
  documento. Le review possono anche essere usate per valutare l'affidabilità
  dell'utente ed eventuali scelte alternative dei prodotti. Si punta comunque
  all'efficienze e all'efficacia.\\
  I db documentali scalano tramite \textbf{distributed hashed table
    (\textit{DHT})}. Definendo una funzione di hash definisco implicitamente il
  dominio della funzione, posso quindi distribuire in modo efficiente sui nodi i
  vari valori di hash. Posso quindi avere diversi server con un nodo che salva
  le coppie chiave-valore con tutte le chiavi di sua responsabilità, è il
  \textbf{chord ring}. Ogni nodo quindi sa che dati deve gestire tramite la
  funzione di hash, avendo un 
  algoritmo di scelta del nodo tramite \textit{finger table}, con complessità
  logaritmica. L'aggiunta di un nodo può portare a buchi di sicurezza ma
  permette una scalabilità orizzontale incredibile.\\
  \textbf{sistemare questa parte!}
\end{shaded}
\section{GraphDB}
È il modello più ricco e complesso tra i NoSQL.\\
Un grafo, da un certo punto di vista, ha proprietà simili a quelle di un modello
ER. È infatti un insieme di nodi e di relazioni tra loro, dove i concetti più
importanti (quelle che in ER sarebbero le \textit{entità}) sono modellate come
nodi. Bisogna quindi studiare come connettere i nodi, infatti il modo con cui
le entità sono semanticamente associate si modellano come relazioni, tramite
archi.\\
Si hanno campi di applicazione dei db a grafo:
\begin{itemize}
  \item social network
  \item sistemi di raccomandazione, che suggeriscono oggetti simili a utenti
  simili
  \item ambito geografico
  \item reti logistiche (per spedizioni etc$\ldots$)
  \item transazioni finanziarie per la \textit{fraud detenction} (riconoscendo
  certi pattern sospetti, anche a livello geografico)
  \item master data management, rappresentazione unica della realtà
  \item bioinformatica
  \item controllo di autorizzazioni e accessi
\end{itemize}
\begin{esempio}
  Per esempio su Twitter avrei i nodi con gli utenti e gli archi rappresentanti
  la relazione \textit{follow}.
\end{esempio}
Si usa una \textbf{descrizione estensionale dei dati}. Si può avere infatti:
\begin{itemize}
  \item \textbf{descrizione estensionale dei dati} quando si descrivono le
  istanze via-via, descrivendo anche istanze di tipo diverso (magari utenti e
  messaggi) usando la stessa sintassi del nodo (ugualmente per gli archi). La
  semantica viene data dall'etichetta
  \item \textbf{descrizione intensionale dei dati}, che bisognerebbe inferirla
  direttamente dai dati e quindi non è applicabile
\end{itemize}
Nei RDBMS si aveva il \textit{join} basato su valori, facendo uno
\textit{scheme understanding} sulle varie tabelle, ottenendo quindi il risultato
tramite anche più operazioni di \textit{join}. Si può avere però il problema
applicativo, avendo magari\textbf{ relazioni ad anello} tra le varie tabelle
ad esempio una tabella ``persona'' che tramite una tabella ``amici'' ritorna su
se stessa, avendo la tabella ``amici'' che esplode in numerosità (magari
cercando amici di amici etc$\ldots$ complicando le cose di livello in
livello ). Avendo potenzialmente livelli infiniti di \textit{join} si va
incontro al \textbf{join bombing}, che fanno soffrire i DBMS. Si può considerare
comunque il \textbf{six separation degree}, uno studio sociologico, che dice
che con sette connessioni si può raggiungere chiunque (ora coi social ne bastano
meno). Simile è il \textbf{Kevin Bacon number}, che dice il numero medio di film
fatti tra artisti per ottenerne uno fatto con Kevin Bacon, anche in questo caso
si arriva allo stesso risultato visto con il \textbf{six separation degree}.\\
Nei sistemi a grafo abbiamo quindi gli archi mentre nei documentali avevamo
identificatori che puntavano ad altri documenti (con un modello basato sui
valori come se fossero \textit{foreign keys} dei RDBMS), comportando, nel caso
dell'embedding, task di modellazione complessi, scarsa efficienza e complessità
nel gestire dati fortemente connessi, mentre, con il referencing starei
simulando una sorta di modello a grafo. \\
I db a grafo hanno forte potenza espressiva permettendo di caratterizzare i nodi
con un certo \textit{tipo} e definire le varie relazioni tramite archi.
\begin{figure}
  \centering
  \includegraphics[scale = 0.7]{img/gdb.pdf}
  \caption{Esempio di db a grafo}
  \label{fig:gdb}
\end{figure}
Coi grafi si possono facilmente rappresentare relazioni umane, sociali,
affettive, lavorative etc$\ldots$ usando dei semplici archi. Si possono
connettere elementi tramite un'infinità di tipi di relazioni.\\
Nei nodi posso rappresentare anche entità complesse, con vari attributi, ovvero
varie \textbf{proprietà} (ovviamente ogni nodo può avere un set di attributi
diversi, avendo fortissima libertà di schema).\\
È un modello molto più immediato rispetto ad un RDBMS. Inoltre, nel caso si
abbiano potenzialmente più relazioni tra due nodi basta contare quanti cammini
orientati (che possono quindi passare per nodi intermedi diversi) esistono tra
quei nodi per ottenere la cardinalità di tali relazioni. \\
Un grafo con le proprietà nei nodi è detto \textbf{property graph}
\begin{figure}
  \centering
  \includegraphics[scale = 1]{img/gdb2.pdf}
  \caption{Esempio di db con rappresentato da un \textbf{property graph}, con
    \textit{user} come chiave e un array di valori}
  \label{fig:gdb2}
\end{figure}
È un modello simile a quello che si fa su una lavagna, è una modellazione simile
a quella umana, basti pensare alle mappe concettuali. \\
Si hanno comunque vari tipi di \textit{property graph};
\begin{itemize}
  \item quando sia nodi che relazioni possono avere proprietà espresse da
  chiave-valore, accettando quindi che i nodi abbiano un tipo
  \item altri ammettono array di valori
\end{itemize}
\textit{Si nota che invece in altri modelli NoSQL come RDF i nodi possono avere
  un tipo ma non proprietà e le relazioni non hanno proprietà, dovendo sempre
  rappresentare il tutto tramite triple ``soggetto-predicato-oggetto''}.\\
Il problema di modellazione dei grafi si ritrova quindi nella scelta di cosa
mettere come nodo e cosa come relazioni (ed eventualmente le proprietà), avendo
varie scelte modellistiche con vari e pro e contro.\\
Ovviamente anche qui ogni vendor ha il suo linguaggio, anche se ne esiste uno
\textit{standard}, chiamato \textbf{gremlin}, che più che essere un linguaggio
di interrogazione è un \textit{linguaggio di attraversamento dei grafi} (dato
che cercare sottografi e attraversare il grafo è la principale tecnica di studio
degli stessi che si ha a disposizione). Questo tipo di linguaggio viene detto
\textbf{vertex-based} che si usano per studiare la validità di predicati
tramite l'attraversamento dei nodi.\\
Si hanno vari vendor quindi e si hanno due tipi di approccio:
\begin{enumerate}
  \item \textbf{graph processing}, dove ci si concentra sul processamento dei
  dati, processamento che viene eseguito tramite i grafi 
  \item \textbf{graph storage}, dove i dati vengono memorizzati in forma di
  grafo 
\end{enumerate}
In entrambi i casi la cosa può essere fatta in modo:
\begin{itemize}
  \item \textbf{nativo}:
    \begin{itemize}
    \item \textit{native graph storage} per indicare che sono ottimizzati e
    progettati per le gestione stessa dei grafi. In questo caso comunque i dati
    vengono memorizzati in modo intelligente, memorizzando in modo vicino i nodi
    e il loro archi, facendo \textit{index free adjacency}, in modo che
    seguire gli archi uscenti da un nodo è molto più immediato. Tra gli esempi
    di db abbiamo \textbf{Neo4j}, che in memoria salva, per ogni nodo, tutti i
    suoi riferimenti, tramite pointers. In scrittura comunque si perdono
    performance, a vantaggio di una lettura molto rapida (al costo comunque di
    molto uso di RAM). Si evita il \textit{join bombing} seguendo i riferimenti
    vicini ad ogni nodo, facendo l'attraversamento
  \end{itemize}
  \item \textbf{non nativo}:
  \begin{itemize}
    \item \textit{non native graph storage} per indicare che i dati vengono
    trasformati da grafo ad un formato diverso tra cui relazionale, object
    oriented db etc$\ldots$ Si rischia comunque il \textit{join bombing}
  \end{itemize}
\end{itemize}
Potrei avere un db con una rappresentazione a tabelle (e non a grafo) studiata
come se fosse un grafo (traducendo poi le query in SQL), questo è l'approccio di
\textbf{FlockDB}.\\
Un'alternativa è avere, come nel caso di \textbf{Titan}, dove posso comunque
usare \textit{Gremlin} ma con dati memorizzati in modo colonnare.\\
Si hanno quindi due approcci principali per gestire un grafo:
\begin{itemize}
  \item \textbf{graph database}, ovvero un DBMS che gestisce in maniera
  persistente un grafo che viene usato per le transazioni. Si ha in questo caso
  la dicitura OLTP
  \item \textbf{graph compute engine}, ovvero tecnologie per l'analisi
  \textit{off line} dei grafi. Si ha in questo caso la dicitura OLAP
\end{itemize}
I graph database supportano le operazioni CRUD (come tutti i modelli NoSQL):
\begin{itemize}
  \item Create
  \item Read
  \item Update
  \item Delete
\end{itemize}
Come abbiamo visto si studiano gli attraversamenti del grafo, che sono
equiparati ai \textit{join} del modello relazionale, per fare le query in modo
performante non facendo join ma attraversando e basta. Sono inoltre costruiti
per essere usati nei sistemi transazionali (OLTP). \\
Si ha un problema grave coi grafi, motivo per cui si hanno i \textit{non native
  graph storage}: \textbf{i grafi non scalano bene e non possono essere
  facilmente frammentati}.\\
Partizionare un grafo raramente è possibile, in quanto comporta un taglio di
relazioni. Se quindi si vuole scalare bisogna passare, alla fine, ad un altro
modello, aggiungendo però costo di comunicazione tra il modello a grafo e il
nuovo modello scelto, aggiungendo un layer software.\\
\subsection{Neo4j}
Abbiamo già introdotto precedentemente Neo4j. Sappiamo che è nativo sia per il
\textit{graph processing} che per il \textit{graph storage} (sovvrendo quindi la
scalabilità).\\
Approfondiamo quindi il \textbf{linguaggio di interrogazione di Neo4j}, chiamato
\textbf{Cypher} (\textit{OpenCypher} nella versione open).\\
Cypher è definito come un \textbf{pattern-matching query language}. È un
linguaggio simil-umano, espressivo, dichiarativo (si dice il cosa e non il
come), che consente operazioni di aggregazione, sorting e di limit (trovare ad
esempio le $n$ top key etc$\ldots$) e permette di aggiornare il grafo.\\
Un formato in modo testuale viene rappresentato con le parentesi tonde per i
nodi e le quadre per gli archi. Le relazioni orientate vengono indicate tramite
``->'' o ``<-''.
\begin{esempio}
  Tramite:
  \begin{center}
    \textit{(c)-[:KNOWS]->(b)-[:KNOWS]->(a), (c)-[:KNOWS]->(a)}
  \end{center}
  con la ``,'' che significa $\land$.\\
  Equivalentemente potevamo avere:
    \begin{center}
    \textit{/c)-[:KNOWS]->(b)-[:KNOWS]->(a)<-(c)-[:KNOWS]}
  \end{center}
  per rappresentare:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 1]{img/neo.pdf}
  \end{figure}
  Vediamo quindi un'interrogazione classica in Cypher, dove si cercano tutti
  gli utenti tali per cui conosce qualcuno $a$ che conosce qualcuno $b$ e che
  sono etichettati come ``Michael'', ritornando $a$ e $b$: 
  \begin{minted}{cypher}
    MATCH(c:user)
    WHERE (c)-[:KNOWS]->(b)-[:KNOWS]->(a),
          (c)-[:KNOWS]->(a), c.user="Michael"
    RETURN a, b
  \end{minted}
  Il risultato sarà una \textbf{tabella} che contiene tutte le coppie $a$ e
  $b$. Avrà dei dati ridondati a causa della dualità intrinseca di avere $a$ che
  conosce $b$, cosa che probabilmente porta ad avere che $b$ conosce $a$.
\end{esempio}
Quindi il risultato \textbf{non sempre} è un grafo (mentre nel documentale era
sempre un documento e nel relazionale una tabella) ma può anche essere altro, ad
esempio una tabella. Non è quindi sempre possibile concatenare query in quanto
potrei non avere un grafo in uscita.\\
Si hanno comunque clausole (coprendo le operazioni CRUD):
\begin{itemize}
  \item \textit{where} per imporre criteri di filtraggio tramite
  pattern-matching dei risultati
  \item \textit{create/create unique}, per creare nodi e relazioni
  \item \textit{delete}, per cancellare nodi, relazioni e proprietà
  \item \textit{set}, per impostare dei valori alle proprietà
  \item \textit{foreach}, per fare un update per elementi del grafo posti in una
  lista (per ogni nodo, per ogni arco etc$\ldots$)
  \item \textit{union}, per unire i risultati più query
  \item \textit{with}, per concatenare query propagando i risultati in pipe
\end{itemize}
\subsection{Grafi e modello relazionale}
Compariamo ora il modello a grafi con il modello relazionale.\\
Per farlo sfruttiamo un esempio: 
\begin{esempio}
  Vogliamo costruire un modello relazionale per la gestione di una server
  farm. Si ha che l'utente accede ad una applicazione che gira su una VM ed
  accede ad un db (primario o secondario). I server che ospitano le VM sono
  installati su sistemi rack controllati da un sistema di load balancing.\\
  Una tipica attività è quella di controllare se ogni elemento del sistema
  funzioni e, in caso di guasto, capire cosa non stia funzionando. Per farlo si
  hanno dei sensori, sia software che hardware, che monitorano le varie
  componenti. \\
  Modello uno schema del mio sistema collegando in una mappa concettuale le
  varie componenti che comunicano tra loro o che sono in relazioni (anche solo
  per dire che un server è in un certo rack). Si modellano così le varie
  istanze. Tale modellazione relazionale può avere il seguente ER:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{img/er.pdf}
  \end{figure}
  Ogni elemento ha l'attributo $on$ per indicare se i sensori dicono che la
  componente funzioni o meno, attributo che verrà usato per le query.\\
  Qualora quindi un'utente mi dica che una certa applicazione non funziona devo
  risalire step by step al server, eventualmente al rack, al db etc$\ldots$,
  avendo una quantità di join non indifferente (6 \textit{join} diversi nel caso
  peggiore).\\
  Una rappresentazione a grafo sarebbe molto più semplice, avendo dei semplici
  cammini anziché avere tutti quei \textit{join}. Si avrebbe quindi un nodo per
  singola componente reale con le relazioni tra le varie singole componenti. \\
  Pensiamo quindi, per esempio, alla seguente query:
  \begin{center}
    Trovare gli asset (server, app, machine virtuali) in stato down per l’utente
    user 3
  \end{center}
  \begin{minted}{cypher}
    START user = node:users(name = "user 3")
    MATCH (user)-[*1..5]-(asset)
    WHERE asset.status! = "down"
    RETURN DISTINCT asset
  \end{minted}
  Parto quindi dal nodo che come nome ``user 3'', trova tutti quegli asset con
  distanza da 1 a 5 rispetto al nodo utente, arrivando quindi fino a 5 livelli
  di profondità. Se tale asset è in down viene restituito una sola volta.
\end{esempio}
\subsection{Ancora su Neo4j}
Tutta la teoria e la letteratura sui grafi viene applicata ai db a grafo, dai
cammini minimi alle clique. Il vero problema resta la modellazione e la scelta
del DBMS. Approfondiamo quindi ancora di più Neo4j.\\
Abbiamo già visto che non ha meccanismi nativi di frammentazione, avendo scelto
di sfruttare la \textit{index free adjacency} e la modellazione a grafo, non
permettendo scalabilità. In compenso garantisce varie caratteristiche
funzionali:
\begin{itemize}
  \item permette le transazioni con tutte le proprietà ACID, con meccanismi di
  locking
  \item garantisce la disponibilità
  \item garantisce la recoverability
\end{itemize}
In merito alla scalabilità, nonostante le limitazioni sopra esposte,
garantisce l'\textbf{alta disponibilità}. L'idea è che è possibile andare a
definire un cluster di nodi Neo4j, alcuni chiamati \textbf{core server}, ovvero
nodi per cui tutte le scritture sono fatte in contemporanea, essendo quindi
sincronizzati. Si hanno poi gli altri nodi che sono chiamati \textbf{replica
  server}, che sono di sola lettura, che vengono creati tramite meccanismi di
replica simili a quelli già trattati. I \textit{replica server} vengono anche
usati per reporting analysis etc$\ldots$. Fatta una query essa comunque non
viene distribuita ma eseguita sul nodo più vicino. Tutto questo sistema è detto
\textbf{casual clustering}.\\
Si ha una bassa \textbf{latenza} nelle query in quanto si usa un indice per
risalire al primo nodo e poi si procede coi cammini, avendo quindi che le
performances non dipendono dalla grandezza del dataset ma solo da quali sono i
dati richiesti dalla query.
\section{Modelli poliglotta}
In situazioni reali, ad esempio un e-commerce, posso avere i dati salvati in
vari DBMS, sia relazionali che NoSQL, a seconda dei dati che contengono (magari
avrei le raccomandazioni in un db a grafo con Neo4j, il catalogo prodotti in un
documentale come MongoDB, le sessioni utente in un db key-value come Redis,
dati finanziari e reporting in un RDBSM, per i log delle attività un modello
NoSQL colonnare etc$\ldots$). Dover gestire tutti questi modelli diversi è molto
completo, ma sensato in base al tipo di dati. Ho quindi una sorta di
\textbf{mondo poliglotta}.
\subsection{ArangoDB}
In questo mondo poliglotta complesso ci arriva in soccorso un particolare DBMS:
\textbf{ArangoDB}.\\
Il DBA dovrebbe conoscere tutti i modelli in uso e tutti i linguaggi associati,
essendo anche in grado di gestire i vari conflitti. ArangoDB introduce il
concetto di \textbf{db poliglotta}, supportando i seguenti modelli:
\begin{itemize}
  \item key-value
  \item documentali
  \item a grafo
  \item a ricerca full-text (come fa anche \textbf{Elastic Search})
\end{itemize}
Rispetto ai modelli precedentemente descritti a grafo con storage non nativo,
tipo \textbf{Titan}, che appunto accettano solo un linguaggio di query a grafo,
il linguaggio di query di ArangoDB, chiamato \textbf{AQL (\textit{Arango Query
    language})}, garantisce la possibilità di interrogare dati in formato json
(quindi documentale), a grafo, in formato key-value o per il full text search.\\
Ad alto livello Arango organizza i dati in database e collezioni (per fare un
parallelo schemi e tabelle), dove le collezioni memorizzano documenti o
similari. I documenti, come già accennato, sono documenti json di profondità
arbitraria (sub-object, sub-arrays etc$\ldots$), non avendo alcun limite da
questo punto di vista. Nel caso semplice tali documenti sono omogenei, avendo
stessi attributi e tipi, e effettuare una query AQL è molto semplice:
\begin{minted}{sql}
  FOR c IN categories
  FILTER c._key IN [ 'books', 'kitchen' ]
  RETURN c

  FOR c IN categories
  SORT .title
  RETURN { _key: c._key, title: c.title}
\end{minted}
Avendo un linguaggio simile a uno di programmazione.\\
Abbiamo quindi, nella prima query, che selezioniamo tutti i $c$ in `$categories$
(che è una collezione) trovando solo le chiavi che sono o ``book'' o ``kitchen''
ritornando $c$. Nella seconda selezioniamo selezioniamo tutti i $c$ in
`$categories$, ordiniamo i risultati in base al titolo e ritorniamo chiave e
titolo.\\
Posso comunque avere documenti eterogenei, con attributi diversi. Possiamo
comunque interrogare usando il modello a grafo e tali query mostrano quali
documenti sono collegati (direttamente o indirettamente) ad altri documenti,
specificando il cammino per ottenere il collegamento. Queste connessioni tra
documenti sono chiamate \textbf{edges} che sono salvati in ``edge
collections''. Gli \textit{edges} hanno sempre due attributi che si riferisocno
ai nodi entranti e uscenti:
\begin{itemize}
  \item \textit{\_from}
  \item \textit{\_to}
\end{itemize}
Avendo sempre archi direzionati del tipo:
\[\_from \to \_to\]
\textbf{anche se si possono effettuare query anche nell'ordine opposto}.\\
\begin{esempio}
  Preso il documento \textit{json}:
  \begin{minted}{json}
    { "_key" : "sofia", "_id" : "employees/sofia"}
    { "_key" : "adam", "_id" : "employees/adam"}
    { "_key" : "sarah", "_id" : "employees/sarah"}
    { "_key" : "jon", "_id" : "employees/jon"}
  \end{minted}
  posso avere un \textbf{edge}, per una certa relazione, del tipo:
  \begin{minted}{json}
    { "_from" : "employees/sofia", "_to" : "employees/adam"}
    { "_from" : "employees/sofia", "_to" : "employees/sarah"}
    { "_from" : "employees/sarah", "_to" : "employees/jon"}
  \end{minted}
  \textit{(Potrei comunque continuare a interrogare il primo json come se fosse
  un documentale)}
\end{esempio}
Rispetto a quanto visto per i modelli a grafo cambia drasticamente la
scalabilità.\\
Si hanno vari metodi di scalabilità, avendo le modalità di deploy:
\begin{itemize}
  \item a singola istanza
  \item master/slave, scritture su master e repliche su slave
  \item active failover
  \item cluster
  \item multiple datacenters
\end{itemize}
Approfondiamo soprattutto gli ultimi tre.
\subsubsection{Active failover}
Nel modello master/slave quando il master fallisce lo slave non può essere
sostituito e quindi si è pensato al modello \textbf{active failover}, dove
esistono delle istanze \textit{single-server}, chiamate \textbf{leader}, che
sono in lettura e scrittura e delle istanze, sempre \textit{single-server},
chiamate \textbf{followers}, che non sono altro i vecchi slave, passivi e non
scrivibili. Il leader invia le operazioni di scrittura ai followers che
replicano in modo asincrono i dati tramite la cosiddetta \textbf{Write-Ahead Log
  (\textit{WAL})}, ovvero il leader prima salva su un file di log e poi sul
disco e tale file di log, tramite replica incrementale viene aggiornato, tramite
un meccanismo di replica chiamato \textbf{apache mesos}. Esiste poi almeno un
\textbf{agency} che controlla le configurazioni di leader e followers e
controlla che siano tutti disponibili. Qualora l'agency si accorgesse che un
server leader non è più disponibile promuove, con un meccanismo elettivo, un
follower a leader. Resta quindi una sorta di master/slave ma con gli agency che
procedono con il controllo e la rielezione.
\subsubsection{Cluster}
Il cluster è un'evoluzione, avendo più \textbf{coordinator} che si connettono
con i client. I coordinator effettuando le query e gli \textbf{agency} non si
occupano di altro se non l'elezione degli stessi e della sincronizzazione dei
servizi per l'intero cluster. Si hanno poi i \textbf{DB server}, di tipo slave,
che, in caso di fallimento, non comporta problemi in quanto l'agency attua le
solite politiche di aumento repliche, se esse sono previste, bilanciando
nuovamente i dati in caso di sharding.\\
Lo sharding avviene in partizioni da almeno 40gb, con un meccanismo simile a
quello di MongoDB: ogni server avrà associato uno shard, e ogni server, avendo
ciascuno circa 3 repliche, avrà le repliche di altri server (oltre alla sua
stessa replica, di cui è \textbf{leading shard}) (??????). Questo sistema
permette di interrogare quasi 
direttamente tutto sullo stesso server (avendo molte repliche) e permette, in
caso di server non funzionante, di poter stabilire che un altro server diventa
il \textbf{leading shard} per un certo frammento (tramite meccanismi soliti di
elezione tramite gli agency), replicando per avere comune le tre repliche tra i
vari server e potendo continuare a fare interrogazioni.\\
Si hanno due soluzioni per la scrittura:
\begin{enumerate}
  \item il protocollo \textbf{read once write all (ROWA)} che obbliga il
  coordinator a scrivere su tutte le repliche
  \item il coordinatore scrive solo dove si ha la copia primaria e poi si hanno
  le repliche, è la soluzione \textbf{oneshard} (che era chiamato \textbf{write
    concern} in MongoDB)
\end{enumerate}
Arango quindi scala molto bene.
\subsubsection{Cluster to cluster}
L'ultimo livello è il \textbf{cluster to cluster}, una soluzione asincrona di
tipo \textbf{one way} in cui prendo un cluster e lo copio interamente in un
altro, usando una coda come \textbf{kafka}. La replica è one way per cui scarico
dal primary alla replica o viceversa in tempi diversi, non
contemporaneamente. Non è pensato per fare una replica del single-server (userei
il master/slave).
\section{Modello key-value}
È un modello abbastanza semplice dove si hanno appunto rapporti chiave-valore. I
valori sono considerati come dei blob, non essendo accessibili/interpretabili
direttamente dal db(??????). Si hanno poche operazioni:
\begin{itemize}
  \item inserire un valore data la chiave
  \item trovare un valore data la chiave
  \item modificare un valore data la chiave
  \item cancellare una chiave e il suo valore
\end{itemize}
È inoltre possibile associare, in alcune soluzioni come \textbf{Redis}, un tipo
ad un valore, per esempio \textit{string, integer, list etc$\ldots$}.\\
Si ha accesso veloce ai dati tramite strutture di hash e posso avere scalabilità
orizzontale. I modelli key-value funzionano solitamente \textit{in memory},
anche nel caso della condivisione dati.\\
Dal punto di vista della nomenclatura si ha:
\begin{table}[H]
  \centering
  \begin{tabular}{c|c}
    relazionale & key-value\\
    \hline
    tabella & bucket\\
    riga & key-value\\
    id-riga & key
  \end{tabular}
\end{table}
Tra i vari DBMS abbiamo:
\begin{itemize}
  \item Redis
  \item Memcached
  \item Riak KV
  \item Hazelcast
  \item Ehcache
\end{itemize}
\subsection{Redis}
nato del 2009 da Salvatore Sanfilippo mentre stava lavorando su una soluzione di
realtime web analytic con mysql. Nasce a causa dei limiti del modello
relazionale, non riuscendo a fare le interrogazioni in realtime. Assunto da
VMware continua a lavorare su Redis fino a mettesi in proprio:
\textit{Redislab} (con la parte di ricerca e sviluppo in Sicilia e la sede
principale negli USA).\\
Redis può gestire chiavi espresse in ASCII. I valori primitivi sono le
\textit{string} ma si hanno vari container di stringhe:
\begin{itemize}
  \item hashes
  \item lists
  \item sets
  \item sorted sets
\end{itemize}
Non si può sempre fare una ricerca per valore all'interno di questi elementi.\\
Sono stati poi aggiunti i \textbf{moduli} per fari formati di file, per
memorizzare file \textit{json} ma anche per \textit{video in streaming}. Offre
quindi blob più strutturati per memorizzare i dati, alcuni a pagamento.\\
Si ha una versione distribuita enterprise, a pagamento. Si possono fare repliche
e avere una soluzione in cluster con un certo numero di shard.\\
Studiamo quindi come avviene lo sharding.\\
Bisogna stabilire una \textit{policy} per capire come dividere i dati e si è
scelto di dividere le chiavi in base al valore di hash. Si può, volendo, anche
dividere le chiavi in base a delle RegEx.\\
Inoltre bisogna capire dove distribuire i dati e si hanno due soluzioni:
\begin{enumerate}
  \item quella di default, detta \textbf{dense}, dove il meccanismo di sharding
  è gestito tramite un meccanismo incrementale, ovvero quando si riempie la
  memoria della macchina si crea un'altro shard su un'altra e così via
  \item una detta \textbf{sparse}, dove, avendo già definito il numero di nodi,
  vado a popolarli fin da subito
\end{enumerate}
Si ha l'alta disponibilità in base a due soluzioni (???):
\begin{enumerate}
  \item una soluzione, detta \textbf{Redis replica of setup} in cui si hanno dei
  nodi che formano il cluster su un datacenter su cui si effettuando le
  operazioni e altri nodi che formano altri cluster su altri datacenter verso
  cui sincronizzo. È quindi un master/slave, replicando per disaster-recovery
  \item una soluzione, chiamata \textbf{active-active setup}, in cui posso
  scrivere e leggere sui cluster presenti in due datacenters diversi (ma magari
  nello stesso rack, parlando di \textbf{Redis cluster with rack zone
    awareness}) avendo in entrambi i datacenter sia i db principali che le
  repliche. Magari avendo protocolli ROWA etc$\ldots$
\end{enumerate}
Di recente si è aggiunta la possibilità di rendere persistenti dati anche se il
loro punto di forza resta la gestione \textit{in memory} (piuttosto che andare a
costruire dei sistemi di \textit{cache} etc$\ldots$).
\section{Wide column}
Studiamo ora il modello \textbf{wide column store}.\\
Anche se storicamente non è così il modello \textit{wide column} può essere
visto come un'evoluzione del modello \textit{key-value} (è nato prima il modello
\textit{wide column}). L'idea è che accanto ha la chiave non ho un valore/blob
ma una riga con degli attributi mono-valore. Si ha quindi una via di mezzo tra
il modello \textit{key-value}, essendo possibile fare interrogazioni su queste
righe e quindi all'interno delle colonne, e il modello documentale, non
riuscendo a garantire la stessa potenza descrittiva (i documenti sono comunque
più espressivi essendo più strutturati, permettendo sub-array e sub-document).\\
Tra le implementazioni troviamo:
\begin{itemize}
  \item \textbf{BigTable}, il primo modello \textit{wide column} introdotto da
  Google
  \item \textbf{Hbase}
  \item \textbf{Cassandra}, che però viene ritenuto spesso \textit{key-value} e
  anche loro stessi si definiscono \textit{key-value}, basando la loro
  distribuzione sulle \textbf{distributed hashed table (\textit{DHT})}
\end{itemize}
\subsection{BigTable}
L'idea dietro \textbf{BigTable} è quella di una mappa multidimensionale
ordinata, persistente e sparsa, ovvero si ha una mappa indicizzata tramite tre
informazioni:
\begin{enumerate}
  \item row key, di tipo \textit{string}
  \item column key, di tipo \textit{string}
  \item timestamp che caratterizzano ogni singolo valore, compreso la column key
  (da immagine sembra così perlomeno). Sono di tipi \textit{int64}
\end{enumerate}
All'interno si hanno poi varie \textbf{column family} con i vari dati,
memorizzati come \textit{string}.\\
BigTable è molto usato in ambito web grazie alla sua capacità di memorizzare
contenuti HTML etc$\ldots$\\
Grazie ai timestamp si garantisce un controllo di concorrenza basato sul
multiversioning. Possiamo dire che i sistemi che implementano \textit{wide
  column} con timestamp supportano transazioni ACID compliant tramite
\textbf{MultiVersion Concurrency Control (\textit{MCC o MVCC})}.\\
Un insieme di righe viene definito \textbf{tablet}. Abbiamo visto che ogni riga
può avere diversi \textbf{column family} e si ha che ciascun column
family può avere diversi \textbf{qualifier} e per ogni qualifier più valori
diversi. La libertà dello schema deriva che ogni column family può avere un
numero diverso di qualifier. Si ha quindi l'approccio \textit{key-value} per
l'accesso tramite chiave e si ha un sistema di indicizzazione per migliorare le
prestazioni. Ogni column family ha un nome, una \textit{string}, è può contenere
altre colonne, che a loro volta appartengono ad una column family, specificata
tramite \texttt{familyName:columnName}.  Un esempio nella figura \ref{fig:col}.
\begin{figure}
  \centering
  \includegraphics[scale = 0.8]{img/col.pdf}
  \caption{Esempio di modello \textit{wide column} implementato da BigTable}
  \label{fig:col}
\end{figure}
Non è semplice facile le range query a causa del meccanismo di hashing.\\
Il timestamp (anche se può essere usato altro) garantisce un \textbf{version
  number} univoco.
\subsection{Hbase}
La versione distribuita di BigTable venne rinominata \textbf{Hbase} (dove $H$
sta per \textbf{Hadoop}).\\
In questo caso i dati sono divisi in tables e ogni table è composta di colonne
che sono a loro volta raggruppate in column family (che possono avere un numero
variabile di colonne, che erano i qualifier in BigTable). Ovviamente supporta il
multiversioning.\\
Un'istanza di Hbase è formata da varie righe, identificate tramite una chiave, a
cui sono associate, per ciascuna riga, varie column family, una per ogni tipo di
dato. La column family viene rappresentata tramite un insieme di coppie
attributo-valore e i valori possono avere timestamp diversi, specificati tramite
\texttt{@timestamp} (\texttt{@ts=value}).\\
Si possono avere le stesse column family, dal punto di vista del nome, in più
righe ma queste possono avere un insieme diverso di valori, garantendo che la
table è \textbf{sparse}, rivelandosi utile per il mapping ``uno a tanti''. I
dati sono tutti \textit{bytes}. Un esempio nella figura \ref{fig:col2}.
\begin{figure}
  \centering
  \includegraphics[scale = 0.8]{img/col2.pdf}
  \caption{Esempio di istanza di Htable, dove si nota quanto detto, avendo le
    stesse column family (``info'' e ``ruolo'') ma con all'interno anche
    attributi diversi (esempio ``consulente'')}
  \label{fig:col2}
\end{figure}
Il modello colonnare non presta attenzione ai \textit{join}, che in questo caso
implicherebbero scorrere l'intera tabella, ma si concentra sul
\textbf{denormalizzare} ovvero a replicare le informazioni nel momento in cui si
crea una column family nuova (???).\\
Quindi lo schema è formato dalle tables e dalle column family, \textbf{le
  colonne non fanno parte dello schema}.\\
Hbase implementa quindi le \textbf{dynamic Columns} in quanto ogni riga ha
differenti colonne, i quali nomi sono codificati nella riga, non facendo parte
dello schema.\\
Il versioning è essenziale per garantire lo storico delle informazioni.\\
SU Hbase ho alcune operazioni fondamentali:
\begin{enumerate}
  \item aggiungere valori
  \item ricercare per valore esatto (e non in modo range-based)
\end{enumerate}
Vediamo quindi come si distribuisce un cluster Hbase. Si hanno tre componenti
principali (ricordando che si basa su Hadoop), avendo un'architettura
master/slave: 
\begin{enumerate}
  \item un master, detto \textbf{HbaseMaster}
  \item tanti region server, detti \textbf{HRegionServer}, che sono i vari slave
  \item il client, detto \textbf{Hbase client}, che dialoga con il singolo
  master 
\end{enumerate}
Nel dettaglio una \textit{region} è un sottoinsieme di righe della tabella, che
vengono partizionate orizzontalmente tramite regole standard di
frammentazione (partizionamento che viene fatto in automatico). Quindi un
\textit{HRegionServer} gestisce un insieme di righe di una o più tabelle e la
sincronizzazione (lettura e scrittura) avviene tramite file di log. Il master
coordina gli slave, assegna le region, riconosce fallimenti degli slave
etc$\ldots$.\\
Il client quindi fa la richiesta al master il quale cerca il dato e sviluppa
l'operazione all'interno della singola region. Nel momento in cui il region
server scrive i dati, attraverso il sistema distribuito chiamato \textbf{Hadoop
  Distributed File System (\textit{HDFS})}, viene fatta la replica e avviene la
sincronizzazione dei dati. Potrei avere più master coordinati tra loro. Il
coordinamento tra i master e i region server avviene tramite il meccanismo di
\textbf{ZooKeeper}, ovvero un orchestratore software. HDFS permette di avere la
memorizzazione orizzontale dei dati e tramite il \textbf{write ahead log
  (\textit{WAL})} prima scrivo sul log e poi sul disco, garantendo la
ricostruibilità dei dati in caso di guasto e garantendo che prima o poi i dati
saranno allineati. ZooKeeper quindi coordina master e region server per
lettura/scrittura mentre HDFS consente l'allineamento e la memorizzazione su un
file system distribuito.\\
Hbase ha un linguaggio di query poco ricco, potendo fare solo una
\textit{select} di dati con un certo valore. Si può comunque fare la ricerca sia
sulle righe che su alcuni attributi (definiti precedente come ``chiave''). Il
sistema di ricerca è estremamente performante.
\subsection{Cassandra}
Come abbiamo detto \textbf{Cassandra} lo studiamo come modello \textit{wide
  column} anche se loro stessi si definiscono \textit{key-value}, essendo
sostanzialmente tale ma usando concetti dei modelli \textit{wide column}.\\
Cassandra è l'evoluzione open source del db NoSQL utilizzato da Facebook e
risulta interessante perché implementa un sistema di distribuzione
completamente diverso da quello master/slave di Hbase.\\
In cassandra esiste il concetto \textbf{column family}, definito all'interno di
un \textbf{key space}, come insieme di coppie
key-value e quindi una column family altro non è che una tabella con le coppie
key-value come righe. Il concetto di column family quindi varia rispetto a
quello di Hbase. Una riga è una collezione di colonne con un nome (sono quindi
come le righe di una tabella relazionale). La chiave corrisponde al nome della
colonna e ogni riga (che è appunto usa serie di coppie key-value definite come
colonne) contiene almeno una colonna. Un esempio di modellazione è visibile alla
figura \ref{fig:cas}.
\begin{figure}
  \centering
  \includegraphics[scale = 0.8]{img/cas.pdf}
  \caption{Esempio di due righe della stessa column family di Cassandra con
    due chiavi e tre colonne (con i tre column name ``nome'', ``id'', ``time'')}
  \label{fig:cas}
\end{figure}
Si possono aggiungere attributi a piacere e modificarli (mentre in Hbase si
creavano/modificavano le column family). Se i valori corrispondenti a tutte le
column name sono uguali a ``-'' si specifica che essi non sono usati. Posso
anche specificare di avere un singolo valore non in uso sempre con ``-''.
\newpage
Il linguaggio di query di cassandra è chiamato \textbf{Cassandra Query language
  (CQL)} molto simile ad SQL. Vediamo un esempio di creazione di tabella (dove
per ogni attributo si specifica il tipo), con la
specifica di una primary key:
\begin{minted}{sql}
  CREATE TABLE users(
    email varchar,
    bio varchar,
    birthday timestamp,
    active boolean,
    PRIMARY KEY (email)
  )
\end{minted}
(si specifica che i campi di tipo timestamp sono specificati in millisecondi).\\
Per intenerimento dati si procede in modo simile, anche qui, ad SQL:
\begin{minted}{sql}
  INSERT INTO users(email, bio, birthday, active)
  VALUES('example@example.com',
         'esempio',
         516513600000,
         true)
\end{minted}
Inutile dire che anche per le query si ha un modello simile ad SQL dove le
\textit{select} leggono uno o più records dalle column family di Cassandra
ritornando un insieme di righe:
\begin{minted}{sql}
  SELECT * FROM users;

  SELECT email FROM users WHERE active = true;
\end{minted}
La particolarità di Cassandra è l'architettura.\\
Se BigTable usava:
\begin{itemize}
  \item Column families
  \item Memtables
  \item SSTables
\end{itemize}
e altri DBMS come \textbf{Amazon Dynamo} dove si ha (???):
\begin{itemize}
  \item hashing consistente per mantenere la replica
  \item partizionamento e replica
  \item routing \textit{one-hop}
\end{itemize}
in Cassandra i ha un'architettura distribuita \textit{Peer-to-Peer} che si basa
sostanzialmente su una modalità simile a \textbf{distributed hash tables
  (\textit{DHT})}. Cassandra  implementa un meccanismo in cui si garantisce
l'\textbf{elasticità} in modo \textbf{trasparente}, aggiungendo i vari nodi in
modo molto efficace e performante. Per farlo si prende lo spazio di
indirizzamento delle chiavi e si assegna l'organizzazione prendendo lo spazio di
indirizzamento, dividendolo nei nodi disponibili. Tramite ZooKeeper si
garantisce anche in questo caso la ridondanza, supportando anche la \textbf{rack
awareness} (dati possono essere replicati tra diversi rack per ``proteggersi''
da guasti di macchine/rack). Inoltre non si ha un \textbf{single point of
  failure}.\\
Analizzando nel dettaglio il partizionamento si ha che i nodi sono divisi in
modo logico in una forma ad anello, detta \textbf{ring topology}. Si usano
quindi i valori hash delle chiavi, associate alle partizioni dati, per assegnare
la chiave e i dati collegati ad un nodo dell'anello. L'hashing viene limitato ad
un certo valore massimo per permettere la struttura ad anello (banalmente quello
che si potrebbe fare anche con una semplice operazione $mod$) e si ha che i nodi
con meno carico si ``spostano'' sull'anello per alleggerire quelli con più
carico (inserendo un nodo in modo da ``spezzare'' i dati in carico ad un certo
nodo).
\begin{figure}
  \centering
  \includegraphics[scale = 0.7]{img/cas3.pdf}
  \caption{Esempio di inserimento di un nodo (``D'') nell'architettura ad
    anello che viene posto, tramite lo studio sull'hash della chiave, tra il
    nodo ``B'' e il nodo ``C''. Con ``valori di X'' si intende che quei dati si
    trovano nel nodo ``X''}
  \label{fig:cas3}
\end{figure}
L'aggiunta di un nodo influisce solo sul nodo precedente e il nodo
successivo. Spesso i nodi vengono replicati almeno 3 volte e viene fatto nei
primi 3 nodi successivi al nodo di cui si sta facendo la replica (nell'esempio
sopra, ipotizzando voler per semplicità fare solo due repliche di ``A'', essere
sarebbero su ``B'' e ``d''). Le repliche di un nodo vengono fatte ad ogni
scrittura dello stesso. Sapendo che ogni nodo monitora il successivo, qualora
questo sia guasto, si può chiedere a quello successivo ancora, garantendo che i
dati non vengono persi, il quale a sua volta replicherà su un nodo successivo
all'ultimo sui cui aveva repliche per avere sempre le 3 repliche. \\
Per monitorare i guasti si usano i \textbf{protocolli gossip}. L'idea è appunto
quella che un nodo chiede al successivo e al successivo ancora se sia vada tutto
bene, ottenendo, di passaggio in passaggio, una conoscenza completa e robusta
dello stato del cluster (ricordando che essendo una rete \textit{Peer-to-Peer}
non si ha alcun master). In caso di failure si procede alla riscrittura dei
dati sul nodo successivo tramite il file di log.\\
Le operazioni di write sono quindi \textbf{write ahead log}, prima scrivo quindi
sul \textbf{commit log} e poi sulla tabella persistente.\\
Parliamo quindi di consistenza dei dati.\\
Cassandra nel suo linguaggio offre la \textbf{read/write consistency} avendo
delle politiche per cui si hanno letture/scritture consistenti:
\begin{itemize}
  \item read consistency: la lettura è data per certa se un nodo risponde,
  politica \textbf{ONE to ALL} o, altrimenti, se un certo numero di nodi è
  d'accordo prima di ritornare il risultato della lettura
  \item write consistency: la scrittura è data per certa se un certo numero di
  nodi viene aggiornato prima di considerare la scrittura valida, politica
  \textbf{ANY to ALL}, oppure se
  almeno un nodo mi risponde o anche se tutti i nodi rispondono
\end{itemize}
Si ha inoltre una politica di \textbf{quorum} per il calcolo della
``maggioranza'' (per la quale anche possono avvenire read/write consistency) che
si basa sul \textbf{replication\_factor}. La richiesta di consistenza viene
fatta nelle query:
\begin{minted}{sql}
  INSERT INTO table (column1,...)
  VALUES (value1,...)
  USING CONSISTENCY ONE
\end{minted}
Dove per esempio si specifica, con \texttt{USING CONSISTENCY ONE} che basta la
conferma di un nodo per validare l'operazione. Potrei usare poi  \texttt{USING
  CONSISTENCY QUORUM}, che si basa sul valore $k$ del
\textit{replication\_factor}, che stabilisce che servono almeno $k$ nodi che
validino l'operazione. Con \texttt{USING CONSISTENCY ALL} specifico che tutti
i nodi devono confermare l'operazione.\\
Il problema è che se chiedo un solo nodo che verifichi l'operazione rischio di
avere inconsistenza, avendo i nodi precedenti che non sono stati magari in grado
di comunicare a tale nodo di aver ottenuto i dati.\\
Posso avere poi \texttt{USING CONSISTENCY ANY}, specificando anche il booleano
\textit{hinted\_handoff\_enable}, che autorizza la scrittura anche se si ha il
nodo offline in quanto un'altro nodo prende la richiesta in carico e, non appena
il nodo originale torna accessibile, ne trasferisce i dati, per procedere dopo
la validazione del primo nodo che risponde.\\
In merito alle operazioni di \textit{delete} si ha che esse semplicemente
rendono il dato non disponibile, essendo più veloce cambiare un flag piuttosto
che cancellare. A causa di ciò comunque si creano nelle tabelle fisiche dei
problemi di spazio, quindi periodicamente si fanno dei \textbf{merge}, ovvero
ogni singolo nodo procede alla ``compattazione'' dei propri dati, sovrascrivendo
i valori non più disponibili.\\
Per assicurare la sincronizzazione dei nodi e per evitare perdita di consistenza
si utilizzano dei \textit{checksum} per comparare i dati di un nodo con quelli
dei successivi. Per effettuare questo controllo, nel dettaglio, vengono usati
degli hash-tree detti \textbf{Merkle tree} nel seguente modo:
\begin{itemize}
  \item vengono mandati degli snapshot dei dati ai nodi successivi
  \item creati e trasmetti ad ogni ``compattazione'' principale (non lo dice ma
  c'è scritto questo nella slide ????)
  \item se due nodi prendono uno snapshot con un intervallo pari a\\
  \texttt{TREE\_STORE\_TIMEOUT} allora i due snapshot sono comparati e in caso
  di successo i dati vengono sincronizzati
  
\end{itemize}
Vediamo nel dettaglio le operazioni di lettura.\\
Le letture inconsistenti vengono fatte usando \texttt{ANY}, posso poi
specificare che mi risponda un nodo con \texttt{ONE} etc$\ldots$, come per le
scritture.\\
Quindi in lettura, i nodi vengono interrogati fino a quando il numero di nodi
che rispondono con il valore più recente non raggiunge un livello di coerenza
specificato da \textbf{ONE to ALL}. Si ha quindi che:
\begin{itemize}
  \item se il livello di consistenza richiesto non
  viene raggiunto i nodi vengono aggiornati con il valore più recente che viene
  quindi restituito
  \item se viene raggiunto il livello di coerenza, il valore viene restituito e
  tutti i nodi che riportavano valori vecchi vengono aggiornati
\end{itemize}
Si hanno quindi, per esempio:
\begin{minted}{sql}
  SELECT * FROM table USING CONSISTENCY ONE  
\end{minted}
\section{Considerazioni finali}
Si è notato come bene o male la scelta di algoritmi di gestione siano alla fine
sempre gli stessi, variano invece al più le architetture (master/slave
\textit{Peer-to-Peer}) ma i vari modelli NoSQL restano comunque da un certo
punto di vista (in merito ad eventuali repliche, distribuzione etc$\ldots$)
tutti simili tra loro. La differenza di modello invece consente la scelta di un
certo sistema NoSQL in base al tipo di dato che bisogna inserire nel
db. Un'altra differenza è il linguaggio usato e la facilità d'uso dello
stesso (e delle altre tecnologie legate al DBMS).\\
La scalabilità è un'altro punto da considerare durante la scelta.
\begin{shaded}
  \textit{Vengono qui aggiunte le cose dette in live.}\\
  \textbf{DynamoDB}, essendo \textit{in memory} e quindi non persistente, viene
  usato per associare su Amazon ogni persona al suo carrello, che deve essere
  sempre disponibile mentre navigo nell'e-commerce. Sono altri sistemi che poi
  manipolano il carrello stesso ma la sola associazione è gestita, in modo
  estremamente performante, tramite DynamoDB, \textit{in memory}.\\
  Parliamo di \textbf{knowledge graph}. Studiati dai creatori di Google nel
  1996. Nel 2000 poi Tim Berners-Lee introduce il \textit{semantic web}. Si ha,
  in un motore di ricerca:
  \begin{itemize}
    \item crawling  
    \item page ranking
    \item word indexing
    \item ricerca di istanze prossime a quanto richiesto
    \item vista dei risultati tramite il ranking
  \end{itemize}
  Si è poi iniziato a cercare parole e non fatti, con la conoscenza
  rappresentata tramite un grafo. La ricerca così più approfondita, riconoscendo
  cose legate alla ricerca ed eventuali altri risultati simili o di
  interesse. Si hanno quindi risultati più specifici, con risultati di ricerca
  arricchiti con studi semantici. Un knowledge graph è solitamente in formato
  RDF ed è quindi un supporto alla ricerca online. Un nuovo campo di ricerca è
  l'estrazione di conoscenza anche dal linguaggio naturale, con concetti
  espressi in formato testuale. Per le query si usano SparQL, Gremlin
  etc$\ldots$
\end{shaded}
\chapter{Architetture di integrazione}
L'80\% del lavoro nel caso di gestione dati si ritrova nella preparazione degli
stessi e nella progettazione.
Nella preparazione dei dati si parla anche di \textbf{data integration}.\\
Si hanno diverse possibilità a seconda di carico di lavoro, tecnologie, qualità
dei dati etc$\ldots$\\
Posso avere (approfonditi più avanti):
\begin{itemize}
  \item consolidamento dei dati
  \item gestione distribuita dei dati, senza consolidare ma facendo
  un'integrazione virtuale, tramite il modello wrapper-moderator
\end{itemize}
Dal punto di vista organizzativo si hanno vari problemi:
\begin{itemize}
  \item autonomia, ovvero il grado di indipendenza tra i diversi DBA nelle
  scelte progettuali
  \item frequenza delle query
  \item valore economico dell'integrazione, ovvero la rilevanza per il processo
  operativo aziendale e decisionale di avere informazioni integrate in input in
  modo da produrre output efficaci
  \item volatilità delle fonti nel tempo, ovvero la frequenza di aggiunta o
  eliminazione di fonti e frequenza di modifica degli schemi delle fonti
  \item complessità gestionale, ovvero lo sforzo da dedicare alle attività di
  gestione relative a database e infrastrutture sia hardware che software, a
  causa della corrispondente complessità delle organizzazioni che utilizzano i
  db
  \item costi di eterogeneità, ovvero i costi nascosti ed espliciti relativi ai
  processi aziendali dovuti all'utilizzo di dati eterogenei
\end{itemize}
Si hanno anche problemi tecnologici:
\begin{itemize}
  \item rilevanza dei dati storici e conseguente necessità di memorizzare
  periodicamente nuovi dati senza cancellare quelli vecchi
  \item complessità della query, in termini di numero di dati e tabelle visitate
  e numero di operatori su di esse, e conseguente complessità temporale
  nell'esecuzione della query
  \item rilevanza delle query rispetto alle transazioni, ovvero l'importanza
  relativa e frequenza delle query rispetto alle modifiche dei dati 
\end{itemize}
In base a tutti questi criteri si hanno varie soluzioni (nelle slide tabella).\\
Con l'introduzione a NoSQL si è visto come sia possibile memorizzare i dati in
tanti modi diversi. Ci si chiede quindi come ``unire'' diversi (per sorgente,
formato natura etc$\ldots$) dataset anche di tipologia diversa, procedendo
all'\textbf{integrazione} dei dati. Questo processo può avvenire anche su
dataset di volume ridotto avendo complessità anche su dati piccoli.\\
Si parla solitamente di:
\begin{itemize}
  \item \textbf{data integration}, dove si hanno diversi set di dati (anche non
  disgiunti) con attributi diversi. Si costruisce un unico insieme con tutti gli
  elementi dei due insiemi, con gli elementi presenti nell'eventuale
  intersezione con tutti gli attributi dei due insiemi di partenza (mentre gli
  altri tengono solo gli attributi iniziali). In pratica si fa un \textit{full
    outer join} (ovvero prendo tutti i valori della tabella di sinistra e li
  matcho, se posso, con quelli della tabella di destra, prendendo però poi anche
  tutti gli elementi che non joinano). Il risultato finale potrebbe non avere a
  priori dell'informazione in più sui dati
  \item \textbf{data enrichment}, dove voglio arricchire i dati di un insieme
  con quelli di altri insiemi, qualora possibile. In altre parole è una specie
  di \textit{left outer join}, prendendo tutti gli elementi della tabella di
  sinistra e aggiungo le informazioni della tabella di destra qualora ci sia un
  match con la tabella di destra, altrimenti non aggiungo nulla. Si ha un
  match diverso dalla semplice uguaglianza tra il set che voglio arricchire e
  quello che contiene i potenziali arricchimenti. Potrei avere quindi
  uguaglianza, similitudine, appartenenza etc$\ldots$ perché posso rilassare i
  vincoli di match quando voglio arricchire l'informazione (sfruttando altre
  relazioni più leggere, magari basate su informazioni terze, per esempio
  relazioni geografiche etc$\ldots$). Il dataset arricchito contiene quindi solo
  gli elementi del dataset iniziale che volevo arricchire ma questi potrebbero
  (potrei non essere riuscito ad arricchire tutti i dati) contenere informazioni
  in più
\end{itemize}
In entrambi i casi per rappresentare l'assenza di un attributo uso un NULL nella
tabella se si parla di un modello relazionale. In caso dei vari modelli NoSQL
seguo gli standard dei vari modelli e dei vari DBMS per rappresentare l'assenza
di valore per un certo attributo.\\
Solitamente si cerca di aggiungere, per integrazione o arricchimento:
\begin{itemize}
  \item informazioni spaziali, con informazioni geografiche dei dati che mancano
  e quindi vanno aggiunte
  \item informazioni temporali, con informazioni ``storiche'' dei dati da
  integrare/arricchire. Si ha però un problema di capire cosa venga orima e cosa
  dopo, comportando ambiguità nel rapporto causa-effetto
\end{itemize}
ma potrebbero esserci infinite altre casistiche.\\
\textit{Si parlerà solo di data integration ma ogni tanto verranno aggiunti
  incisi in merito all'enrichment.}
\section{Data integration}
Immaginiamo di avere due dataset con due schemi, espliciti o meno a seconda che
siano basati sul modello relazionale o siano modelli NoSQL (e in quanto caso
bisogno inferirli in modo abbastanza preciso vedendo magari i primi documenti
etc$\ldots$), da voler integrare.\\
Ipotizziamo che i due schemi rappresentino la stessa informazione ma da due
punti di vista. Si hanno, per esempio, uno schema \textit{Libro}, con titolo,
ISBN e autori (specificati da nome e data di nascita), e uno schema
\textit{Autore}, con nome, data di nascita e libri scritti (specificati da
titolo e ISBN). Ho quindi due schemi ugualmente informativi ma con diversa
rappresentazione (ma stesso modello), rendendo quindi difficile l'integrazione
che prevede diverse operazioni. Ovviamente la situazione si complica ancora di
più se ho modelli eterogenei, dal punto di vista dei modelli, avendo modelli sia
relazionali che non. Bisogna anche scegliere 
il modello per lo schema integrato finale e non esiste una soluzione ottima a
priori ma deve essere scelta in base allo scopo dello schema finale. Tale scelta
influenzerà ovviamente le tecniche di costruzione dello schema finale. Per
procedere quindi scelgo in primis il modello dello schema finale e come prima
cosa, dopo averlo scelto, converto il modello di partenza di modello diverso in
quel modello, ottenendo quindi omogeneità, dal punto di vista dei modelli, dei
modelli di partenza.\\ 
Formalizziamo un poco quanto detto. Si hanno vari livelli di
eterogeneità/conflitti:
\begin{itemize}
  \item \textbf{eterogeneità di nome}, legato a come sono state etichettate le
  eterogeneità. Ho quindi concetti di primo livello (che a seconda del modello
  sono entità, documenti, attributi, nodi etc$\ldots$) che rappresentano le 
  stesse informazioni ma con label diverse
  \item \textbf{eterogeneità di tipo}, ovvero a come modellizzo effettivamente
  un particolare ``pezzo'' della realtà
\end{itemize}
Oltre a questi si ha appunto l'\textbf{eterogeneità di modello}, che abbiamo
visto sopra come risolvere fin dal principio, che ``copre'' ogni altro tipo di
eterogeneo.
\subsection{Eterogeneità di nome}
Partiamo con questa prima categoria. \\
Si hanno situazioni di:
\begin{itemize}
  \item \textbf{sinonimia}, ovvero rappresento lo stesso concetto con nomi
  diversi, che sono sinonimi. Sintassi diversa e semantica uguale
  \item \textbf{omonimia}, ovvero rappresento concetti diversi con gli stessi
  nomi. Questo caso è più complicato da riconoscere (ad esempio potrei avere
  \textit{Città} in due schemi ma in uno rappresenta quella di nascita e in un
  altro quella di residenza) e rischia di comportare diversi errori, magari
  portando a dover decidere quale valore associare senza sapere come decidere,
  integrando in modo errato e perdendo informazione. Raramente si ha accesso
  alla documentazione dei dataset e quindi bisogna sperare che i vari attributi
  siano il meno ambigui possibile. Sintassi uguale e semantica diversa
  \item \textbf{iperonimia}, ovvero tra i due concetti uno è più di alto livello
  rispetto all'altra, secondo magari una relazione \textit{IS-A} (esempio il
  concetto \textit{Persona} rispetto a quelli \textit{Uomo} e \textit{Donna},
  con il primo che include i secondi)
\end{itemize}
In fase di progettazione dello schema integrato devo quindi riconoscere e
gestire i casi appena elencati prima di procedere con l'integrazione.
\subsection{Eterogeneità di tipo}
In questo caso si ha che lo stesso concetto viene rappresentato in modo
strutturalmente diverso in due schemi, potendo quindi avere, ad esempio:
\begin{itemize}
  \item domini di definizione differenti per lo stesso attributo in due schemi
  (ad esempio da una parte il dominio delle città italiane e dall'altro quelle
  islandesi)
  \item un attributo in uno schema e avere un valore derivato in un altro
  schema, avendo quindi, nel primo caso, un attributo indipendente mentre nel
  secondo si ha una dipendenza funzionale con un altro attributo
  \item un attributo in uno schema e un entità in un altro schema 
  \item un attributo in uno schema e una gerarchia di generalizzazione in un
  altro schema 
  \item un'entità in uno schema e una relazione in un altro schema o magari, nel
  modello a grafo, avendo in uno schema un nodo e un arco nel secondo
  \item diversi livelli di astrazione per lo stesso concetto in due schemi
  (ad esempio due entità con nomi omonimi legati da una gerarchia IS-A in due
  schemi)  
  \item diverse granularità nei domini di definizione 
  \item diverse cardinalità nelle stesse relazioni 
  \item key conflicts (la chiave in uno schema è diverso da quella dell'altro)
\end{itemize}
Si hanno quindi tantissimi casi in cui di presentano eterogeneità.
\subsection{Data integration system}
Succede quindi che si ha una \textbf{trasformazione di schema}. Si procede con
uno \textbf{schema matching} per capire come unificare i vari schemi, capendo,
per esempio, quali attributi coincidono (anche se magari con sintassi
diverse). Si procede poi con l'\textbf{integrazione} vera e propria, ottenendo
quindi lo schema integrato. Il software che procede con queste fasi è detto
\textbf{data integration system} che procede prendendo e mettendo insieme delle
sorgenti dati e cerca di ottenere uno schema integrato. Vengono presi e uniti
tutti gli attributi degli schemi.\\
\textit{In modo analogo funzionano i datawarehouse dove ho schemi diversi,
  solitamente relazionali ma non sempre, che vengono messi insieme e quindi
  manipolati}.\\
Si hanno tantissimi modi per fare \textbf{schema matching} e si fa ancora molta
ricerca in merito. Si hanno match singoli, match combinati, match basati sullo
schema (vedendo label etc$\ldots$), match basati sui contenuti (vedendo i valori
etc$\ldots$) etc$\ldots$ (per un albero più dettagliato \footnote{Rahm, Erhard
  \& Bernstein, Philip. (2001). A Survey of Approaches to  Automatic Schema
  Matching.. VLDB J.. 10. 334-350. 10.1007/s007780100057.} vedere figura
\ref{fig:tree})
\begin{figure}
  \centering
  \includegraphics[scale = 0.4]{img/tree.jpg}
  \caption{Esempio di albero per lo schema matching } 
  \label{fig:tree}
\end{figure}
Spesso bisogna fare schema matching \textit{on the fly} dovendo integrare nuovi
schemi in modo assai rapido. Si hanno anche sistemi su grande scala con sistemi
\textit{pay as you go}, dove serve altrettanta velocità.\\
Vediamo quindi meglio le fasi:
\begin{enumerate}
  \item \textbf{schema transformation o pre-integration}, è una fase quindi
  propedeutica che presi in input $n$ schemi me li restituisce omogenei tramite
  tecniche di model trasformation o reverse engineering
  \item \textbf{Correspondences investigation}, dove si studiano le
  corrispondenze tra gli schemi tramite tecniche specifiche
  \item \textbf{schemas integration e mapping generation}, dove si definiscono
  lo schema integrato e le regole di mapping necessarie
\end{enumerate}
Avendo da integrare/arricchire decine di schemi si hanno diverse strategie. Si
hanno due scelte fondamentali:
\begin{enumerate}
  \item integro a coppie. Grazie a questa scelta binaria ho due possibilità:
  \begin{enumerate}
    \item prendere uno schema, integrarlo con un altro e integrare il risultato
    con un altro ancora etc$\ldots$. Ottengo una soluzione a scala con un albero
    binario non bilanciato
    \item uso un approccio bilanciato aggregando schemi ``vicini'' e aggregando
    poi i vari risultati delle integrazioni. Ottengo quindi un albero binario
    bilanciato 
  \end{enumerate}
  \item integro con più schemi contemporaneamente. Ho due approcci:
  \begin{enumerate}
    \item integrare tutti gli schemi insieme (\textbf{questo bisogna evitarlo}
    anche solo per $n>3$)
    \item integrare in modo iterativo, in modo analogo a quanto fatto con il
    bilanciamento binario ma ovviamente non con un numero fisso di schemi da
    integrare ogni volta (magari ne integro due o magari tre, raramente di più
    per ovvi motivi)
  \end{enumerate}
\end{enumerate}
Si ha appunto anche il problema che posso dire le stesse cose con strutture
completamente diverse. Si cercano quindi \textbf{corrispondenze semantiche}
sfruttando la conoscenza pregressa sul dominio applicativo. Si possono sfruttare
sia le \textbf{equivalenze} che le \textbf{generalità} (\textit{IS-A}) per
cercare le eventuali corrispondenze. Si rischia di avere anche
\textbf{disgiunzioni} tra attributi completamente diversi magari di schemi
uguali rischiando di avere problemi.\\
In uno schema non relazionale identificare i mapping è quindi molto arduo ma
bisogna anche generare i nuovi mapping e le nuove regole di associazione.\\
Classificando i matching si trovano anche nuove tipologie di conflitto che
devono essere risolti con trasformazioni opportune. Dobbiamo quindi meglio
definire in primis i conflitti. Si hanno diversi tipi di conflitti:
\begin{itemize}
  \item \textbf{conflitti di classificazione} quando elementi omologhi
  descrivono insiemi diversi di oggetti del mondo reale (ad esempio docenti e
  relatori di tesi, che sono comunque docenti ma non tutti i docenti sono
  relatori). Si risolve tramite generalizzazione o specifica della gerarchia (i
  relatori sono una parte dei docenti)
  \item \textbf{conflitti descrittivi} quando tipi omologhi hanno proprietà
  diverse o le stesse proprietà sono descritte diversamente. Si hanno:
  \begin{itemize}
    \item conflitti di nome, ovvero nomi diverse per le stesse cose
    \item conflitti di composizione, ovvero diversi attributi e/o metodi (magari
    avendone di più o di meno o anche diversi)
  \end{itemize}
  La soluzione dipende dai casi e può, per esempio, prevedere l'inclusione di
  tutti gli attributi (usando l'uguaglianza) o la scelta di alcuni (usando
  l'ereditarietà) (???)
  \item \textbf{conflitti strutturali} dove si hanno gli stessi elementi
  rappresentati tramite strutture diverse (magari da una parte un elemento è
  un'intera entità e dall'altra solo un attributo). Per risolvere si sceglie,
  per massimizzare l'informazione ottenuta, è prendere la struttura meno
  vincolata (quindi tra entità e attributo scelgo entità). Scelgo quindi la
  struttura ``più importante'' secondo la logica del ``più cose ho meglio è''
  (mentre nel caso di datawarehouse si sceglie di non salvare determinate
  informazioni) 
  \item \textbf{conflitti di frammentazione} dove magari lo stesso elemento è in
  un singolo oggetto in uno schema mentre è frammentato nell'altro. Si risolve
  tramite l'aggregazione dell'elemento frammentato in quello unico (???). Si
  integrano i dati quindi anche nell'ottica di ``rimetterli insieme'' (questo
  tipo di tracciamento è comodo anche nella lotta evasione fiscale)
\end{itemize}
Parliamo quindi delle \textbf{regole di mapping} definite tra gli schemi
``sorgente'' e quelli ottenuti dopo l'integrazione e possono essere trovate per
lo schema sorgente guardando alle trasformazioni effettuate per la risoluzione
di omologie e conflitti avute durante il processo di integrazione effettuato
sullo schema sorgente. Le regole mi dicono che un elemento chiamato in un modo
nello schema integrato ha una precisa corrispondenza con un certo elemento di
uno schema sorgente.\\
Se si hanno conflitti a livello direttamente di istanze, avendo che li stessi
dati (lo stesso oggetto del mondo ideale) in sorgenti diversi hanno valori
diversi, i problemi aumentano. Questo è un problema fondamentale nel data
integration, anche nel caso dell'arricchimento. Posso avere:
\begin{itemize}
  \item un ``semplice'' \textbf{conflitto di attributi} avendo due entità che
  rappresentano lo stesso oggetto reale con attributi diversi (anche solo uno)
  per la stessa proprietà. Le chiavi in questo caso sono coerenti
  \item un \textbf{conflitto di chiave} (detto anche \textbf{conflitto di entità
    \textnormal{o} conflitto di tupla}) se ho chiavi primarie diverse in due
  entità diverse che però rappresentano lo stesso oggetto (banalmente entità
  che rappresenta lo studente nei db della Bicocca usa la matricola mentre in un
  altro il codice fiscale) mentre gli altri attributi coincidono
\end{itemize}
Una scelta di soluzione, detta \textbf{currency}, prevede, in caso di
conflittualità di valore di attributo, di scegliere il valore inserito
temporalmente per ultimo, nella speranza che sia il più aggiornato (ma spesso
non è esattamente così). Le tecniche che trattano i conflitti a livello di
istanza possono essere applicate in due diverse fasi del ciclo di vita di un
sistema di integrazione dati, vale a dire:
\begin{itemize}
  \item a \textbf{design time}
  \item a \textbf{query time}
\end{itemize}
I problemi principali si hanno con i conflitti di chiave non sapendo bene se
gli stessi oggetti sono presenti nei vari dataset e questi problemi saltano
fuori a query time. Tuttavia, durante il design time decide la
strategia da seguire per risolvere i conflitti prima che le query vengano
elaborate, ovvero nella fase di progettazione del sistema di integrazione dei
dati e le tecniche a query time incorporano le specifiche della strategia da
seguire all'interno della formulazione della query. Si hanno quindi le
\textbf{funzioni di risoluzione dei conflitti} che cercano, dati due valori
conflittuali, di restituire quello più probabile. Un criterio di misura si basa
sull'affidabilità delle sorgenti stesse da cui proviene il conflitto. In altri
casi si ragiona in termini di funzioni matematiche di minimo, massimo, media,
varianza etc$\ldots$ in presenza di valori numerici (introducendo probabilmente
errori, di sottostima, sovrastima etc$\ldots$ ma l'importante è esserne
consapevoli per stimare l'impatto dell'integrazione sui dati). Per valori non
numerici si hanno altre funzioni, ad esempio la concatenazione nel caso di
stringhe o la scelta delle più lunghe/corte. Ovviamente non si può sapere la
risposta giusta ma si può imporre una logica di stima. È comunque buona pratica
tenere traccia tali conflitti e in caso siano troppi preoccuparsi dell'effettiva
validità dei dati. Il data integration assume quindi due forme:
\begin{enumerate}
  \item \textbf{deduplicazione}, dove si scopre se ci sono concetti del mondo
  reale replicati nella tabella, avendo che l'integrazione viene fatta solo su
  quella tabella (non avendo schema matching), internamente. Posso avere quindi
  valori discordanti nella stessa tabella, magari per errori inserimento
  avendo stringhe con distanza sintattica bassa (qualche carattere, secondo la
  \textbf{distanza di edit}). Si usano tecniche empiriche. Questo check va fatto
  di valore in valore ma non 
  si può avere certezza avendo che distanze brevi si hanno anche tra parole
  potenzialmente corrette (basti pensare a Carlo e Carla, entrambi
  potenzialmente corretti per un ipotetico Carl, che potenzialmente è errato in
  lingua italiana). Si procede quindi, in assenza di chiave, ad una pulizia di
  eventuali duplicati cercando di ``fonderli'' del risultato corretto più
  probabile. La distanza di edit va bene solo per stringhe piccole, per quelle
  grosse si hanno altre funzioni (come la Jaccard index function)
  \item integrazione in due tabelle diverse, avendo schema matching
  etc$\ldots$. Si supponga quindi di voler fondere due tabelle senza chiave
  primaria. Si cerca quindi una pseudo-chiave primaria (usando l'insieme più
  grande di attributi comuni alle tabelle che siano significativi, ad esempio
  nome, cognome e luogo di nascita) matchando le tabelle su di essa. Ma potrei
  anche qui non matchare per errori di scrittura. Faccio quindi un matching
  approssimato e concatenando le due tabelle e procedendo poi con la deduplica
  della tabella risultante. Potrei comunque sempre ragionare con la distanza di
  edit etc$\ldots$ restando con le tabelle separate, procedendo con una
  combinazione lineare degli score delle distanze (riscalate su $\{0,1\}$, con 1
  match perfetto, facendo una somma pesata e dividendo per il numero di score),
  decidendo una soglia oltre la quale si ha il match. Si usano tecniche
  probabilistiche. Si procede quindi la 
  fusione delle tabelle e al raffinamento del risultato con l'eliminazione dei
  duplicati 
\end{enumerate}
Oltre a tecniche empiriche e probabilistiche si hanno anche tecniche miste e/o
basate su conoscenza a priori.\\
Si sono finora trascurati quei casi in cui si sta palesemente rappresentando la
stessa cosa ma con termini diversi (magari con nomi contratti, esempio Leonardo
che diventa Leo.). \\
Si ha infatti il \textbf{record linkage}, ovvero la tecnica di risoluzione dei
conflitti a livello di istanza, dove, date le tabelle in input, posso avere in
output: 
\begin{itemize}
  \item \textbf{matching tuples}, ovvero i valori che sicuramente matchano
  (anche se non perfetti ma con score oltre una certa soglia)
  \item \textbf{non matching tuples}, ovvero i valori che sicuramente non
  matchano
  \item \textbf{possible matches}, ovvero una sorta di ``area grigia'' dove non
  si sa come dare risposta (come appunto l'esempio dl nome abbreviato). La
  scelta della soglia per il match e di quella per il non match modifica la
  cardinalità di questi casi, ad  esempio una coppia di soglie troppo cattiva
  produrrà come conseguenza molti casi dubbi, essendo quest'area quella compresa
  tra le due soglie
\end{itemize}
In mezzo subentra l'umano che ``a mano'' sistema i valori, non avendo altro
modo. Da qualche hanno si è anche introdotto il machine learning, tramite
\textbf{Deepr} (basato su reti neurali, trainato tramite specifici dati di
training), per sostituire l'uomo nell'analisi di questa zona di mezzo. Le
soluzioni di machine learning sono fortemente legate al dominio. Un esempio
classico di problema è quello relativo all'indirizzo di casa nelle
anagrafiche. In tal caso prima si cerca di normalizzare l'indirizzo (espandendo
abbreviazioni di vie, piazze etc$\ldots$, esempio v. D. Alighieri diventa Via
dante Alighieri) facendo poi il confronto sulle stringhe. altra tecnica è
usare la geocodifica (tramite maps etc) usando poi latitudine e longitudine (con
comunque un margine di differenza, dato dall'errore anche del GPS in uso
civile e dagli standard statali di definizione degli indirizzi). Si nota quindi
come il dominio degli indirizzi ha una risoluzione 
particolare (oltre al fatto che lo standard degli stessi varia di stato in
stato) e molti domini specifici hanno tecniche specifiche (difficilmente 
apprese tutte da una rete neurale (???)). \\
Si è dato per scontato un altro problema: le dimensione delle tabelle. La
il check dei valori tra le tabelle esplode nel numero di confronti. Bisogna
quindi ridurre lo \textbf{spazio di ricerca}, non potendo fare il prodotto
cartesiano tra le tabelle ($ A\times B$), anche perché nichilizzerebbe la
scalabilità. Si hanno tantissimi \textbf{blocking method} per ridurre lo spazio
di ricerca. Una volta ottenuto un sottoinsieme dello spazio di ricerca posso
partire con vaie tecniche empiriche (ad esempio ordinare sui cognomi, e sperare
che siano corretti o con al più pochi errori, per effettuare poi il check). Si
applica poi un \textbf{modello di decisione} per il check, tramite deep
learning, semantica, funzione di risoluzione delle stringhe etc$\ldots$, ovvero
un qualsiasi meccanismo per ottenere match, unmatch e possible match. Per farlo
si necessita anche di un benchmark che sono anch'essi fortemente dipendenti dal
dominio e quindi non sempre disponibili. Si hanno benchmark sugli indirizzi
etc$\ldots$. \\
Vediamo, riassumendo, nel \textbf{record linkage probabilistico} si ha:
\begin{enumerate}
  \item \textbf{preprocessing}, normalizzazione dei formati (espansione
  abbreviazioni, lowecase, eliminazione spazi e caratteri speciali etc$\ldots$ o
  passando ad altre 
  codifiche, come la geocodifica) secondo uno standard
  \item \textbf{blocking}, riduzione dello spazio di ricerca. Uno dei metodi è
  il \textbf{sorted neighbour} che implementa il blocking scegliendo una
  chiave, ordinando i record in base alla chiave univoca e spostando una
  finestra sui file ordinati, confrontando a destra e a sinistra nelle due
  tabelle gli elementi in questa finestra. Le finestre sono dinamiche in quanto
  potrei avere match sul bordo che andrebbero persi
  
  \item \textbf{compare}, scegliendo una funzione di distanza e cercando un
  sample con coppie conosciute di match e unmatch. Bisogna anche valutare per
  ogni valore di distanza la frequenza di 
  matching e unmatching
  \item \textbf{decide}, valutando la distanza e le soglie
\end{enumerate}
\textit{Per confrontare se due date sono uguali prima le normalizzo al modello o
europeo o americano, le normalizzo in base al carattere separatore e poi
confronto}.\\
Per la fusione posso:
\begin{itemize}
  \item ignorare un conflitto, mettendo tutti i valori disponibili
  \item cercare di evitare i conflitti basandomi sui metadati, sul sorgente o
  sulle istanze  
  \item gestire i conflitti tramite le funzioni di risoluzione dei conflitti e
  alle altre cose discusse sopra
\end{itemize}
Si hanno elenchi delle varie strategie possibili (ignorare, scegliere il più
recente, scegliere il valore medio e molte altre), ognuna delle quali, come
anche il record linkage, può introdurre errori. La scelta delle soglie inoltre
può portare ad essere meno conservativi, introducendo falsi positivi, o troppo
conservativi, riducendo drasticamente i match. Le varie scelte dipendono
comunque dai singoli casi.
\section{Architetture per data integration}
\textbf{L'ordine di questa sezione è dubbio, forse andava ad inizio capitolo o
  forse no.}\\
Si parte come abbiamo detto da una serie di schemi separati in vari db, con
modelli diversi. Posso avere anche file csv etc$\ldots$ Bisogna quindi
raccogliere le varie informazioni e integrarle in un'unico dataset. Si hanno più
tecnciche:
\begin{itemize}
  \item la tecnica del \textbf{consolidamento} dove si raccolgono vari dataset
  dalle varie sorgenti informative, li integro come visto precedentemente e
  infine salvo il risultato in un nuovo db, che sarà la mia nuova architettura
  dati 
  \item la tecnica che prevede di rappresentare i vari schemi separati in modo
  che essi siano mantenuti inalterati, non memorizzando il risultato
  dell'integrazione, ma dando possibilità all'utente di scrivere query sui dati
  integrati. Si produce quindi un nuovo schema globale che comunica tramite un
  mediatore software per prendere le query. Questo è la vera e propria tecnica
  di \textbf{virtual data integration}
\end{itemize}
\textbf{Nel capitolo si parla soprattutto di virtual data integration anche se
  spesso non specificato}.\\
Il data integration è un è un'importante area di ricerca e business che ha lo
scopo principale di consentire ad un utente un accesso uniforme a più
fonti di dati autonome ed eterogenee, attraverso la presentazione di una visione
unificata di questi dati. Trovare questa uniformità tra elementi eterogenei è
complesso perché bisogna trovare differenze e somiglianze in ogni schema per
potersi conformare. Bisogna quindi riconciliare tutte le eterogeneità di schema
e di istanza.\\
Il vantaggio di architetture d'integrazione rispetto alle architetture federate
è la capacità di gestire meglio le eterogeneità, soprattutto se complesse, a
livello di schema. Inoltre i sistemi federati non possono gestire eterogeneità a
livello di istanze a causa della qualità (accuratezza, incompletezza
inconsistenza etc$\ldots$) dei dati. D'altro canto nelle architetture federate si
ha un livello di autonomia in poco più basso nel senso che nei modelli integrati
si assume che si hanno sorgenti provenienti da chissà dove mentre nei sistemi
federati i nodi sono scelti in modo più consapevole.\\
Si hanno due approcci principali alla data integration:
\begin{enumerate}
  \item \textbf{integrated read-only views (\textit{Mediation})}, dove si ha
  integrazione solo per lettura. I dati letti nei vari db restano quindi
  invariati. Questa soluzione a livello aziendale viene scartata in favore delle
  datawarehouse, avendo persistenza dei dati e profondità storica maggiore
  (anche se perdo flessibilità nel momento in cui si rimuove una sorgente dati)
  \item \textbf{integrated read-write views (\textit{Mediation with update})},
  dove si hanno anche gli update. È estensione dell'architettura di Mediation
  per supportare gli aggiornamenti in una vista integrata (dovendo poter
  accedere ai vari db anche in scrittura). Questa cosa è
  difficile e anche poco studiata in letteratura e quindi non la tratteremo. In
  questo caso si preferiscono i modelli federati
\end{enumerate}
L'integrazione dei dati è il problema di combinare i dati che risiedono in
diverse fonti e fornire all'utente una visione unificata di questi dati può
quindi essere letteralmente definita come \textbf{global virtual schema
  (\textit{GS})}.\\
Parto dai vari db coi vari modelli e schemi e creo un GS che ha un certo
modello. Si hanno mapping tra i vari schemi e il GS e quindi l'utente può
effettuare delle query passando per il GS e a runtime il GS capisce dove sono le
informazioni richieste, converte il linguaggio di query in quello necessario per
il db dove si trova l'informazione ed effettua la query (che viene eseguita
localmente sul db). Sempre on the fly si risolvono i conflitti e viene
restituito il risultato della query. Potrei avere anche integrazione pay as you
go.\\
Si hanno quindi tre elementi fondamentali in un'architettura di integration:
\begin{enumerate}
  \item il GS
  \item le varie sorgenti
  \item il mapping tra le sorgenti e il GS
\end{enumerate}
Si hanno quindi due componenti fondamentali:
\begin{enumerate}
  \item un \textbf{mediator}, che data una query la frammenta e la riscrive per
  poter lavorare con i vari schemi locali. Inoltre il mediator mette insieme i
  vari risultati, risolve i conflitti e risponde alla query
  \item vari \textbf{wrapper}, che agganciano ogni schema locale delle sorgenti
  al GS, rappresentando la sorgente in uno schema compatibile con quello del
  GS. Essi ricevono query nel linguaggio del GS e rispondono di conseguenza
\end{enumerate}
Le architetture \textbf{wrapper-mediator} sono lo standard nel mondo del virtual
data integration, nel dettaglio della \textbf{lazy integration} (\textit{capire
  che 
  dice al minuto 12.05}) consentendo maggior flessibilità (\textbf{dal minuto 12
  al minuto 12.30 si capisce davvero poco, controllare}).\\
Bisogna studiare GS e mapping.\\
Si hanno due tipiche architetture del sistema, a seconda della direzione dei
mapping: 
\begin{enumerate}
  \item una in cui si parte dagli schemi locali per arrivare al GS
  \item una in cui si parte dal GS e si arriva gli schemi locali
\end{enumerate}
In ogni caso lo scopo è interrogare l'informazioni come se fosse unica.\\
\textbf{Nelle slide qualche esempio.}\\
Il mediator costruisce uno schema unificato di diverse fonti di informazioni
(eterogenee) e consente a un utente di formulare una query su di esso. La query
dell'utente viene quindi trasformata in una serie di sottoquery, una per
ciascuna fonte di dati coinvolta nella query e il risultato viene sempre
raccolto e mergiato dal mediator che lo restituisce all'utente.\\
Il mediator quindi deve riconciliare le varie istanze mettendo insieme le varie
risposte dei vari db. Si hanno varie azioni, da svolgere in breve tempo (essendo
tutto on the fly), fatte dal mediator:
\begin{itemize}
  \item raggruppare informazioni sulla stessa entità del mondo reale 
  \item rimuovere la replicazione dei tra le varie fonti di dati 
  \item risolvere le inconsistenze tra le varie fonti di dati 
  \item ottenere accuratezza, completezza, etc$\ldots$ tra i dati provenienti da
  diverse fonti di dati   
\end{itemize}
L'interazione con il mediator è quindi divisa in due fasi:
\begin{enumerate}
  \item la creazione della rappresentazione unificata (\textbf{Publishing phase
    at design time}), creando GS e mapping
  \item la formulazione e l'esecuzione di una query nella rappresentazione
  unificata (\textbf{Querying Phase}). Questa fase è a runtime
\end{enumerate}
Nella fase di \textbf{publishing} i problemi sono quelli di modellare e definire
linguaggi 
per lo schema unificato, costruendo anche i mapping. Eventualmente devo gestire
gli update. Devo anche estrarre lo schema dei vari db per capire come
procedere.\\
Nella fase di \textbf{querying} bisogna stabilire il linguaggio, fare query
unfloding/rewriting grazie ai mapping. Devo inoltre cleaning e fusion dei
risultati. Bisogna inoltre convertire i vari linguaggi da GS a schemi locali.\\
Il mapping è centrale in entrambi. Possiamo vedere il sistema di virtual data
integration come una tripla:
\[(G,S,M)\]
dove:
\begin{itemize}
  \item $G$ indica lo schema globale
  \item $S$ l'insieme degli schemi sorgente
  \item $M$ il mapping
\end{itemize}
Dobbiamo appunto costruire uno schema globale virtuale. Il problema è capire
quali dati reali nelle sorgenti di dati corrispondono a quei dati virtuali
rappresentati per scelta dallo schema globale. Per il mapping si hanno vari
approcci, a seconda della relazione tra lo schema globale e quelli locali,
rispetto al concetto di vista (che in SQL ricordiamo costruire una sorta di
tabella virtuale):
\begin{itemize}
  \item \textbf{GAV (Global As View)}, in cui lo schema globale viene creato
  sulla base dell'osservazione degli schemi sorgente, attraverso un
  processo di integrazione intenzionale degli schemi sorgenti (si pensi
  anche al processo di consolidamento, o ad una situazione in che vogliamo
  rappresentare in modo integrato tutto il contenuto informativo
  dell'architettura dati di un'organizzazione). L'approccio si può
  riassumere dicendo che lo schema globale è una vista di quelli locali (tutte,
  contemporaneamente). Ogni tabella dello schema globale è una vista degli
  schemi locali. In questo caso il mapping va dagli schemi locali a quello
  globale. Il vantaggio è che si prendono tutte le informazioni disponibili,
  procedo tramite arricchimento dei dati ma potrebbe capitare che non si ottiene
  lo schema voluto. GAV si presta poco agli update in caso di rimozione (in caso
  di aggiunta semplicemente rifaccio l'integrazione e aggiungo elementi allo
  schema globale, anche se questo ha un costo) in quanto tutti i concetti che
  prima avevano a che fare con la 
  sorgente tolta vanno cambiati, avendo poi problemi anche a livello di istanze.
  Questo approccio ci dice come recuperare i dati da istanze di origine locale
  (reale) per ciascuna tabella dello schema globale (virtuale). Con il GAV il
  mediatore procede tramite unfloding delle query, cioè la sostituzione nella
  query dei riferimenti allo schema globale con l'espressione della
  view globale in termini delle view locali. Bisogna però riportare i
  \textit{join} operazione che abbiamo già visto rischia di essere complessa
  e poco efficiente. Nonostante tutto gli approcci GAV sono solitamente quelli
  più usati
 
  \item \textbf{LAV (Local As View)}, dove lo schema globale viene progettato a
  priori indipendentemente dagli schemi sorgente, da cui prenderò solo i pezzi
  di informazione di interesse. Il mapping tra i sorgenti e lo schema globale si
  ottiene definendo ciascun sorgente sullo schema globale. Può capitare che
  concetti locali non siano nello schema globale ma ogni concetto globale avrà
  una qualche rappresentazione locale. Questo approccio rende più semplice
  l'evoluzione temporale dello schema globale in quanto in caso di update
  aggiungendo uno schema locale non cambia lo schema globale (che è fisso) ma si
  ha solo un eventuale arricchimento del mapping con un nuovo query mapping. Il
  mapping va quindi dallo schema globale a quelli sorgente/locali avendo che il
  contenuto di ogni schema locale è descritto in termini di vista sullo schema
  globale. Se una colonna non è presente nel globale ma solo nel locale rispondo
  NULL. Questo approccio ci dice come le istanze sorgenti (reali). Il query
  management è più complesso del GAV ma è più semplicemente estendibile del GAV
  contribuiscono/sono associati alle tabelle dello schema globale (virtuale) 
  \item \textbf{GLAV (Global and Local As View)} dove il mapping tra sorgenti e
  schema globale si ottiene definendo un insieme di viste, alcune sullo schema
  globale e altre sulle origini dati. È un approccio ibrido
\end{itemize}
\begin{esempio}
  Un mapping GAV è del tipo:
  \begin{minted}{sql}
    Create View GProf as
    Select S1.Name as Name, S1.Age as Age from S1
    Union
    Select S2.Name as Name, S2.Age as Age from S2
  \end{minted}
  (prendo tutto insieme. facendo una union)\\
  Con la seguente query per sapere i nomi dei professori che hanno più di 50
  anni:
  \begin{minted}{sql}
    Select GProf.Name
    From GProf
    Where Age > 50
  \end{minted}
  Avendo poi un'operazione di unfolding infatti nella query il mediator
  sostituire i riferimenti a GProf con la specifica della vista in termini degli
  schemi locali 
\end{esempio}
\begin{esempio}
  Vediamo la stessa query in approccio LAV.\\
  Si hanno nello schema globale delle create view di quello locale:
  \begin{minted}{sql}
    Create View S1 (Name,Age) as
    Select GProf.Name as S1.Name, GProf.Age as S1.Age
    From GProf
  \end{minted}
   \begin{minted}{sql}
    Create View S2 (Name,Age) as
    Select GProf.Name as S2.Name, GProf.Age as S2.Age
    From GProf
  \end{minted}
    Con la seguente query per sapere i nomi dei professori che hanno più di 50
  anni:
  \begin{minted}{sql}
    Select GProf.Name
    From GProf
    Where Age > 50
  \end{minted}
  Il mediator quindi parte dalle viste per cercare i nomi dei richiesti in
  entrambe le viste mettendo poi insieme i risultati.\\
  \textbf{Questo esempio è semplificato in quanto ho schemi locali dello stesso
    tipo.}
\end{esempio}
\textit{In generale i sistemi di integrazione hanno ancora scarse prestazioni.}
\begin{shaded}
  \noindent
  \textit{Vengono qui aggiunte le cose dette in live.}\\
  \textbf{Guardare slide incontro, c'è tutto.}
\end{shaded}
\section{Big Data integration}
\textbf{Queste esercitazioni sono tra le peggiori mai sentite nella mia vita. La
prof parlava in modo completamente sconnesso, ho cercato invano un senso logico,
non l'ho trovato. Non ho messo particolari ''(???)'' per segnalare dubbi perché
ne servirebbe uno ogni 5 parole. Auguri!}\\
Con Big Data integration si intende l'unione di big data e data integration. Con
i big data ci si concentra su volume dei dati, velocità di collezione e analisi,
priorità, valore dei dati, varierà dei dati, veridicità dei dati etc$\ldots$\\
Si hanno due soluzioni principali:
\begin{enumerate}
  \item \textbf{data warehouse} che consiste nel creare un unico archivio
  (tramite materialized view) di dati da diverse fonti offline. Questo è un
  business milionario. Essendo i dati materializzatati (tramite liste
  materializzate) insieme è veloce fare query ma si hanno problemi di
  consistenza 
  \item \textbf{virtual integration} che consiste nel supportare la query su
  uno schema mediato (sistema wrapper/mediator (???)) applicando la
  riformulazione della query online. Si ha il link e la fusione delle varie
  risposte ricevute
\end{enumerate}
Bisogna in ogni caso accedere a tante sorgenti (e non tanto di dati) che
cambiano molto nel tempo. Tutto quanto appena detto sui big data in merito ai
dati diventa in merito ai sorgenti: volume dei sorgenti, velocità di collezione
e analisi, priorità, valore dei sorgenti, varierà dei sorgenti, veridicità dei
sorgenti etc$\ldots$\\ 
Fare big data integration è importante per costruire grafi di conoscenza di
grandi dimensioni, detti anche \textbf{knowledge bases} (si pensi ad esempio a
quella di Google, \textbf{google knowledge graph}). Una knowledge base è
un'insieme di entità con relazioni tra 
loro dove le classi delle entità e le relazioni stesse vengono definite in un
vocabolario. Si ottiene quindi una rete di connessioni.\\
L'altro fattore per cui è importante è perché dai knowledge graph possiamo
ottenere dati geo spaziali, utili in vari scopi della società. Un altro fine è
per fare analisi scientifica, magari di geni, malattie etc$\ldots$\\
Fin'ora si è studiato il ``small'' data integration, che era molto più semplice,
dove comunque ogni ``pezzo'' arrivava da un certo sorgente, ma in ongi caso in
termini ``piccoli''. Si avevano quindi, nello ``small'' data integration:
\begin{itemize}
  \item \textbf{scheme alignment}, che vuole risolvere il problema della
  struttura. Se si hanno due sorgenti con schemi diversi si ``allineano'' ad uno
  schema unico. Spesso si ha eterogeneità nei modi di esprimere classi,
  relazioni e attributi (anche in sorgenti con lo stesso schema). Ci si
  concentra quindi sullo schema
  \item \textbf{record linkage}, dove si individuano delle istanze che sono le
  stesse nonostante l'ambiguità di rappresentazione nei vari sorgenti. Sposto
  quindi l'attenzione sul contenuto, avendo spesso eterogeneità nel
  rappresentare le stesse entità
  \item \textbf{data fusion}, dove si uniscono e riconciliano i contenuti che
  sono in mismatch nei vari sorgenti. Si cerca di risolvere, secondo varie
  tecniche i conflitti di valore
\end{itemize}
risolvendo le varie ambiguità, di semantica, rappresentazione (nonché
inconsistenze) etc$\ldots$ tramite pipeline con appunto queste tre fasi.\\
Nel caso dei big data si hanno però moltissimi dati e conseguenti inconsistenze,
incompletezze, duplicazioni etc$\ldots$ Si hanno quindi varie tecniche.
\subsection{Schema alignment}
Analizziamo un attimo la prima fase.\\
Partiamo nel caso, passando al big data, in cui si hanno dati molto specifici in
un certo dominio. In primis ci si deve chiedere come sono distribuiti tali dati
nel web, studiando eventuali provider di tali dati, che li raccolgono da diverse
sorgenti etc$\ldots$ Si studiano eventuali incongruenze. Analizziamo meglio la
distribuzione dei dati. Bisogna capire come queste informazioni possono essere,
tramite tecniche di information extraction, raccolte da diversi
sorgenti. Bisogna anche capire il volume e la copertura di tali informazioni,
che sono sullo stesso dominio ma presi da sorgenti diverse. Vediamo uno
studio.\\
Con DMP12 si sono studiati 9 domini con accesso a grandi db di dominio
specifico, con entità che hanno attributi che possono valere come
identificatori. In questo caso la metodologia dello studio comprendeva:
\begin{itemize}
  \item utilizzare tutta la cache nell'engine di ricerca
  \item le pagine web hanno entità con un identificatore
  \item si aggregano insiemi di tutte le entità da ogni sorgente
\end{itemize}
Con lo studio si è ottenuto che fin da subito ha valori molto alti di copertura
delle informazioni \textbf{1-coverage} (\textbf{grafici su slide}) e che resta
stabile aumentando i sorgenti. Già con 100 sorgenti su 100000 si aveva copertura
massima. La statistica \textbf{k-coverage} mi raccoglie la ridondanza delle
informazioni presenti, avendo lo stessa frazione di dati in almeno $k$ sorgenti
diversi. Per una \textbf{5-coverage} ho copertura al 90\% con 5000 sorgenti e al
95\% con 100000 sorgenti. I dati erano in merito ai numeri telefonici dei
ristoranti. Al variare dei dati cambiano le statistiche: una \textbf{1-coverage}
sui siti dei ristoranti si ha l'80\% con 100 sorgenti e il 95\% con 10000
(copertura che calava molto aumentando il $k$, grafici su slide).\\
In merito a quanto bene erano connessi i sorgenti in un dato dominio si può
ragionare in merito all'engine di ricerca. Con DMP12 si è considerato il grafo
entità-sorgente per vari domini. Si ha un grafo bipartito con entità e sorgenti
come nodi, collegati da archi se un'entità è in un certo sorgente. Si è quindi
studiato il grafo, studiando la sua connettività si studia la robustezza
dell'algoritmo di estrazione delle informazioni al variare delle scelte iniziali
di architettura. Si parte da un insieme di entità, dette \textbf{seed}, e a
partire da queste se ne cercano altre su altri sorgenti, contando le iterazioni
necessarie per arrivare ad una \textit{convergenza}, alla quale ci si ferma. Si
studia anche il \textit{diametro} del grafo (esempio su slide). Si studiano
anche ridondanza dei dati e eventuali overlap.\\
Facciamo quindi delle considerazioni fin ali dello studio.\\
Per quanto riguarda la distribuzione anche per i domini con forti aggregatori,
dobbiamo studiare a fondo i sorgenti per costruire un database
ragionevolmente completo, sopratutto per un $k$ alto di coverage (per aumentare
la confidenza). In merito alla connettività si ha che le orgenti in un dominio
sono ben collegate, con un alto grado di ridondanza e overlap dei
contenuti e questo resta vero anche se alcuni aggregatori principali vengono
rimossi. \\
Un secondo studio (grafici sempre su slide), LDL+13, è stato fatto su due
domini, sulla qualità del web. Bisogna sperare in questo caso di avere dati
``puliti'', sapendo che una bassa qualità comporta un grande impatto.\\
Si è quindi studiata la consistenza dei dati, scoprendo una forte inconsistenza,
a causa di ambiguità semantica in primis. Un altro problema sono gli errori di
unità (precisione diversa) o errori veri e propri (con valori
incompatibili). Spesso le sorgenti si copiano a vicenda senza controllare la
validità dei dati copiati, portando a problemi di stima (facendo sembrare
statisticamente corretto un valore che non lo è).\\
Un altro problema è quello del volume dei dati, avendo milioni di sorgenti con
dati specifici per certi domini. Nel web, secondo CHW+08, si hanno 154 milioni
di tabelle, 10 milioni di sorgenti ad alta qualità, per MKK+08, 10 milioni di
tabelle relazionali, per EMH09. Per i big data si hanno evidenti difficoltà
quindi di schema alignment. Fare data warehouse sarebbe costosissimo ed è
complicato fare virtual integration.\\
Continuando coi problemi si ha la velocità:
\begin{itemize}
  \item da 43.000 a 96.000 fonti Deep Web (con form HTML), secondo B01
  \item 450.000 database, 1,25 milioni di interfacce di query sul Web, secondo
  CHZ05  
  \item 10 milioni di fonti Deep Web di alta qualità, secondo MKK+08 
  \item Molte fonti forniscono dati in rapida
  evoluzione, ad esempio, i prezzi delle azioni 
\end{itemize}
È quindi difficile capire l'evoluzione della semantica, è quasi impossibile
catturare dati che cambiano rapidamente (nonché pensare a dei data warehouse a
causa dei costi).\\
Un altro problema è la varietà dei sorgenti e la loro veridicità (da LDL+13 si
ha che scarsa qualità dei dati dei sorgenti del deep web). Tutti questi problemi
sono delle challenge moderne da risolvere.\\
Vediamo quindi alcune tecniche per lo schema alignment.\\
Mediamente si hanno 3 step, per abilitare il linkage e ottenere risultati
semanticamente significativi (per esempi guardare slide): 
\begin{itemize}
  \item \textbf{mediated schema}, dove si specifica la modellazione delle
  informazioni con uno schema che mi permetta di unificare e di allineare tutti
  i sorgenti, ottenendo una sorta di unico schema finale ``mediatore''. Si crea
  una vista unica con tutte le sorgenti e si catturano le informazioni
  principali del dominio studiato. Solitamente questa fase è manuale  e lo
  schema mediato contiene più informazioni degli schemi dei singoli sorgenti ma
  potrebbero essrci casi in cui si hanno informazioni mancanti
  \item \textbf{attribute matching}, dove si cercano corrispondenze tra gli
  attributi dello schema mediato e quelli dei singoli sorgenti. Si
  ``connettono'' gli attributi dei sorgenti con quelli del mediatori, di solito
  si ha una corrispondenza ``1:1''. Un attributo del mediatore può essere una
  combinazioni di attributi del sorgente (o viceversa)
  \item \textbf{schema mapping}, dove si costruisce il mapping tra ogni sorgente
  e il mediatore. Tale mapping specifica la relazione di semantica tra il
  contenuto dei sorgenti e quello del mediatore e può essere usato per
  riformulare una query indirizzata al mediatore (che verrà poi suddivisa in
  ogni sorgente). Ci sono tre tipi di mapping, già visti (esempi su slide):
  \begin{itemize}
    \item GAV
    \item LAV
    \item GLAV
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[scale = 0.5]{img/bds.jpg}
    \caption{Schema dello spazio delle strategie di schema alignment}
    \label{fig:bds}
  \end{figure}
\end{itemize}
Si hanno diverse tecniche che considerano volume e varietà:
\begin{itemize}
  \item integrazione di interfacce di query di deep web in GMG+04 e CHZ05 
  \item scansione, indicizzazione di dati deep web in MKK+08
  \item estrazione di dati strutturati da tabelle web in CHW+08, LSC10, PS12 e
  DFG+12 ed elenchi web in GS09 e EMH09. Lo studio delle tabelle è difficoltoso
  per il loro numero (154 milioni di tabelle relazionali, con 5.5 milioni di
  attributi e 2.6 milioni di schemi). Devo usare la ricerca per keyword. SI
  hanno problemi di ambiguità per le tabelle, si ha che non tutte le
  tabelle sono rilevanti per le query e spesso hanno anche specifiche
  funzionalità. Si usano delle featureRank per stimare la qualità delle tabelle
  basandosi su:
  \begin{itemize}
    \item query independent features (numero di righe/colonne, intestazioni,
    numero di NULL etc$\ldots$)
    \item query dependent features
    \item regressione lineare
    \item feature pesate
  \end{itemize}
  Si ottiene quindi uno score delle tabelle.\\
  Si ha anche lo schemaRank che specifica anche la coerenza dello schema. Data
  $p(x)$ la frazione dello schema univoco che contiene l'attributo $x$ definisco
  il \textbf{point-wise mutual information} tra due attributi $a$ e $b$ come:
  \[pmi(a,b)=\log_2\left(\frac{p(a,b)}{p(a)\cdot p(b)}\right)\]
  e la coerenza è la media di tutti i $p(a,b)$ per ogni coppia di attributi
  nello schema. \\
  Per l'annotazione delle tabelle in LSC10 ci si chiede, data una
  tabella Web, quali entità si verificano in quali celle, quali sono i tipi di
  colonna e le relazioni tra le colonne. Il testo nelle celle descrive entità ma
  può essere ambiguo e le intestazioni non usano vocabolari
  controllati. Risolvere questi problemi mi permette di fare query sui metadati
  relazionali ed estrarre conoscenza dalle tabelle. Per annotare le tavelle uso
  cataloghi che consistono in una gerarchia di tipi, entità che sono istanze
  di tipi (possibilmente multipli) e relazioni binarie (rappresentati come
  alberi, esempi su slide). I cataloghi si usano per stimare la qualità dei
  contenuti delle celle della tabella, studiando la similarità. Annotare mi
  permette di estrarre la semantica e di arricchire le tabelle. Si hanno diversi
  approcci per etichettare i varie elementi (righe, colonne, relazioni
  etc$\ldots$). \\
  Con DFG+12 si cercano invece tabelle simili in un ``corpus'' di altre
  tabelle. Cerco tabelle simili ad una tabella data. Le tabelle connesse sono ad
  esempio: 
  \begin{itemize}
    \item tabelle candidate ad una \textit{union} con l'aggiunta di nuove
    entità.
    Si usano tre segnali per stabilire se sono candidate alla \textit{union}:
  \begin{itemize}
    \item il rumore di entità e tipi
    \item la cura di entità e tipi
    \item il conto delle co-occorrenze di entità e tipi
  \end{itemize}
  Alcuni approcci usano:
  \begin{itemize}
    \item la similarità di Jaccard pesata sui set di etichette per la coerenza
    delle entità corrispondenza del peso massimo bipartito per la coerenza dello
    schema 
  \end{itemize}
    \item tabelle candidate per una \textit{join} con l'aggiunta di nuovi
    attributi. Le tecniche sono analoghe alla \textit{union}
  \end{itemize}
  Si cercano le migliori tabelle tramite un ``punteggio di relazione'' con
  quella in analisi. La metodologia è a tre step:
  \begin{enumerate}
    \item si verifica la consistenza delle entità tra la tabella data e quella
    di cui si sta studiando la relazione
    \item si verifica se si possono espandere le entità della tabella data con
    l'altra
    \item si verifica che i due schemi delle due tabelle siano consistenti,
    avendo schemi simili
  \end{enumerate}
  Un approccio naive calcola il punteggio per ogni coppia di tabelle. Si usano
  quindi filtri per ottenere meno comparazioni (usando i filtri come criteri di
  blocco per classificare le tabelle ed esegui solo confronti di correlazione
  anll'interno dei sottoinsiemi creati dai blocchi) e
  più veloci (applicando in 
  sequenza vari filtri, basandosi ulla selettività e l'efficienza computazionale
  dei filtri). Come filtri utili si hanno che le tabelle condividano colonne o
  entità. 
  \item sistemi di dataspace in FHM05, HFM06 e DHY07. È un approccio quasi
  impossibile per volume, varietà e velocità, a causa di un approccio pay-as-you
  go (inizia con poco e poi ``esplode'')
  \item integrazione dei dati basata sulla ricerca per parole chiave in TJM+08
  e con dati dinamici in TIP10
\end{itemize}
Una big data integration automatica è possibile in DDH08 ma si ha necessità di
modellare l'incertezza sulla semantica degli attributi nei sorgenti. Si crea
automaticamente uno schema mediato da un insieme di origini tramite schemi
probabilistici (tramite mapping da sorgenti a mediatori pesati) che offrono il
beneficio di una modellazione dell'incertezza. Si ha una sorta di clustering
degli attributi, anche se si ha incertezza sulla sua accuratezza. \\
\subsection{Record linkage}
Passiamo quindi alle seconda fase, il \textbf{record linkage}, che si basa sul
contenuto.\\
Si hanno tre step principali per studiare scalabilità, semantica e similarità:
\begin{enumerate}
  \item \textbf{blocking}, usata per rendere gli step successivi più
  efficiente. Si creano blocchi per creare sottoinsiemi più specifici per gli
  step successivi. Si ragiona quindi in termini di scalabilità producendo
  blocchi più piccoli con record simili tra loro. Si usano funzioni di blocking
  su valori di uno o più attributi. La creazione dei blocchi deve essere il più
  efficiente possibile
  \item \textbf{pairwise matching}, ogni record viene comparato con un altro
  per stabilire le coppie che matchano. Si computa la similarità di record
  nello stesso blocco. Si hanno decisioni locali all'interno dei blocchi che
  possono non essere consistenti a livello globale
  \item \textbf{clustering}, dove l'obiettivo è assicurare la consistenza
  globale. Si studia come partizionare l'insieme di tutti i record in modo che
  ogni partizione si riferisca in modo distinto ad una entità, garantendo la
  semantica nelle partizioni.
\end{enumerate}
Anche in questo caso si hanno tecniche legate al volume, dovendo gestire
milioni di record:
\begin{itemize}
  \item map-reduce, una tecnica per rendere più efficacie la parallelizzazione
  in presenza di molti dati, in VCL10 e KTR12
  \item blocchi adattivi in DNS+12, MKB12 e VN12
  \item blocchi in data space eterogenei, in PIO+12 e PKP+13
\end{itemize}
Per la velocità si ha la tecnica del record linkage incrementale in WGM10 e
WGM13. Il numero dei sorgenti esplode velocemente e rende i confronti
estremamente costoso. Le tecniche tradizionali di record linkage non sono
infatti applicabili per i big data ma appunto conviene una strategia
incrementale, aggiornando i link esistenti quando si hanno aggiornamenti dei
dati. \\
In merito alla prioritizzazione si ragiona su un ordine di priorità degli
schemi.\\
Si cerca di matchare dati strutturati e testi/dati non strutturati, in
KGA+11 e KTT+12, per quanto riguarda la varietà (esempio su slide). Si cerca
inoltre di matchare tabelle e cataloghi, con LSC10. \\
Le sfide quindi nel record linkage sono
quelle di matchare specifiche strutturate con altre non strutturate (e non
strutturate con altre non strutturate). Si procede parsando il testo,
semanticamente, offline o online. L'operazione ha tre step (\textbf{non si
  capisce nemmeno quali siano, sta delirando, aiuto!}):
\begin{enumerate}
  \item \textbf{string tagging} per identificare tramite tag parse plausibili
  per ottenere il parser ottimo dei un certo testo. Si crea un indice invertito
  sui record strutturati tale per cui l'indice invertito ritorna un insieme di
  nomi di attributi associati alla stringa nei record strutturati. È importante
  che ci sia un parse plausibile in modo che ogni attributo abbia un parse
  distinto. Si studiano quindi le ambiguità
  \item si usa una funzione di matching che stabilisce uno score tra il record
  strutturato e il testo. Si usa un vettore di feature similarity per la
  similarità tra gli attributi
  \item Bho
\end{enumerate}
Matchare dati strutturati non è così banale, a causa di limiti nei dati,
all'eterogeneità etc$\ldots$ Spesso si usano gli identificatori se
disponibili.\\
In merito alla veridicità, con LDM+11, si provvede ad un link temporale dei
record, studiando quando sono veri. \\
\textbf{Per esercitazione 5 guardare le slide e pregare (se avete qualcuno da
  pregare)}. 




\chapter{Data quality}
Il tema del \textbf{data quality} è particolarmente importante perché, dopo aver
approfondito come gestire i dati e le loro architetture è bene studiare anche i
dati in se. \\
un esempio di scarsa data quality è quella di avere lo stesso dato con valori
diversi nello stesso posto, anche semplicemente una pagina web.\\
I dati sono una rappresentazione della realtà e questo porta al fatto che la
realtà viene da noi modellata tramite alcuni dati specifici, tramite un pezzo
della realtà e tale rappresentazione potrebbe non essere a priori oggettiva,
anche a causa di strutture linguistiche e/o basate sui sensi. Si definiscono
quindi:
\begin{itemize}
  \item \textbf{utilità} come precisione della rappresentazione interna
  del dato rispetto a il compito svolto (e quindi, ad esempio, un'immagine
  ritoccata potrebbe essere più utile per determinati scopi)
  \item \textbf{fedeltà} come l'aderenza del dato alla realtà
\end{itemize}
Si è capito che la qualità è un concetto complicato.
\begin{definizione}
  Diamo una serie di definizioni:
  \begin{itemize}
    \item \textbf{qualità}: caratteristiche di un artefatto che influiscono
    sulla sua capacità di soddisfare le esigenze e le aspettative dell'utente,
    dichiarate o implicite. Un dato è di qualità se è adatto all'uso che se ne
    deve fare. Si ha il concetto di \textbf{fitness for use}. ``La qualità dei
    dati è negli occhi di chi li usa e non nelle mani di chi li produce''
    \item \textbf{dimensione}: una caratteristica specifica che descrive la
    qualità delle informazioni, solitamente non misurabile ma potrebbero esserlo
    \item \textbf{sottodimensioni}: sotto-caratteristiche che spesso
    classificano una certa dimensione
  \end{itemize}
  Le dimensioni misurabili lo sono tramite \textbf{metriche}, che, secondo le
  definizioni dello standard ISO 9126-1 e secondo il framework ISM3 sono
  un'insieme di procedure che comprende:
  \begin{itemize}
    \item una procedura (o metodo) di misurazione, cioè un algoritmo che prende
    l'elemento da misurare e lo associa a una misura (sia esso un valore
    ordinale o un intervallo)
    \item una corretta unità di misura (o scala), cioè
    il dominio dei valori restituiti dalla procedura di misurazione
  \end{itemize}
  Si possono avere metriche diverse per la stessa dimensione 
\end{definizione}
La qualità dei dati è un concetto che può essere espresso attraverso molteplici
dimensioni, ad esempio la accuratezza (magari anche solo per errori di typo nei
dati), la comprensibilità, la completezza (avendo magari valori a NULL),
l'inconsistenza (avendo contraddizioni nei dati) etc$\ldots$\\  
\textbf{Su slide tabella con le metodologie}.\\
Un altro aspetto da tenere in considerazione è che si hanno metriche:
\begin{itemize}
  \item \textbf{oggettive}, modi formali e precisi per misurare le metriche per
  una dimensione di qualità in termini di valori di un dominio,
  indipendentemente dalla percezione/valutazione umana 
  \item \textbf{soggettive}, modi per misurare le metriche per una dimensione di
  qualità che dipendono dalla percezione (di solito valutazione esplicita e
  ordinale) delle persone coinvolte nel processo di misurazione, dipendente
  dalla percezione/valutazione umana 
\end{itemize}
Vediamo qualche aspetto nel dettaglio.\\
Partiamo dall'\textbf{accuratezza}.
\begin{definizione}
  La precisione di un valore $v$ è definita come la vicinanza tra $v$ e un
  valore $v'$ considerato come la corretta rappresentazione del fenomeno del
  mondo reale che $v$ intende rappresentare.\\
  Impatta valori alfanumerici e si può applicare alle tuple e alle relazioni.
\end{definizione}
Un esempio di inaccuratezza è un typo.\\
Si hanno due sottodimensioni di questa qualità:
\begin{enumerate}
  \item \textbf{accuratezza sintattica}, magari controllando se la stringa è
  presente in un insieme di valori di riferimento per il dominio trattato
  (magari un vocabolario, una lista di città etc$\ldots$). Ci sono dati su cui
  non è applicabile. Generalmente si fa:
  \begin{itemize}
    \item in base alle stringhe
    \item in base ai singoli token di una stringa
  \end{itemize}
  \item \textbf{accuratezza semantica}, molto difficile da verificare (magari
  tramite un'altra sorgente dati per cross validare). Per piccolissimi db si fa
  a mano ma è generalmente impensabile
\end{enumerate}
Conoscere lo schema aiuta a valutare la accuratezza dei valori e si hanno
diverse metriche. In primis posso dire se un dato è vicino a quello che mi
aspetto in un certo dominio tramite una funzione di distanza che confronta il
valore che ho e il dominio di riferimento. Dopo questo posso pensare a
correggere eventualmente il dato. Si cerca il valore più vicino al dominio di
riferimento. Potrei anche fare una stima statistica per capire quale sia il
valore corretto. Per il calcolo delle distanze su stringhe uso la solita
\textbf{distanza di edit (non normalizzata)} ma non è la sola. In fase di
assesment considero solo 
valori con una massima distanza di edit stabilita e nella fase di improvment
posso stimare la giusta correzione per ottenere la stringa magari corretta
(magari in corrispondenza di typo). La distanza sintattica potrebbe però essere
poco informativa in merito alla semantica (si pensi a Domenico spesso abbreviato
in Mimmo). Possiamo introdurre la \textbf{distanza di edit normalizzata} come:
\[1-\frac{edit(a,b)}{n}\]
con $n$ numero massimo di simboli. Si ottiene un valore tra 0 e 1, con 1 che
indica che i valori che sono identici, accuratezza pari a uno implica distanza
nulla (misuro il complemento a 1 della distanza). Tali valori tra 0 e 1 sono
comodi in 
quanto spesso le metriche danno risultati che sono in questo range e posso
quindi effettuare una somma pesata tra le varie metriche.\\
Si hanno vari tipi di funzione di edit (ma limitate a stringhe corte):
\begin{itemize}
  \item Hamming distance
  \item Levenshtein distance
  \item Jaro-Winkler
\end{itemize}
o di distanza di token (per stringhe composte da più stringhe, esempio ``Sesto
San Giovanni''):
\begin{itemize}
  \item Jaccard index
  \item Sorensen-Dice
  \item N-gram
\end{itemize}
Si possono avere problemi nel trovare la reference table di un certo dominio. \\
Passiamo alla completezza.
\begin{definizione}
  Definiamo la \textbf{completezza} di un insieme di dati come
  la copertura con la quale il fenomeno osservato è rappresentato nell'insieme
  di dati. Avere ad esempio dei NULL impedisce la completezza.\\
  Impatta valori alfanumerici e numerici e si può applicare alle tuple, agli
  attributi, agli oggetti e alle relazioni.\\
  Si ha che, normalizzando sempre con $1-$ per avere massima completezza a 1:
  \begin{itemize}
    \item la completezza di tupla è basata sul numero di NULL di una certa riga
    rispetto al numero di attributi presenti
    \item la completezza di attributo è basata sul numero di NULL di una certa
    colonna rappresentante un attributo
    \item la completezza di tabella è basata sul numero di NULL dell'intera
    tabella 
    \item la completezza di oggetto è il numero di valori non nulli in tutti gli
    oggetti rappresentati nelle tuple 
  \end{itemize}
\end{definizione}
Nelle precedenti definizioni, abbiamo assunto una ipotesi di \textbf{mondo
  chiuso}, ovvero tutto
ciò che è rappresentato nella db è vero, tutto il resto è falso. Questa è
la tipica ipotesi che si fa nei DBMS. Posso ipotizzare un \textbf{mondo aperto},
come nei DBMS NoSQL, dove si tace di ciò che non si sa, avendo \textbf{object
  completness}, tenendo conto del fatto che gli oggetti rappresentabili sono
più delle tuple della tabella, e di tale cardinalità serve una stima
indiretta. Quindi nel mondo n on relazionale è difficile stimare la completezza,
non avendo certezza del numero di oggetti.  \\
Parliamo ora di \textbf{proprietà temporali}, di livello di aggiornamento,
ovvero di \textbf{currency}. Si parla anche di tempestività. Trattando il tempo
è difficile dare una definizione formale e specificare la semantica trattata. Si
ha che la \textbf{currency} misura con quale rapidità i dati sono aggiornati
rispetto 
al corrispondente fenomeno del mondo reale. Una prima misura della currency è
il ritardo temporale tra il tempo $t1$ dell’evento del mondo reale che ha
provocato la variazione del dato, e l’istante $t2$ della sua registrazione nel
sistema informativo ma questa misura è costosa in quando generalmente l'evento
non è ben noto, non sapendo quando si hanno cambiamenti etc$\ldots$. Un'altra
metrica di currency è vederla come come differenza tra tempo di arrivo alla
organizzazione e tempo in cui è effettuato l’aggiornamento (cosa misurabile in
presenza di log, ad esempio, sui tempi di update). Un altra metrica è la
differenza rispetto al metadato ``ultimo aggiornamento effettuato'' che, per
valori con periodicita’ di aggiornamento nota, currency calcolabile in maniera
approssimata ma poco costosa (ma non si hanno informazioni in merito alle
modifiche del dato). \\
La \textbf{tempestività} misura quanto i dati sono aggiornati rispetto a un
particolare processo (o ai processi) che li utilizza. Quindi è dipendente dal
processo, a differenza della currency, ed è associabile al momento temporale in
cui deve essere disponibile per il processo che utilizza il dato. Posso avere
dati obsoleti per il processo di chi li usa ma con alta currency.\\
Trattiamo infine la \textbf{consistenza}.\\
\begin{definizione}
  Si definisce la \textbf{consistenza} in due modi:
  \begin{enumerate}
    \item come consistenza dei dati con i vincoli di integrità o dipendenze
    funzionali definiti sullo schema (ad esempio i vincoli di integrità nel
    modello relazionale). Si hanno anche dei vincoli di consistenza, detti
    business rule, che possono riguardare uno o più attributi, relazioni
    etc$\ldots$ e possono essere espressi in termini di probabilità. Si usano
    anche vincoli di integrità in logica e si hanno dei data edit nelle indagini
    statistiche 
    \item come consistenza delle diverse rappresentazioni di uno stesso oggetto
    della realtà presenti nella base di dati (come visto nella parte di data
    integration)    
  \end{enumerate}
\end{definizione}
Invece con \textbf{accessibilità} si esprime la capacità di un utente di
accedere ai dati a partire dalla propria cultura, stato fisico e psichico e
dalla tecnologie disponibilità, magari dal punto di vista culturale, dello stato
fisico, delle tecnologie etc$\ldots$\\
Un altro aspetto importante che va sempre considerato è il \textbf{tradeoff tra
  varie qualità}, ad esempio consistenza e completezza nel modello relazionale
possono essere non conciliabili quando si voglia rispettare l'integrità
referenziale. Nel dominio statistico si ha tradeoff tra tempestività e
completezza/accuratezza, che vengono però privilegiate (bisogna quindi studiare
quanto è ``sporco'' un certo dato). Nel web si preferisce la tempestività
rispetto ad accuratezza/completezza.
\section{Quality improvment}
Avendo quindi già introdotto la fase di \textbf{quality assesment} passiamo a
quella di \textbf{quality improvment}, ovvero al miglioramento dei dati
stessi.\\
L'obiettivo della fase di miglioramento, una volta misurata la qualità dei dati
a aver scoperto che magari sono di bassa qualità rispetto alle esigenza,
è quello di migliorare i dati. La qualità dei dati è un problema di tipo
multidimensionale, potendo decidere di migliorare anche solo alcuni aspetti
relativi alla qualità dei dati (come la completezza, la consistenza,
l'aggiornamento temporale etc$\ldots$), sapendo che ci sono dei tradeoff quindi
il miglioramento di alcuni aspetti va a discapito della qualità di altri. Si
hanno in generale due strategie in fase di miglioramento: 
\begin{enumerate}
  \item \textbf{data-driven}, migliorando il dato stesso in quanto tale. Si
  punta a migliorare la qualità dei dati modificando direttamente il valore dei
  dati attraverso il confronto con altri dati ritenuti di buona qualità. Ad
  esempio, i valori dei dati obsoleti vengono aggiornati aggiornando un database
  caratterizzato da una valuta più alta. Si migliora il dataset stesso. Questo è
  una soluzione molto usata in data science. Questa strategia si applica se la
  raccolta dati è continuativa.\\
  Si hanno varie procedure:
  \begin{itemize}
    \item \textbf{acquisizione di nuovi dati}, che migliora i dati acquisendo
    dati di qualità superiore per sostituire i valori che sollevano problemi di
    qualità. Questa strategia può essere utilizzata per tutte le dimensioni
    esaminate nella fase di valutazione 
    \item \textbf{record linkage}, detta anche \textbf{identificazione degli
      oggetti}, che confronta i dataset con valori ``sporchi'' con una fonte
    certificata o di qualità superiore, identificando le tuple/record nei due
    dataset che potrebbero fare riferimento allo stesso oggetto del mondo
    reale e pulendo i ``valori'' sporchi con corrispondenti valori di qualità
    superiore
    \item \textbf{affidabilità della fonte}, dove si selezionano le fonti di
    dati sulla base della qualità dei loro dati 
  \end{itemize}
  \item \textbf{process-driven}, migliorando il processo di acquisizione dei
  dati (datti errati possono portare ad errori sistematici), ottenendo che il
  dataset viene alimentato con dati corretti. Si punta 
  quindi a migliorare la qualità ridisegnando i processi che creano o modificano
  i dati. Ad esempio, un processo può essere riprogettato includendo un'attività
  che controlla il formato dei dati prima dell'archiviazione. Tale strategia fa
  riferimento all'ambito detto \textbf{Business Process Reengineering (BPR)},
  ovvero avendo la possibilità di riprogettare i processi, magari scoprendo che
  una sorgente era di qualità troppo bassa.\\
  Si hanno due tecniche:
  \begin{enumerate}
    \item \textbf{process control}, dove si aggiungono elementi e/o procedure
    di controllo nel processo di produzione dei dati quando:
    \begin{itemize}
      \item vengono creati nuovi dati
      \item vengono aggiornati i set di dati 
      \item il processo accede a nuovi set di dati
    \end{itemize}
    Verificando errori e la qualità dei dati stessi.\\
    In questo modo, viene applicata una strategia ``reattiva'' agli eventi di
    modifica dei dati, evitando così la degradazione dei dati e la propagazione
    degli errori
    \item \textbf{process redesign}, dove si ridisegnano i processi per
    rimuovere le cause della scarsa qualità e introduce nuove attività che
    producono dati di qualità superiore. Se la riprogettazione del processo è
    radicale, questa tecnica viene definita \textit{reingegnerizzazione dei
      processi aziendali}. Queste tecniche sono puntuali e non sono facilmente
    integrabili in un sistema di raccolta continua dei dati
  \end{enumerate}
\end{enumerate}
Nel lungo termine le tecniche process-driven sono più efficaci, eliminando il
problema alla radice, ma sono estremamente costose nel breve termine. Le
tecniche data-driven sono più economiche ma costose nel lungo e quindi sono
adatte per un'applicazione ``one-shot'' e, quindi, sono consigliati per i dati
statici. \\
Approfondiamo il data-driven.\\
Si hanno miglioramenti specifici per la dimensione:
\begin{itemize}
  \item \textbf{accuratezza} tramite confronto dei valori con un dominio di
  riferimento. Questo è il tipo di tecnica che abbiamo considerato
  per l'accuratezza sintattica nella fase di quality assesment
  \item \textbf{completezza} tramite completamento di dati incompleti con
  tecniche specifiche che sfruttano la conoscenza sui dati 
  \item \textbf{consistenza} tramite l'identificazione dei dati corretti
  sfruttando vincoli di integrità, dipendenze funzionali o dati derivati
  etc$\ldots$, correggendo poi tramite sorgenti esterni o acquisendo nuovamente
  i dati
\end{itemize}
Come tecnica si usa anche la \textbf{error localization and correction} che
identifica ed elimina gli errori di qualità dei dati rilevando i valori che non
soddisfano un dato insieme di regole, dette \textbf{edits} nelle metodologie per
i le analisi statistiche (queste tecniche sono infatti studiate principalmente
nel dominio statistico). Rispetto ai dati elementari, i dati statistici
aggregati (come media, somma, massimo etc$\ldots$), sono meno sensibili alla
localizzazione probabilistica e alla correzione dei valori probabilmente
errate. Sono state proposte tecniche per la localizzazione e correzione degli
errori per incongruenze, dati incompleti e valori anomali, ovvero valori
significativamente diversi da tutti gli altri valori in un set di dati per dati
elementari e dati statistici.\\
Un altra tecnica di quality improvment è la \textbf{deduplica} che corrisponde
al raggruppamento di record di dataset che si riferiscono alla stessa entità nel
mondo reale (raggruppo i doppioni). Praticamente la deduplica è un'operazione di
record linkage fatta su un'unica tabella. Si hanno tre gruppi in output alla
deduplica:
\begin{enumerate}
  \item coppie/gruppi di tuple che matchano, corrispondenti allo stesso
  oggetto reale
  \item coppie di tuple che non matchano che corrispondono a entità distinte nel
  mondo reale 
  \item possibili coppie/gruppi di tuple, per le quali non è stato possibile
  giungere una conclusione definitiva, e sono necessarie ulteriori indagini (e
  costi) per assegnarle al gruppo match o mismatch
\end{enumerate}
Un'altra attività classica, propedeutica ad altre attività (è una sorta di fase
di preprocessamento), è la fase di
\textbf{normalizzazione}, trasformando le stringhe, scritte nello standard del
dominio di riferimento, effettuando, per ogni dimensione, il riconoscendo della
stessa e il relativo miglioramento. Per la normalizzazione di domini specifici,
tipo i nomi delle vie etc$\ldots$, esistono tool specifici.\\
Si ha quindi il miglioramento di singoli valori o di intere tuple, migliorando
accuratezza sintattica, completezza, currency e consistenza, in base a certe
metriche. \\
Ogni valore che non matcha con la tabella di riferimento (spesso disponibili
online a seconda del dominio ma non sempre) deve essere corretto. Il
miglioramento dell'accuratezza sintattica viene effettuato sostituendo il valore
errato con quello a distanza inferiore nella tabella di riferimento, dove la
distanza può essere valutata dopo una precedente verifica dei valori e delle
loro caratteristiche (si nota come le attività data-driven sono costose dal
punto di vista temporale). \\
Mentre per migliorare l'accuratezza possiamo adottare procedure standard di
confronto con le tabelle di riferimento, per la completezza è molto più
complesso, non avendo un elemento da cui partire, avendo a disposizione solo un
valore NULL, non riuscendo sempre a ricostruire il valore corretto per
sostituirlo al NULL. Spesso comunque si ha un contesto per aiutare a rimuovere i
NULL, quindi, la procedura più intuitiva è eseguire una nuova acquisizione di
dati incompleti riempiendo di volta in volta più valori NULL possibili. Poiché
di solito si tratta di un'attività molto costosa, 
possiamo adottare diverse euristiche che di solito dipendono dal contesto. \\
Per la completezza si usano anche i \textbf{dati derivati}, definiti come dati
per i quali esiste una formula matematica per la quale questi dati possono
essere ottenuti a partire da un altro set di dati già conosciuto (esempio banale
un prezzo con o senza IVA).\\
Bisogna migliorare la consistenza vedendo se la dipendenza funzionale sui dati
derivati e altri vincoli di integrità intra-relazionale possono essere
sfruttati, oltre che per la completezza completezza, anche per la consistenza.\\
\textbf{Esempio case study su slide}.\\
Dopo tutte queste operazioni si ottiene un db già abbastanza pulito ma si hanno,
solitamente, ancora problemi con spazio di miglioramento. Si userà quindi il
\textbf{record linkage}.
\subsection{Record Linkage}
Il \textbf{record linkage} viene usato anche in altri contesti oltre il data
quality con anche altri nomi (come object recognition, object matching,
deduplicazione, se si tratta di un solo dataset, etc$\ldots$). \\
Prima della fase di data integration si ha lo \textit{scheme integration}, con
approcci architetturali. Dal punto di vista di integrazione si hanno, si
ricorda, gli approcci GAV o LAV, ma in entrambi i casi si hanno elementi
rappresentanti lo stesso oggetto reale in db diversi che
potrebbero non essere facilmente  integrati a causa, magari, dell'assenza di una
chiave primaria univoca. Bisogna quindi studiare come collegare i record
presenti nei vari db tramite \textbf{record linkage} e mettere poi insieme i
risultati con un'operazione di \textbf{fusion}, cercando di studiare eventuali
ridondanze o ambiguità.\\
Il record linkage consiste quindi nell'identificare le stesse osservazioni in
diversi file/db. In generale si parla di \textbf{entity resolution task/record
  linkage} avendo varie ``varianti'':
\begin{itemize}
  \item \textbf{deduplicazione}, che considerando una sola tabella è usata
  principalmente con il modello ER. Si procede raggruppando record che
  corrispondono allo stesso oggetto reale normalizzando lo schema e riducendo il
  numero di record nel dataset. Come variante si hanno i cluster di calcolo
  \item \textbf{record linkage}, dove appunto si matchano da un archivio dati
  deduplicato a un altro (bipartito). Si ha anche la versione $k$-partita
  lavorando con più data store. Generalmente questo metodo proposto nei
  datastore relazionali, ma più frequentemente applicato a record non
  strutturati da varie fonti 
  \item \textbf{canonicalizzazione}, che generalmente fornisce il record più
  completo, attribuisce i valori tramite la fusione, costruendo dei
  \textbf{single version of truth}, usando metodi probabilistici. Questo metodo
  è tipico nel \textit{master 
    data management}, dove si raccolgono tutti i dati per poi integrarli
  \item \textbf{referencing}, detto anche \textbf{entity disambiguation}, dove
  si matchano record ``sporchi'' con uno ``pulito'', con una tabella di
  riferimento deduplicata che è già stata canonizzata. Questo metodo
  generalmente utilizzato per atomizzare più record sulla stessa chiave primaria
  e donare informazioni aggiuntive al record  
\end{itemize}
Si hanno vari tool per la entity resolution:
\begin{itemize}
  \item \textit{NTLK}, un natural language toolkit
  \item \textit{Dedupe*}, per lo structured deduplication
  \item \textit{Distance}, un'implementazione in C per distance metrics
  \item \textit{Scikit-Learn}, per machine learning models
  \item \textit{Fuzzywuzzy}, per fuzzy string matching
  \item \textit{PyBloom}, per probabilistic set matching
\end{itemize}
In input al record linkage si ha quindi una serie di tabelle e in output un
insieme di tuple che matchano e un insieme di tuple che non matchano, nel caso
relazionale. Si hanno anche tuple per cui non si sa dire nulla o per le quali
non si ha certezza su match/mismatch. Lo stesso ragionamento si applica anche a
modelli non relazionali (dati nei nodi per un modello a grafo, valori dei
documenti nel documentale etc$\ldots$).\\
Si hanno varie tecniche per la comparazione delle coppie:
\begin{itemize}
  \item quella \textbf{empirica}, basata sulla distanza di simboli nei valori
  delle tuple
  \item quella \textbf{probabilistica}, dove presumo di conoscere un campione di
  frequenze di "distanze" tra coppie di tuple, note come corrispondenti o non
  corrispondenti e dove potrei decidere quelli corrispondenti e quelli non
  corrispondenti nell'universo proiettando tale conoscenza sul campione
  nell'universo della coppia. Si usano soglie per poter discernere match e
  mismatch 
  \item quelle \textbf{knowledge based}, dove si decide in base a regole per il
  match delle tuple
  \item \textbf{mista}, sia probabilistica che knowledge based 
  
  \item le recenti tecniche di \textbf{machine learning}
\end{itemize}
Ovviamente grosse tabelle portano, confrontando tutti gli elementi ogni volta, a
complessità assurde. Si hanno quindi vari step per il record linkage:
\begin{itemize}
  \item costruzione dello spazio di ricerca iniziale come prodotto
  cartesiano di tutti i dataset in input. Tale spazio è immenso. Questa è una
  fase di preprocessamento e in questa fase si cerca di ottenere un formato
  unico per tutti i sorgenti, sia dal punto di vista del modello che da quello
  dei dati (normalizzando i vari valori di uno stesso dominio, ad esempio avendo
  un solo modo per definire via, senza abbreviazioni, rimuovendo spazi
  etc$\ldots$) 
  \item si usano tecniche di blocking per ridurre lo spazio di ricerca,
  ottenendo uno spazio di ricerca ridotto. Si hanno varie tecniche di blocking
  in letteratura e vari algoritmi. Tra tutti si ha il \textit{sorted neighbour}
  dove si prende un attributo e cerco di ordinare la tabella sugli insiemi degli
  attributi in modo tale che i gruppi degli attributi che confronto siano vicini
  tra loro, tutti con tutti (esempio ordino per cognome in modo da mettere
  insieme record con cognomi vicini per vedere magari errori, ovviamente
  funziona entro certi limiti). Ci si muove tra gli elementi tramite sliding
  window per specificare meglio i controlli, riducendo i confronti (senza la
  finestra ho $n^2$ confronti, con $n$ righe, con la finestra di lunghezza $m$
  righe ho $m^2\frac{n}{m}$, quindi $m\cdot n$ confronti, avendo $m\ll
  n$). Questa fase è facilmente distribuibile, facendo finestre di controllo in
  parallelo i più nodi
  \item sullo spazio ridotto si usa un metodo di comparazione, a cui è associato
  un metodo di decisione, per definire i match, i mismatch e quelli di cui non
  si ha certezza, che chiamiamo possible match (non sempre si hanno, dipende
  dalle soglie usate, una soglia di certo non permette di creare i possible
  match due invece si, nel mezzo tra le due si hanno i possible match). Si hanno
  metodi probabilistici, metodi non supervisionati come il clustering e anche
  supervisionati come: 
  \begin{itemize}
    \item svm
    \item logistic
    \item random forest
    \item boosting gradient
  \end{itemize}
  Confrontando quindi le varie coppie di dati
\end{itemize}
Questo è lo schema di base diciamo e in ogni fase si ha un processo di quality
assesment.\\ 
Il primo problema è che i dataset potrebbero essere di formato diverso,
relazionale o meno, e quindi, come i data integration, si cerca di trasformare
un modello nell'altro. Potrei dover unire in un unico formato anche immagini e
mappi, oltre a documenti json o csv etc$\ldots$ \\
\textbf{Esempio del processo su slide.}\\
Approfondiamo quindi l'uso di tecniche \textbf{probabilistiche} per la fase di
comparazione, avendo un \textbf{probabilistic record linkage}, dove comunque le
prime fasi restano normali.\\
Quindi si cerca la probabilità di match tra le coppie usando dei vettori di
attributi:
\[P_{match}=\sum_{score\in A}w\cdot score\]
Avendo:
\begin{itemize}
  \item $w$ pesi tra 0 e 1, avendo peso maggiore per feature meglio predittibili
  \item $A$ vettore degli attributo ciascuno con un punteggio $score$
\end{itemize}
Se il punteggio così pesato supera una soglia ottengo un match.\\
Si hanno anche meccanismi basate su regole (avendo magari che lo score di un
attributo deve essere maggiore di una soglia e quello di un altro di un'altra
soglia). Sebbene la formulazione di regole sia difficile, è possibile applicare
regole specifiche del dominio rendendo più specifico l'approccio.\\
La soluzione proposta da \textbf{Fellegi Sunter}, quella probabilistica, è
ottimale quando gli attributi di confronto erano condizionatamente
indipendenti, permettendo comunque di avere un'area di ambiguità.\\
Bisogna quindi per ogni coppia definire una funzione di distanza, imponendo un
insieme finito di attributi da usare per valutare tale distanza, potrebbero
esserci attributi non rilevanti. Un modo ``ottimale'' è scegliere attributi
dello stesso significato e con la maggior accuratezza possibile per evitare
errori. Si hanno anche attributi discriminatori per i quali i domini contengono
un alto numero possibile di valori (ad esempio la lista di nomi, anche
se in questo caso la distribuzione è eccessiva, o dei cognomi, già più
significativa). Meno attributi si prendono in considerazione più rischio falsi
positivi, aumentando i matching, anche se aumento le prestazioni. Scegliere
troppi attributi può portare a falsi negativi, avendo tantissimi mismatch. È
quindi un discorso che va approfondito di caso in caso.\\
L'algoritmo di Fellegi Sunter, per la fase decisionale, suggerisce di
campionare la frequenza di match e non match, prendendo una coppia che si sa
già che matcha e valutando per ogni distanza la frequenza di match/mismatch
(???). Per ogni distanza avrò alla fine associata una percentuale di match. Si
fa un ragionamento in stile SVM anche se senza iperpiani ma con un separatore
più complesso.\\
\textbf{Esempio su slide.}\\
All'aumentare della distanza ovviamente aumentano i match. Alla fine del
ragionamento posso produrre le soglie (o la soglia) da usare per lo studio
dell'intero dataset.\\
Si rischiano comunque falsi positivi e falsi negativi che possono essere ridotti
aumentando i possible match, allargando le soglie.\\
Il record linkage, nel calcolo della distanza, è comunque molto legato al
dominio.\\
Approfondiamo ora le tecniche di comparazione basate sul machine learning. Si
hanno approcci supervisionati, SVM, random forest, conditional random fields
(nel caso di formati non relazionali) etc$\ldots$ \\
Ci sono anche tecniche non supervisionate, tramite clustering, con l'idea di
usare K-means per raggruppare elementi simili. Posso fare anche clustering
gerarchici p usando la expectation maximization. Si hanno anche tecniche di
apprendimento attivo come il commitee of classifiers.\\
Matchare testi descrittivi lunghi è difficile ma con tecniche di distanza non
euclidea e di embedding, in un contesto comunque di machine learning, si riesce
ad ottenere dei risultati.\\ 
In generale comunque il record linkage deve essere veloce, anche a discapito di
un minimo di qualità.\\
Bisogna quindi valutare la qualità in termini di:
\begin{itemize}
  \item true positive, TP
  \item true negatives, TN
  \item false positive, FP
  \item false negative, FN
\end{itemize}
avendo:
\[recall=\frac{TP}{TP+FN}\]
\[precision=\frac{TP}{TP+FP}\]
Avendo un'elevata conoscenza di dominio posso usare tali conoscenze per la
comparazione e la cosa è difficilmente generalizzabile.
\subsection{Data fusion}
Passiamo quindi alla fusione dei record che rappresentano lo steso oggetto nel
mondo reale.\\
Bisogna gestire i conflitti. Si hanno varie tecniche \textit{(traduzione a
  spanne da slide, il prof non ha detto quasi nulla}):
\begin{itemize}
  \item \textbf{ignorare il conflitto}, non è una buona soluzione. Queste
  strategie non consentono di prendere la decisione in base a ciò che è in
  conflitto con i dati e a volte non vengono rilevati i conflitti di
  dati. Esempio di strategia di ignoramento è \textbf{Pass It On}, che presenta
  tutti i valori e quindi rimuove il conflitto con la soluzione all'utente 
  \item \textbf{evitare il conflitto}, con strategie basate su istanze e
  metadati, scegliendo di prendere istanze senza conflitti, prendendo silo
  valori consistenti senza dire niente degli altri. Queste strategie
  riconoscono l'esistenza di possibili conflitti generali, ma non rilevano e
  risolvono i singoli conflitti esistenti. Invece, gestiscono dati in conflitto
  applicando una decisione unica allo stesso modo a tutti i dati, ad esempio
  preferendo un dato da una fonte speciale con la strategia \textbf{Trust your
    friend} 
  \item \textbf{risolvere il conflitto}, prendendo una sorgente più sicura e
  facendo una scelta a caso su vari aspetti, non arrivo ad una soluzione ottima
  (???). Queste strategie considerano i dati e i metadati prima di risolvere il
  conflitto. Si hanno due strategie:
  \begin{enumerate}
    \item decidere quando scegliere un valore tra tutti i valori già presenti
    (ad esempio il dato più recente vedendo i log)
    \item strategie di mediazione, quando scelgono un valore che non
    necessariamente esiste tra i valori inconsistenti
  \end{enumerate}
\end{itemize}
La scelta di fusione deve essere comunicata a chi poi usa i dati altrimenti si
rischiano molti problemi (magari si è scelto di scegliere lo stipendio minimo in
caso di inconsistenza e un addetto della contabilità deve saperlo).
È il tema della \textbf{provenance dei dati}, il tenere traccia delle decisioni
prese. Con al data fusion si ottiene potenzialmente qualcosa con pochi NULL.\\
\textbf{Su slide esempio con intero caso di studio.}\\
Alcune note con conclusive.\\
La \textbf{data preparation} è un buon modo per ottenere ottimi risultati,
tramite normalizzazione di dati e schemi e \textit{imputation}. Si rendono più
facili i riscontri tramite distanze sintattiche. Nel data normalizazion si cerca
di:
\begin{itemize}
  \item convertire tutto in lowercase e rimuovere gli spazi
  \item usare uno spell checker per rimuovere typo conosciuti
  \item espandere abbreviazione
  \item convertire nickname
  \item fare lookup lessicali
  \item applicare tecniche di stemming, tokenizzazione lemmatizzazione delle
  parole (bho ???)
\end{itemize}
Per lo schema normalizazion si ha il tentativo di:
\begin{itemize}
  \item matchare nomi di attributi che specificano la stessa cosa
  \item spezzare attributi dove necessario
  \item gestire valori nulli
  \item rappresentare i dati in modo strutturato
\end{itemize}
Si ha poi l'imputation, la fase di decisione, dove si cerca di:
\begin{itemize}
  \item capire come gestire/comparare i NULL
  \item capire come gestire/comparare, in modelli non relazionali, l'assenza di
  valori, magari mettendo nei none o valori medi con una certa distribuzione, a
  seconda del caso
\end{itemize}
Queste scelte impattano molto sul record linkage.\\
\textbf{Su elearning slide del seminario Accenture}.
\begin{shaded}
  \textit{Vengono qui aggiunte le cose dette in live.}\\
  Si ha un forte legame tra architetture dati e machine learning. Si ha forte
  interesse nel ben definire i dati da dare in pasto agli algoritmi di machine
  learning, per avere buoni risultati (\textbf{su slide intera pipeline e varie
    analisi in merito al discorso}).\\
  \textbf{Le slide di questo incontro sono programma d'esame.}
\end{shaded}
\chapter{Big data}
Si studia ora la tematica dei \textbf{big data} ovvero come definire piattaforme
per la gestione di un grosso volume di dati.\\
Non si ha una definizione formale di big data ma nel 2012 Gartner disse:
\begin{center}
  \textit{Big data is high volume, high velocity, and/or high variety
    information assets.} 
\end{center}
Anche se si resta molto nel generico. Si identificano comunque le prime
\textbf{tre ``V'' dei big data}:
\begin{itemize}
  \item \textbf{volume}, parlando di \textit{data at rest}
  \item \textbf{variety}, parlando di \textit{data in many form}
  \item \textbf{velocity}, parlando di \textit{data in motion}
\end{itemize}
Successivamente si è anche aggiunta la \textbf{quarta ``V'' dei big data}:
\begin{itemize}
  \item \textbf{veracity}, parlando di \textit{data in doubt}
\end{itemize}
indicando che non si ha effettiva fiducia nei dati prodotti. Si hanno poi altre
due ``V'' (spesso legate al concetto di \textbf{open data}, al come si usano i
dati): 
\begin{itemize}
  \item \textbf{visibility}, parlando di \textit{data in the open}
  \item \textbf{value}, parlando di \textit{data of many value}
\end{itemize}
Analizzare i dati su singola macchina è lento, costoso e difficile. La prima
idea è quindi quella di distribuire i dati, distribuendo il loro storage ma
anche il calcolo, in sistemi paralleli. Bisogna quindi studiare
sincronizzazione, deadlock, bandwith, coordinazione, fallimenti etc$\ldots$,
tipici dei sistemi paralleli. Negli ultimi anni, prima grazie ai grossi player e
poi grazie alle soluzioni cloud, si hanno ottime soluzioni dal punto di vista
del parallelismo soluzioni complesse ``sotto la scocca'' ma semplici da usare ad
alto livello.\\ 
L'elemento fondamentale è quindi il passaggio da solo storage all'aggiunta
dell'elaborazione di grandi quantità di dati, possibilmente con scalabilità
lineare (all'aumentare dei nodi scalo linearmente). Le elaborazioni vengono
quindi spostate dove sono i dati e non viceversa infatti normalmente ci si
connetteva al db, si effettuava la query, si scaricavano in locale i risultati e
si elaboravano. Ma questo non è efficiente con grandi quantità di dati (anche
nell'ordine di terabyte) quindi conviene spostare l'elaborazione (che pesa molto
meno in termini di codice sorgente). Ovviamente bisogna approfondire il tema
della gestione dei fallimenti (i nodi possono cadere), soprattutto girando su
hardware economico (commodity). Si hanno anche problemi di estensibilità che
devono essere gestiti.\\
I dati ormai sono il ``nuovo petrolio'' in qaunto bisogna \textit{trovarli},
\textit{estrarli}, \textit{rifinirli}, \textit{distribuirli} e usarli per
guidare l'economia.\\
Si hanno:
\begin{itemize}
  \item \textbf{grandi giacimenti di dati}, come i social, il web e gli opendata
  \item \textbf{piccoli giacimenti di dati}, come quelli locali di sensori, IoT,
  CRM (di piccoli volume)
\end{itemize}
Per estrarre i dati si hanno query, app dedicate, wrapper, log, stream
etc$\ldots$ Si hanno sorgenti nativamente relazionali (come CMR), alcuni
nativamente non relazionali, alcuni sono fermi altri in movimenti, alcuni da
sensori etc$\ldots$ comportando varie tecniche di estrazione.\\
Dal punto di vista dello storage si hanno due soluzioni:
\begin{itemize}
  \item \textbf{on-premises} (in casa)
  \item \textbf{on-cloud}
\end{itemize}
La scelta è fondamentale in ottica \textbf{ecosistema digitale}, ovvero un
insieme di tecnologie che si usano bene insieme. Sceglitore dove mettere i dati
comporta scelte specifiche sull'ecosistema digitale. Al momento i grandi
produttori di cloud (Amazon, Microsoft, Google, SAP etc$\ldots$) hanno un
ecosistema digitale per cui diventa anche difficile scegliere il migliore in
base ai propri utilizzi. Spesso si hanno comunque accordi commerciali con alcuni
produttori. Le soluzioni cloud comunque ``vincolano'' ad un certo ecosistema,
rendendo scomodo lì'uso di tecnologie di altri produttori.\\
I dati fanno comunque raffinati e puliti, si hanno procedure di estrazione,
trasformazione e caricamento, studi di privacy, integrazione e analisi dati con
data mining e analytics. \\
Si ha anche un problema di trasporto, garantendo interoperabilità per i dati
grezzi, mentre le piattaforme di big data costruiscono dei \textbf{wallet
  garden} ovvero dei ``giardini murati'', dove si ha integrazione solo
all'interno di quel dato ecosistema digitale. Si creano dei lock-in per cambiare
provider molto importanti (dovendo in caso riscrivere molto codice per
``trasportare i dati da un giardino all'altro'').\\
In una architettura di big data si ha quindi, oltre al sistema di storage, anche
un sistema di analisi dati. I darti vengono caricati, gestiti e messi in un'area
di staging. I dati vengono quindi processati e poi gestiti dal punto di vista
della sicurezza, del monitoraggio e della pulizia. Bisogna ovviamente anche
gestire l'accesso ai dati.
In generale si ha un hardware fisico, un framework per lo storage etc$\ldots$\\
Le nuove piattaforme di analisi dati non nascono in ambiente relazionale e i
risultati di tale analisi, insieme ai dati relazionali, finiscono nel data
warehouse. Un'alternativa posso usare, se ho solo dati relazionali, un
datawarehouse. Se si aggiungono dati non relazionali posso anche pensare di
aggiungere semplicemente un \textbf{data lake}, un'architettura di big data
analytics che fa analisi anche con machine learning (usando ad esempio R) e poi
contribuisce a popolare il datawarehouse.\\
Non sempre avere più fati comunque è la soluzione migliore, in quanto da la
percezione di avere analisi per forza migliori (ma non sempre è vero).\\
Il rischio di overfitting comunque è sempre dietro l'angolo, non sempre i dati
sono la risposta e non sempre l'analisi dati porta a correlazioni sensate
(spesso da modelli predittivi, basati comunque su dati biased, si passa a
modelli prescrittivi).
\section{HDFS}
Concentriamoci sullo storage di grandi quantità di dati.\\
I dati devono poter essere distribuiti per essere poi elaborati. Il sistema
software che permette di memorizzare dati che potenzialmente possono essere di
qualunque volume, con qualunque tasso di velocità di arrivo e di qualunque tipo
(per le prime tre ``V'', volume, velocity, variety, avendo \textit{any size, any
  rate, any type}). La tecnologia più efficace non è nulla che abbiamo già visto
(pensare ad un NoSQL obbliga ad un certo formato di dato) ma dobbiamo ragionare
in ottica di file system. Abbiamo quindi file system apposta per dati che
devono essere distribuiti, tra cui \textbf{Hadoop Distributed File System
  (\textit{HDFS})}, derivato open di \textbf{Google Distributed File System
  (\textit{GDFS})}. HDFS è molto performante anche sulle commodity, consente
replica tra nodi anche con fallimento di nodi, ha un meccanismo nativo per cui
ogni frammento di dato è replicato su tre nodi diversi e ha un approccio
\textbf{write once read many} (d'aiuto nella tipica situazione di raccolta dati
dai social e successiva analisi, sentiment analysis etc$\ldots$). I dati non si
spostano verso i ``workers'' ma 
è l'opposto. Prima distribuisco i dati sui vari nodi di elaborazione ed eseguo
lì i vari task. Questo permette di gestire il ``collo di bottiglia'' dei dischi,
che hanno accesso lento ma velocità effettiva accettabile, e di gestire meglio
la RAM.\\
Si fa quindi \textit{scale out} e non \textit{scale up}, avendo hardware
economico, si ragiona la gestione di pochi file grandi da elaborare di volta in
volta e si immagina un approccio di tipo \textit{write once read many}.\\
L'idea di HDFS consiste nel prendere un file lo si spezza in blocchi da 64MB (o
multipli, spesso 128MB), ogni pezzo, detto \textit{chunk}, viene distribuito in
almeno tre 
nodi. Si ha poi un'architettura master-slave che coordina la distribuzione e
l'accesso. Si ha il nodo \textbf{namenode} che ha tutte le informazioni
dell'infrastruttura HDFS (il master) e una serie di \textbf{datanode} (gli
slave) su macchine poco costose dove si salvano i chunk e che mandano
periodicamente degli heartbit al namenode (in modo che il namenode percepisca i
guasti e provveda con nuove repliche). Le richieste si fanno
al \textbf{namenode} che provvede poi a reindirizzare verso i giusti
\textbf{datanode}. Il namenode è il single point of failure ma si ha sempre un
\textbf{secondary namenode} verso il quale periodicamente si fa una transazione
dei log del namenode. I datanode non dialogano tra loro ma solo con il namenode
avendo un concetto di \textbf{cluster membership} per permettere al namenode di
coordinarli.  \\
Il write once read many contribuisce alla coerenza dei dati (tante richieste e
poche scritture, assumendo un carico di lavoro di tipo
\textbf{OnLine Analytical Processing (\textit{OLAP})}) e tramite il 
namenode si permette al client di accedere direttamente ai giusti datanode coi
chunk desiderati. \\
Tipicamente si ha una replica nello stesso rack, una in un rack vicino e una
terza molto lontano (ulteriori repliche i nodi arbitrari).  \\
Il namenode ha quindi tutte le informazioni in memoria, l'elenco dei file,
l'elenco dei blocchi, i soliti attributi/metadati di un file system (nome dei
file, nome 
delle directory, owner, dimensione etc$\ldots$ non il tipo, che non è dato
dall'estensione) e i log.\\ 
Il namenode quindi coordina i vari elementi e le operazioni tra file. Il
namenode si occupa anche di curare il ``benessere'' del sistema facendo repliche
e ribilanciamento, eliminando nodi morti.\\
I datanode invece ricevono i dati e verificano che siano corretti. Inoltre
segnalano il loro stato al namenode. La correttezza del dato viene verificata
tramite checksum, usando CRC32.
\section{Map reduce}
Si passa ora all'elaborazione dei dati.\\
\textbf{Map reduce} è un modello di programmazione/engine, ormai in disuso,
utile per capire nuove tecnologie. Questo engine permette di scrivere programmi
che sono realizzati in stile funzionale e eseguiti in parallelo su grandi
cluster di macchine poco costose. In un programma di map reduce si hanno due
fasi:
\begin{itemize}
  \item \textbf{map}, ovvero una funzione che processa coppie chiave/valore per
  generare coppie intermedie chiave/valore. Indicando con $k$ key e $v$ value:
  \[(k,v)\to[(k',v')]\]
  \item \textbf{reduce}, ovvero una funzione che mergia tutti i valori
  intermedi associati alla stessa chiave intermedia:
  \[(k',[v'])\to[(k',v')]\]
\end{itemize}
Si hanno poi:
\begin{itemize}
  \item \textbf{partition}, ovvero il numero di partizioni di $k'$. Spesso poi
  si usa un hash della chiave:
  \[hash(k')\,\,mod\,\,n\]
  dividendo lo spazio delle chiavi per l'esecuzione parallela della reduce
  \item \textbf{combine}, ovvero:
  \[(k',[v'])\to[(k',v'')]\]
  dove si hanno dei mini-reducers in memoria dopo il map usati come task di
  ottimizzazione per ridurre il traffico di rete
\end{itemize}
In pratica si prende l'insieme di attività e lo si divide, nella
\textit{partition}, in vari ``worker'' che 
producono risultati che poi vengono messi insieme, nella
\textit{combine}. Ovviamente bisogna studiare ogni step di divisione lavori,
gestione degli stessi e unione dei risultati:
\begin{itemize}
  \item come assegnare lavori alle unità di lavoro
  \item cosa succede se ci sono più work che workers
  \item cosa succede se i workers hanno bisogo di condividere risultati
  parziali
  \item come aggregare risultati parziali
  \item come capire che tutti i workers hanno terminato le loro attività
  \item cosa fare se un workers muore
\end{itemize}
L'idea base consiste, nella map, nell'iterare su un gran numero di record,
possibilmente 
eseguiti localmente, ed estrarre qualcosa di interessante da ognuno. Si mettono
poi insieme i risultati ordinando i risultati intermedi. Infine, nella reduce,
si aggregano i 
risultati e si genera il file di output. Tra map e reduce su ha una fase di
\textbf{shuffle \& sort} . Il framework si occupa di tutto il
resto, il developer si occupa solo della componente funzionale (map, reduce,
partition e combine) .\\
Quindi map reduce si occupa, su un filesystem distribuito, di:
\begin{itemize}
  \item gestione scheduling, rassegando i task di map e reduce ai worker
  \item gestione di data distribution, spostando i processi verso i dati
  \item gestione della sincronizzazione, ottenendo, ordinando e unendo i dati
  intermedi
  \item gestione di errori e fallimenti hardware, effettuando l'eventuale
  ripristino 
\end{itemize}
La comunicazione tra i nodi avviene in HDFS. Si istanzia un job di map reduce e
il client chiede al \textbf{job tracker}, che tiene conto dei vari job, di far
partire il lavoro. Il job tracker cerca tra i nodi liberi e assegna i job da far
eseguire, i cosiddetti \textbf{task tracker}. Il task tracker viene eseguito
localmente e riceve i parametri dal client e lancia su una vm locale o la map o
la reduce. Il job tracker periodicamente cheide se tutto funziona tramite
heartbit. \\
(\textbf{esempi su slide per map reduce})\\
Soluzioni moderne si trovano con \textbf{Spark}, che lavora in memory e non su
file system etc$\ldots$
\section{Hadoop}
Uniamo HDFS e map reduce e otteniamo \textbf{Hadoop}.\\
Hadoop quindi è un framework dove si ha un sistema di storage distribuito e un
sistema di calcolo parallelo. Su alcuni nodi si ha il job tracker e il namenode
(magari anche su macchine diverse). Sui nodi meno costosi si hanno poi datanode
e task tracker. Di fatto è sempre master/slave con single point of failure con
gestione repliche etc$\ldots$\\
Nell'ecosistema di Hadoop si hanno anche:
\begin{itemize}
  \item \textbf{pig} come linguaggio di scripting che viene tradotto in
  map-reduce. Esegue problemi di data analysis come un workflow ETL
  \item \textbf{hive} che fornisce un'interfaccia SQL-like per i dati in HDFS,
  in pratica si tratta il tutto alla datawarehouse (\textbf{Apache Hadoop based
    data warehouse}). Infatti map reduce aveva limiti (non riusabilità,
  complessità etc$\ldots$) e quindi si è pensato di aggiungere questo strato per
  l'interazione con SQL. Hive è intuitivo, genera automaticamente piani di
  esecuzione per le query, consente analisi alla datawarehouse e consente di
  tradurre query tramite esecuzioni di piani Hadoop Map Reduce. \textbf{Esempio
    su slide}. Architetturalmente si ha un'interfaccia web, si scrive la query,
  si ha la fase di parser, optimizer e planning, si eseguono script di map
  reduce user-defined o job già presenti di default (???). I dati vengono quindi 
  salvati in HDFS. Si ha un software quindi che serializza/deserializza i dati
  traducendo l'SQL (???). LA cosa viene usata in machine learning, data/text
  mining etc$\ldots$
\end{itemize}
\subsection{YARN}
Nella versione 1.0 di Hadoop map reduce doveva fare troppe cose quindi alla
versione 2.0 si è deciso di disaccoppiate le funzioni di gestione del cluster e
quelle di gestione dei singoli job, usando gli slave per la gestione dei job. Si
ha quindi \textbf{Yet Another Resource Negotiator (YARN)}. Ora quindi hanno un
\textbf{resource manager (RM)} globale, che gestisce l'intero cluster, e un
\textbf{application manager (AM)} per ogni applicazione. Per creare un nuovo job
si chiede all'RM le risorse e poi l'AM le gestisce come in map reduce.\\
In Hadoop 2.0 si ha quindi HDFS, YARN come \textit{data operating system} e
vari motori di elaborazione dati (Map Reduce, spark etc$\ldots$). Su questi poi
si hanno, per esempio:
\begin{itemize}
  \item hive
  \item programmi java
  \item Accumulo, key/value
  \item Hbase, colonnale
  \item Storm
  \item Spark, un processing engine in memory
\end{itemize}
Un esempio di architettura è \textbf{Cloudera} dove si hanno, oltre a HDFS,
Hbase, YARN:
\begin{itemize}
  \item Impala, un engine SQL
  \item Mahout, per il machine learning
  \item Sqopp, per il passaggio tra SQL e HDFS e viceversa
  \item Flume, un servizio distribuito, affidabile e disponibile per
  raccogliere, aggregare e spostare in modo efficiente grandi quantità di log di
  dati 
\end{itemize}
Si ha quindi HDFS per lo storage (insieme a soluzioni specificatamente non
relazionali come Hbase) e poi si ha un ecosistema digitale con vari servizi.\\
Le piattaforme cloud offrono soluzioni simili con varie data platform (con
sistemi di storage ed elaborazione, con un data lake dove si buttano i dati che
poi vanno analizzati e processati):
\begin{itemize}
  \item AWS
  \item AZURE
  \item Google Cloud Platform (un filo meno evoluto e meno strutturato)
\end{itemize}
Bene o male sono tutti molto simili.\\
\textbf{Su slide immagini delle varie data platform e breve spiegazione dei vari
  strumenti}
\chapter{Data management for machine learning}
Vediamo un argomento che unisce architetture dati e machine learning.\\
Normalmente in machine learning si parte da dati di training già puliti, si fa
il training e si ottiene il modello che a sua volta, con una serie di dati
reali, produce dei risultati. In produzione tutto questo non è sufficiente e non
si ha tanta importanza nella fase di \textit{train} e \textit{serve} ma nei dati
in 
se. Bisogna prepare bene i dati per il ML, avendo il rischio di \textbf{garbage
  in, garbage out} in quanto i computer elaborano in modo acritico anche un
insieme di dati in entrata palesemente insensati (garbage in) producendo, a loro
volta, un risultato insensato (garbage out). La fase di \textit{prepare} dei
dati è quindi fondamentale, mediamente:
\begin{itemize}
  \item $80\%$ del tempo è \textit{data preparation}
  \item $5\%$ del tempo è \textit{identificazione del modello}
  \item $5\%$ del tempo è \textit{training}
  \item $5\%$ del tempo è \textit{deployment} e altri lavori minori 
\end{itemize}
Bisogna poi, per i risultati, fare \textit{data validation}, \textit{data
  monitoring} ed eventuali correzioni.\\
I dati di input vengono mediamente divisi in training (grandi) e serving
(piccoli) come già sappiamo.\\
Nella fase di \textit{prepare} dei dati di training bisogna valutare quali sono
le 
feature rilevanti, fare data exploration dei valori delle feature, quali sono le
migliori pratiche per estrarre dai valori delle feature i migliori valori per il
modello di machine learning etc$\ldots$ Si parla di \textbf{feature selection} e
\textbf{feature engineering}. Bisogna poi valutare il modello, capendo se è
abbastanza buono, se deve essere modificato o se servono più dati o più
feature.\\
In produzione però bisogna andare oltre, basti pensare che magari una fonte dei
dati subisce un refactoring e tutto il modello si ``rompe'', avendo \textbf{data
failure}, con performance che crollano o peggio, se si parla di reinforcment
learning, si ha il modello che viene allenato con dati errati (e riaddestrare è
costoso). Si vede quindi che il problema non è il modello in se ma i dati.\\
Bisogna quindi validare i dati in input al modello prima di darli in pasto al
modello per capire se è tutto corretto. Bisogna tenere conto anche delle
``deviazioni'' dei dati che dipendono dalle situazioni a contorno
dell'environment (ad esempio un modello per il traffico addestrato l'anno scorso
non vale ora causa pandemia). I problemi vanno quindi fixati, per poter poi
ricreare i dati di testing  serving e riprendere l'intero ciclo di vita.\\
Dal punto di vista del personale si hanno almeno tre figure:
\begin{itemize}
  \item \textbf{l'esperto di ML}, che studia il modello
  \item \textbf{il software/web engineering (\textit{SWE})}, che effettivamente
  sviluppa il modello
  \item \textbf{il site reliability engineering (\textit{SRE})}, che si occupa
  appunto di fixare i vari problemi (sia dei dati che di interazione con il
  cliente) 
\end{itemize}
\begin{figure}
  \centering
  \includegraphics[scale = 0.65]{img/pipe.pdf}
  \caption{Esempio di pipeline completa di machine learning in produzione.}
  \label{fig:pipe}
\end{figure}
In figura \ref{fig:pipe} può essere studiata la \textbf{data understanding}
nell'ambito del machine learning, avendo un \textit{sanity check} prima di
trainare il primo modello e avendo altre analisi a ciclo dopo. Tra i check
classici nel sanity check abbiamo:
\begin{itemize}
  \item il controllo di valore massimo, minimo e più frequente di ogni feature,
  studiando l'accuratezza sintattica come dimensione di qualità
  \item l'istogramma dei valori continui e categorici per vedere la
  distribuzione dei valori 
  \item se una feature è presente in un numero sufficiente di esempi per lo
  studio in analisi, studiando quindi la completezza come dimensione di qualità
  \item se una feature ha il giusto numero di valori, dando un vincolo di
  consistenza ai dati
\end{itemize}
Non sempre i dataset reali sono ``puliti''.\\
Se sapessimo a priori cosa cercare basterebbero una seria di query per la
\textit{data exploration}. La realtà però è che spesso non si sa come è fatto un
dataset e quindi si sfrutta la \textit{data visualization}, ovvero strumenti di
visualizzazione per poter guardare graficamente i vari elementi per procedere
poi al sanity check. Mediamente per la visualizzazione si usano metriche basate
sulla deviazione e, in ogni caso, si hanno informazioni da visualizzare più
interessanti/utili di altre.\\
Questo è all'inizio del processo mentre durante la
fase di messa in produzione e delle varie iterazioni si hanno altri aspetti per
la data understanding. In ordine di importanza:
\begin{itemize}
  \item \textbf{analisi feature-based}, studiando la sensitività di una feature
  rispetto ai dati e studiando le conseguenze di eventuali variazioni degli
  stessi. Si usano approcci visuali o alla datawarehouse (???)
  \item \textbf{analisi del ciclo di vita dei dati}, studiando dipendenze delle
  feature nascoste (fattore necessario in alcuni modelli, come quello bayesano
  che assume indipendenza delle feature). Può essere successo che un dato sia
  stato spezzato in due dati tra loro dipendenti 
  \item \textbf{altre questioni aperte}, tra cui la fairness del modello di
  machine learning, ovvero studiando il ``pregiudizio'' di alcuni modelli
  rispetto a certe classi (magari perché addestrati solo su alcune classi o
  avendo pochi dati per alcune classi, ad esempio predire il nuovo presidente
  degli USA e ottenere solo uomini, non avendo donne nello storico dei dati),
  avendo quindi modelli biased perché i dati sono biased (il modello è corretto
  ma i dati no). Questo orimo aspetto è molto importante nel mondo del
  lavoro. Un'altra questione è quello di cercare di ``fregare'' i modelli 
  di ML tramite dati di tipo \textit{adversarial}. Si parla in questo caso di
  \textbf{reti GAN} e \textbf{deep fake}, ovvero sistemi in cui si usa un
  secondo sistema con cui si cerca di ingannare il primo classificatore con dati
  simili ma ``sintetici''. In pratica si cerca di imparare dal passato per
  ricreare qualcosa di simile. Questa seconda questione ha più valenza nel mondo
  della ricerca
\end{itemize}
Per la fase di \textbf{data validation} si studiano eventuali cambiamenti nei
dati in input al modello (anche solo un uppercase che diventa lowercase con le
feature in uppercase che diventano \textit{rare}),
fattore che può rovinare i risultati (è una situazione tipica in produzione).\\
Bisogna capire come fare le correzioni e si deve riconoscere quando accadono
queste cose. Si hanno delle \textit{best practice}:
\begin{itemize}
  \item \textbf{alert}, facendo un \textit{continuous data cleaning} posso avere
  alert per errori tecnici ma anche per cambi di dati (magari per variazioni dei
  tempi a cui si riferisocno i dati ad esempio i dati presi in pandemia sono
  diversi da quelli di due anni fa)
  \item \textbf{playbook}, che contengono le classi di problemi e vanno
  consultati manualmente (per ora)
\end{itemize}
Si hanno errori che hanno anche impatto basso sulle prestazioni. Bisogna
considerare anche che eventuali correzioni possono avere conseguenze su altre
fasi della pipeline.\\
Passiamo alla \textbf{data preparation}. Bisogna fare \textbf{feature
  engineering} per capire come migliorare le performance,
aggiungendo/rimuovendo dati/feature/attributi (e studiandone la qualità prima
dell'aggiunta). Si hanno tecniche per passare da un dato grezzo in una
rappresentazione (magari booleana, tramite \textit{One Hot Encoding}) più adatta
ai modelli di ML (queste tecniche si fanno a Data Analytics). Tra le altre
tecniche standard abbiamo:
\begin{itemize}
  \item Normalization
  \item Bucketization
  \item Winsorizing
  \item Feature crosses
  \item usare un modello pre-addestrato o l'embedding per estrarre feature 
\end{itemize}
Anche i risultati di reti neurali possono essere usati per alimentare i dati,
tramite \textit{rapresentational learning} (???).\\
La feature engineering richiede molta conoscenza di dominio e avere buoni dati
richiede anche una cardinalità minore degli stessi per ottenere ottime
prestazioni. Conviene quindi migliorare la qualità dei dati più che aumentarli a
dismisura, nella maggioranza dei casi di classificazione. In altri campi, come i
sistemi generativi di linguaggi (naturali), quindi questo trade-off sembra
essere non così ben definito in ogni caso. Si hanno tecniche di data management
per migliorare la qualità delle label etc$\ldots$\\
Aggiungere feature in produzione è comunque complesso, anche dal punto di vista
computazionale, oltre al fatto che aggiungere dati è costoso.
\end{document}


% LocalWords:  Machine Learning DBMS DataBase System hashmap db workload system
% LocalWords:  storage Administrator DBA GUI SPARC performanti primis query SQL
% LocalWords:  join compiler DLL DDL Language step parsing Catalog metadati sql
% LocalWords:  tree parser plan select sottotabella FROM Where Stafford mgrssn
% LocalWords:  Bdate Pnumber l'ottimizzatore where Statistics Branch Bound work
% LocalWords:  transaction begin commit rollabck OLTP OnLine Processing ACID PL
% LocalWords:  rollback UNDO recovery serializzabile conflict equivalence locks
% LocalWords:  two phase locking shared deadlock timestamps lock serializzabili
% LocalWords:  unlock nothing vendor cloud DDBMS l'ottimizzatore Peer to client
% LocalWords:  object oriented OO example json warehouse LAV Local As View ODBC
% LocalWords:  routing processing vendors middleware DTP Distributed devices TM
% LocalWords:  GRID warehouses everything SMP Oracle RAC systems performances
% LocalWords:  big sottotabelle soft fail chunk ricostruibilità l'updates local
% LocalWords:  l'updates mapping rewriting replication decomposition global dur
% LocalWords:  optimization localization send receive semijoin runtime layer LM
% LocalWords:  monitoring response setup bytes gigabit throughput AssiGN eno RM
% LocalWords:  ename resp sse l'updates LocalWords l'updates fragment dell read
% LocalWords:  requests only transactions write insert update distribuited All
% LocalWords:  ottimizzatore reliability control concurrency schedules ROWA RPC
% LocalWords:  serializzabilità items primary copy stricted centralized wait of
% LocalWords:  Call external slide PDF ack abort resource managers log RMs not
% LocalWords:  prepare ready checkpoint dump img png decision recoverable all
% LocalWords:  records failure broadcast warm restart undo presumed xopen TX XA
% LocalWords:  interface IBM Microsoft technologies realtime based snapshot and
% LocalWords:  transactional merge over loss disaster mission critical load OOP
% LocalWords:  kilometri consolidation distribution balancing offline entry BTC
% LocalWords:  reporting warehousing commerce social testing compliant many GB
% LocalWords:  source directional resolution detention Tier staging event apply
% LocalWords:  publish publisher distributor subscriber intra inter paradigm js
% LocalWords:  relational mismatch Blockchain mapper Delete framework group SHA
% LocalWords:  multicore Blockchains blockchain bitcoin criptovalute genesis WT
% LocalWords:  block hash torrent double spanning Leporati ethereum monero spam
% LocalWords:  zcash foreniscs criptovaluta Satoshi Nakamoto paper proof part
% LocalWords:  spending bitcoins wallet miners Moore RIPEMD iniettiva one way
% LocalWords:  nonce miner mining farm stake fee blockchains ether ETH smart CP
% LocalWords:  contract Solidity struct function bytecode virtual machine remix
% LocalWords:  outOfGas OpenZeppelin startup overflow metamask l'ide Rapsten ms
% LocalWords:  Rinkeby autority Truffle Ganache main MetaMask webapp GoQuorum
% LocalWords:  permission permissionless third TTP permissioned audit Chaos EOS
% LocalWords:  webassembly Hyperledger authority Foundation SAP CryptoKitties
% LocalWords:  collezzionabili NoSQL for large banks Codd logisticamente closed
% LocalWords:  terabyte gigabyte world assumption NULL value pre porting array
% LocalWords:  workaround persistency table comodity microservizi Amazon Google
% LocalWords:  Facebook schemaless CAP theorem Basic Available Eventually Lynch
% LocalWords:  consistency governance Brewer webserver Gilber Availability key
% LocalWords:  Partition partition availability Eventual stores column family
% LocalWords:  document databases graph RDF tuple caching blob hashing hashed
% LocalWords:  distributed wide hashata ranged property root MongoDB identifier
% LocalWords:  filesystem referencing Bson binary embedding engine WiredTiger
% LocalWords:  MMAP language MQL secondary sync hearbeat replicaSet count tag
% LocalWords:  sharding aware chunks available utility live tracciabilità GDPR
% LocalWords:  Gitlab compatible PosgreSQL backuppati droppato Azure ER tupla
% LocalWords:  intensionale processamento colonnare denormalizzare enrichment
% LocalWords:  inaccuratezza
