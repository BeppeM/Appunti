\documentclass[a4paper,12pt, oneside]{book}

% \usepackage{fullpage}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage[safe,extra]{tipa}
% \usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{fontspec}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{cancel}
\usepackage{braket}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{color}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage[cache=false]{minted}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}
\makeatletter
\def\cceq{\mathrel{\vcenter{\hbox{:}}{=}}}
\def\Cceq{\mathrel{\vcenter{\hbox{::}}{=}}}
\makeatother
\usepackage{tikz}\usetikzlibrary{er}\tikzset{multi  attribute /.style={attribute
    ,double  distance =1.5pt}}\tikzset{derived  attribute /.style={attribute
    ,dashed}}\tikzset{total /.style={double  distance =1.5pt}}\tikzset{every
  entity /.style={draw=orange , fill=orange!20}}\tikzset{every  attribute
  /.style={draw=MediumPurple1, fill=MediumPurple1!20}}\tikzset{every
  relationship /.style={draw=Chartreuse2,
    fill=Chartreuse2!20}}\newcommand{\key}[1]{\underline{#1}}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,shapes,backgrounds,petri}
\tikzset{
  place/.style={
    circle,
    thick,
    draw=black,
    minimum size=6mm,
  },
  transition/.style={
    rectangle,
    thick,
    fill=black,
    minimum width=8mm,
    inner ysep=2pt
  },
  transitionv/.style={
    rectangle,
    thick,
    fill=black,
    minimum height=8mm,
    inner xsep=2pt
  }
} 
\usetikzlibrary{automata,positioning}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\definecolor{darkgreen}{rgb}{0.18, 0.43, 0.08}
\definecolor{watergreen}{rgb}{0.16, 0.66, 0.60}

\lstdefinelanguage{conc}{
  keywords={C},
  keywordstyle=\color{blue}\bfseries,
  keywords=[2]{skip},
  keywordstyle=[2]\color{watergreen}\bfseries,
  keywords=[3]{if, else, endif, for, while, then, endwhile, endfor, do},
  keywordstyle=[3]\color{darkgreen}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  classoffset=4, % starting new class
  otherkeywords={>,<,.,;,-,!,=, +, /, *},
  morekeywords={>,<,.,;,-,!,=, +, /, *},
  keywordstyle=\color{darkgray},
  classoffset=0,
}
\lstset{
  language=conc,
  extendedchars=true,
  basicstyle=\footnotesize\ttfamily,
  showstringspaces=false,
  showspaces=false,
  tabsize=2,
  breaklines=true,
  literate={./}{{{\color{red}./}}}2 {.^}{{{\color{red}.\^{}}}}2
  {:}{{{\color{red} $\ \Cceq\ $}}}1
  {./}{{{\color{purple}./}}}2 {.^}{{{\color{purple}.\^{}}}}2
  {~}{{{\color{purple} $\ \cceq\ $}}}1
  {./}{{{\color{green}./}}}2 {.^}{{{\color{green}.\^{}}}}2
  {|}{{{\color{green} |}}}1
  {./}{{{\color{orange}./}}}2 {.^}{{{\color{orange}.\^{}}}}2
  {;}{{{\color{orange} ;}}}1, 
  showtabs=false
}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}


\title{Modelli della Concorrenza}
\author{UniShare\\\\Davide Cozzi\\\href{https://t.me/dlcgold}{@dlcgold}}
\date{}

\pgfplotsset{compat=1.13}
\begin{document}
\maketitle

\definecolor{shadecolor}{gray}{0.80}
\setlist{leftmargin = 2cm}
\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}
\tableofcontents
\renewcommand{\chaptermark}[1]{%
  \markboth{\chaptername
    \ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%
\newcommand{\simplies}{{\implies}}
\newcommand{\siff}{{\iff}}
\newcommand{\notimplies}{\;\not\!\!\!\simplies}
\newcommand{\notiff}{\;\not\!\!\!\iff}
\chapter{Introduzione}
\textbf{Questi appunti sono presi a lezione. Per quanto sia stata fatta
  una revisione è altamente probabile (praticamente certo) che possano
  contenere errori, sia di stampa che di vero e proprio contenuto. Per
  eventuali proposte di correzione effettuare una pull request. Link: }
\url{https://github.com/dlcgold/Appunti}.\\
\textbf{Le immagini presenti in questi appunti sono tratte dalle slides del
  corso e tutti i diritti delle stesse sono da destinarsi ai docenti del corso
  stesso}.
\chapter{Introduzione alla Concorrenza}
La \textbf{concorrenza} è presente in diversi aspetti della quotidianità.
Un primo esempio di \textbf{sistema concorrente} non legato all'informatica è 
la \textit{cellula vivente}: può essere vista come un dispositivo che
trasforma e manipola dati per ottenere un risultato. I vari processi all'interno
di una cellula avvengono in modo concorrente, il che la rende un \textit{sistema
  asincrono}. Un secondo esempio può essere
quello dell'\textit{orchestra musicale}: i vari componenti suonano spesso
simultaneamente rappresentando un \textit{sistema sincrono} (ovvero un sistema
che funziona avendo una sorta di ``cronometro'' condiviso dai vari attori). 
Un esempio informatico invece è un \textit{processore multicore} (anche se in
realtà anche se fosse \textit{monocore} sarebbe comunque un sistema concorrente
per ovvie ragioni). Anche una \textit{rete di calcolatori} è un modello
concorrente, nonché i \textit{modelli sociali umani}.
\subsubsection{Caratteristiche comuni}
I modelli concorrenti hanno alcuni aspetti comuni, tra cui:
\begin{itemize}
  \item competizione per l’accesso alle risorse condivise
  \item cooperazione per un fine comune (che può portare a competizione)
  \item coordinamento di attività diverse
  \item sincronia e asincronia
\end{itemize}
\subsubsection{Studio}
Durante lo studio e la progettazione di sistemi concorrenti si hanno diversi problemi
peculiari che rendono il tutto molto complesso. Un sistema
concorrente mal progettato può avere effetti catastrofici.  

Per poter sviluppare modelli concorrenti si necessita innanzitutto di:
\begin{itemize}
  \item \textbf{linguaggi}, per specificare e rappresentare sistemi concorrenti. 
  \begin{itemize}
    \item \textbf{linguaggi di programmazione} (con l'uso di \textit{thread},
    \textit{mutex}, scambio di messaggi, etc$\ldots$ con i vari
    problemi di \textit{race condition}, uso di variabili condivise
    etc$\ldots$).
    
    \item linguaggi rappresentativi, come ad esempio  
    una \textit{partitura musicale} (nella quale si visualizza bene la natura
    \textit{sincrona}).

    \item \textbf{task graph (\textit{grafo delle
        attività})}, nel quale i nodi sono le attività (o eventi) mentre gli archi
    rappresentano una \textit{relazione d'ordine parziale}, come per esempio una
    \textit{relazione di precedenza} sui nodi.

    \item \textbf{algebre di processi}, simile ad un sistema di equazioni, con simboli
    che rappresentano eventi del sistema concorrente e operatori atti a comporre
    fra loro i vari sottoprocessi del sistema concorrente. Ogni ``equazione''
    descrive un processo che costituisce un elemento di un sistema concorrente.
    
    \item \textbf{modelli}, per modellare sistemi concorrenti in astratto. Un
    esempio è dato dalle \textbf{reti di Petri}, che modellano un sistema
    concorrente partendo dalle nozioni di \textit{stato locale} di uno dei
    componenti del sistema e di \textit{evento locale} che ha un effetto su alcune
    componenti (e non tutte). Si ha quindi rappresentato un \textit{sistema
      dinamico} che si evolve nel tempo e la cui evoluzione è rappresentata tramite
    \textit{relazioni di flusso}).

  \end{itemize}
  
  \item \textbf{logica}, per analizzare e specificare sistemi concorrenti.
  \item \textbf{model-checking}, per validare formule relative a proprietà di
  sistemi concorrenti.
\end{itemize}

\chapter{Logica}

\emph{Si ringrazia
  \MYhref{https://github.com/bigboss98/Appunti-1/tree/master/Primo Anno/Fondamenti}{Marco
    Natali}
  \footnote{Link al repository:
    https://github.com/bigboss98/Appunti-1/tree/master/Primo Anno/Fondamenti} per questo
  ripasso}.\\\\   
La logica è lo studio del ragionamento e dell’argomentazione e, in particolare,
dei procedimenti inferenziali, rivolti a chiarire quali	procedimenti di pensiero
siano validi e quali no. Vi sono molteplici tipologie di logiche, come ad
esempio la logica classica e le logiche costruttive, tutte accomunate dall'essere
composte da 3 elementi: 
% Elementi di una Logica
\begin{itemize}
  \item \textbf{Linguaggio}: insieme di simboli utilizzati nella Logica per
  definire le cose.
  \item \textbf{Sintassi}: insieme di regole che determina quali elementi
  appartengono o meno al linguaggio.
  \item \textbf{Semantica}: permette di dare un significato alle formule del
  linguaggio e determinare se rappresentano o meno la verità.
\end{itemize}

\section{Logica proposizionale}
Ci occupiamo della \textit{logica classica} che si compone in \textit{logica proposizionale} 
e \textit{logica predicativa}.
La logica proposizionale è quindi un tipo di logica classica che presenta come
caratteristica principale quella di essere un linguaggio limitato, ovvero caratterizzato dal poter
esprimere soltanto proposizioni senza possibilità di estensione ad una
classe di persone.
\newpage
\subsection{Sintassi}
Il linguaggio di una logica proposizionale è composto dai seguenti elementi:

% Elementi linguaggio logica proposizionale
\begin{itemize}
  \item Variabili Proposizionali atomiche (o elementari): $P,Q,R,p_i, \dots$. 
  \item Connettivi Proposizionali: $\land, \lor, \neg, \implies, \siff$
  \item Simboli Ausiliari: ``(`` e ``)'' (detti delimitatori)
  \item Costanti: $T$ (\textit{True, Vero, $\top$}) e $F$ (\textit{False, Falso,
    $\bot$})
\end{itemize}

La sintassi di un linguaggio è composta da una serie di formule ben
formate ($FBF$) definite induttivamente nel seguente modo:
% definizione formule ben formate
\begin{enumerate}
  \item Le costanti e le variabili proposizionali: $\top,\bot,p_i\in FBF$.
  \item Se $A$ e $B \in FBF$ allora $(A \land B)$,$(A \lor B)$,$(\neg A)$,$(A
  \simplies B)$, $(A \siff B)$ sono delle formule ben formate.
  \item nient'altro è una formula
\end{enumerate}

\textbf{In una formula ben formata le parentesi sono bilanciate.}

\begin{esempio}
  Vediamo degli esempi:
  \begin{itemize}
    \item $(P \land Q) \in FBF$  è una formula ben formata\newline
    \item $(PQ \land R) \not \in FBF$ in quanto non si rispetta la sintassi del
    linguaggio 
    definita. 
  \end{itemize}
\end{esempio}

% Definizione delle sottoformule
\begin{definizione}

  Sia $A \in FBF$, l'insieme delle sottoformule di $A$ è definito come segue:
  \begin{enumerate}
    \item Se $A$ è una costante o variabile proposizionale allora A stessa è la
    sua 
    sottoformula.
    \item Se $A$ è una formula del tipo $(\neg A')$ allora le sottoformule di A
    sono 
    A stessa e le sottoformule di $A'$; 
    $\neg$ è detto connettivo principale e $A'$ sottoformula immediata di A.
    \item Se $A$ è una formula del tipo $B \circ C$, allora le sottoformule di A
    sono A stessa 
    e le sottoformule di B e C; $\circ$ è il connettivo principale e B e C sono
    le due sottoformule immediate di A. 
  \end{enumerate}

\end{definizione}
È possibile ridurre ed eliminare delle parentesi attraverso l'introduzione della
precedenza tra gli operatori, definita come segue: 
$$
\neg, \land, \lor, \simplies,\siff
$$

In assenza di parentesi una formula va parentizzata privilegiando le
sottoformule 
i cui connettivi principali hanno la precedenza più alta.\newline
In caso di parità di precedenza vi è la convenzione di associare da destra a
sinistra. Segue un esempio:
$$
\neg A \land (\neg B \simplies C) \lor D 
\hbox{ diventa }
((\neg A) \land ((\neg B) \simplies C) \lor D)
$$
\subsubsection{Albero Sintattico}
% Definizione di albero sintattico
\begin{definizione}
  Un albero sintattico $T$ è un albero binario coi nodi etichettati da simboli
  di $L$, che rappresenta la scomposizione di una formula ben formata $X$
  definita 
  come segue: 
\end{definizione}
\begin{enumerate}
  \item Se $X$ è una formula atomica, l'albero binario che la rappresenta è
  composto 
  soltanto dal nodo etichettato con $X$
  \item Se $X = A \circ B$, $X$ è rappresentata da un albero binario che ha la
  radice 
  etichettata con $\circ$, i cui figli sinistri e destri sono la
  rappresentazione di $A$ e $B$ 
  \item Se $X = \neg A$, $X$ è rappresentato dall'albero binario con radice
  etichettata 
  con $\neg$, il cui figlio è la rappresentazione di $A$
\end{enumerate}

Poiché una formula è definita mediante un albero sintattico, le proprietà di una
formula 
possono essere dimostrate mediante induzione strutturale sulla formula, ossia
dimostrare 
che la proprietà di una formula soddisfi i seguenti 3 casi:
\begin{itemize}
  \item è verificata la proprietà per tutte le formule atomo $A$
  \item supposta verifica la proprietà per $A$, si verifica che la proprietà è
  verificata per $\neg A$ 
  \item supposta la proprietà verificata per $A_1$ e $A_2$, si verifica che la
  proprietà è verifica per $A_1 \circ A_2$, per ogni connettivo $\circ$.
\end{itemize}
\newpage
\subsection{Semantica}
La semantica di una logica consente di dare un significato e un'interpretazione
alle formule del Linguaggio.\newline
\begin{definizione}
  Sia data una formula proposizionale $P$ e sia ${P_1,\dots,P_n}$, l'insieme
  degli 
  atomi che compaiono nella formula $A$. Si definisce come
  \emph{interpretazione} una 
  funzione $v:\{P_1,\dots,P_n\} \mapsto \{T,F\}$ che attribuisce un valore di
  verità 
  a ciascun atomo della formula $A$.
  \\
  $v:P\to\{0,1\}$ è un'\textbf{assegnazione booleana} 
\end{definizione}

I connettivi della Logica Proposizionale hanno i seguenti valori di verità:
% Tabella di Verità degli operatori
\[
  \begin{array}{ccccccc}
    \toprule
    \text{A} & \text{B} & A \land B & A \lor B & \neg A & A \simplies B & A
                                                                          \siff
                                                                          B \\
    \midrule
    F & F & F & F & T & T & T \\
    F & T & F & T & T & T & F \\
    T & F & F & T & F & F & F \\
    T & T & T & T & F & T & T \\
    \bottomrule
  \end{array}
\]
Essendo ogni formula $A$ definita mediante un unico albero sintattico,
l'interpretazione $v$ 
è ben definita e ciò comporta che data una formula $A$ e un'interpretazione
$v$, 
eseguendo la definizione induttiva dei valori di verità, si ottiene un unico
$v(A)$. 

% Tipologie di formule
\begin{definizione}
  Una formula nella logica proposizionale può essere di diversi tipi:
  \begin{itemize}
    \item \textbf{Valida o Tautologica:} la formula è soddisfatta da qualsiasi
    valutazione della Formula 
    \item \textbf{Soddisfacibile NON Tautologica:} la formula è soddisfatta da
    qualche valutazione 
    della formula ma non da tutte.
    \item \textbf{Falsificabile:} la formula non è soddisfatta da qualche
    valutazione della formula. 
    \item \textbf{Contraddizione:} la formula non viene mai soddisfatta
  \end{itemize}
\end{definizione}

\begin{teorema}
  Si ha che:
  \begin{itemize}
    \item $A$ è una formula valida se e solo se $\neg A$ è insoddisfacibile.
    \item $A$ è soddisfacibile se e solo se $\neg A$ è falsificabile
  \end{itemize}
\end{teorema}


\subsubsection{Modelli e decidibilità}
Si definisce \emph{modello}, indicato con $M \models A$, tutte le valutazioni
booleane 
che rendono vera la formula $A$.
Si definisce \emph{contromodello}, indicato con $\not\models$, tutte le
valutazioni booleane 
che rendono falsa la formula $A$.

La logica proposizionale è decidibile (posso sempre verificare il significato di
una formula). 
Esiste infatti una procedura effettiva che stabilisce la validità o no di una
formula, o se questa 
ad esempio è una tautologia.
In particolare il verificare se una proposizione è tautologica o meno è
l’operazione di decidibilità principale che si svolge nel calcolo
proposizionale. 

\begin{definizione}
  Se $M \models A$ per tutti gli $M$, allora $A$ è una tautologia e si indica
  $\models A$.
\end{definizione}

\begin{definizione}
  Se $M \models A$ per qualche $M$, allora $A$ è soddisfacibile.
\end{definizione}

\begin{definizione}
  Se $M \models A$ non è soddisfatta da nessun $M$, allora $A$ è
  insoddisfacibile.
\end{definizione}
\subsection{Equivalenze Logiche}
\begin{definizione}
  Date due formule $A$ e $B$, si dice che $A$ è \emph{logicamente equivalente} a
  $B$, 
  indicato con $A \equiv B$, se e solo se per ogni interpretazione $v$ risulta
  $v(A) = v(B)$. 
\end{definizione}

Nella logica proposizionale sono definite le seguenti equivalenze logiche,
indicate con $\equiv$: 
\begin{enumerate}
  \item \textbf{Idempotenza:}
  \begin{align*}
    A \lor A  \equiv  A \\
    A \land A  \equiv  A \\
  \end{align*}
  \item \textbf{Associatività:}
  \begin{align*}
    A \lor (B \lor C) \equiv  (A \lor B) \lor C \\
    A \land (B \land C)  \equiv  (A \land B) \land C
  \end{align*}
  \item \textbf{Commutatività:}
  \begin{align*}
    A \lor B  \equiv  B \lor A \\
    A \land B  \equiv  B \land A
  \end{align*}
  \item \textbf{Distributività:}
  \begin{align*}
    A \lor (B \land C)  \equiv & (A \lor B) \land (A \lor C)\\
    A \land (B \lor C)  \equiv & (A \land B \lor (A \land C)
  \end{align*}
  \item \textbf{Assorbimento:}
  \begin{align*}
    A \lor (A \land B)  \equiv  A
    A \land (A \lor B)  \equiv  A
  \end{align*}
  \item \textbf{Doppia negazione:}
  \begin{equation*}
    \neg \neg A \equiv A
  \end{equation*}
  \item\textbf{Leggi di De Morgan:}
  \begin{align*}
    \neg (A \lor B)  \equiv  \neg A \land \neg B \\
    \neg(A \land B)  \equiv  \neg A \lor \neg B
  \end{align*}
  \item \textbf{Terzo escluso:}
  \begin{equation*}
    A \lor \neg A \equiv T
  \end{equation*}
  \item \textbf{Contrapposizione:}
  \begin{equation*}
    A \simplies B \equiv \neg B \simplies \neg A
  \end{equation*}
  \item \textbf{Contraddizione}
  \begin{equation*}
    A \land \neg A \equiv F
  \end{equation*}
\end{enumerate}
\subsubsection{Completezza di insiemi di Connettivi}
Un insieme di connettivi logici è completo se mediante i suoi connettivi si può
esprimere un qualunque altro connettivo.
Nella logica proposizionale valgono anche le seguenti equivalenze, utili per
ridurre il linguaggio: 

\[(A \simplies B)  \equiv  (\neg A \lor B) \]
\[(A \lor B)  \equiv  \neg(\neg A \land \neg B) \]
\[(A \land B)  \equiv  \neg(\neg A \lor \neg B) \]
\[(A \siff B) \equiv  (A \simplies B) \land (B \simplies A) \]

L'insieme dei connettivi $\{ \neg,\lor,\land \}$, $\{ \neg,\land \}$ e $\{
\neg,\lor \}$ sono completi.

\chapter{Correttezza di programmi sequenziali}
Introduciamo l'argomento con un esempio.
\begin{esempio}
  Definiamo una funzione in C che riceve un vettore, un intero (la lunghezza del vettore) e
  e restituisce un ulteriore numero intero.
  \begin{listing}[ht]
    \begin{minted}{c}
      int f(int n, const int v[]) {
        int x = v[0];
        int h = 1;
        while (h < n) {
          if (x < v[h])
          x = v[h];        
          h = h + 1;
        }
        return x;
      }
    \end{minted}
    \caption{Esempio di funzione in C}
    \label{listing:1}
  \end{listing}
  Chiedendoci cosa fa la funzione scopriamo che si occupa di cercare il massimo in
  un vettore.\\
  La strategia della funzione è quella di spostarsi lungo il vettore e
  conservare in $x$ il valore massimo fino ad ora trovato. Arrivati alla fine
  del vettore so che in $x$ avrò il valore massimo.\\
  Più formalmente suppongo che $n$ sia $n > 0$, per dire che ho almeno un
  elemento nel vettore. Suppongo inoltre che $v[i]\in\mathbb{Z},\,\,\forall i\in
  \{0,\ldots, n-1\}$. Abbiamo fissato le \textbf{condizioni iniziali}.\\
  All'inizio $x$ è il massimo del sotto-vettore con solo il primo elemento
  ($v[0..0]$) e dopo l'assegnamento di $h$ in $v[o..h-1]$, che, con $h=1$ mi
  conferma che x è il massimo in $v[0..0]$. \\
  Al termine di una certa iterazione $x$ contiene il massimo 
  tra i valori compresi tra $v[0]$ e $v[h-1]$ (detto altrimenti il massimo in
  $v[0..h-1]$). Inoltre al fine di una certa iterazione mi aspetto che $h\leq
  n$.\\
  Possiamo quindi dire che quando esco dal ciclo $x$ è il massimo in $v[0..h-1]$
  ma in questo momento $h=n$ e quindi $x$ è il massimo del vettore.\\
  Consideriamo ora la parte iterativa. All'inizio di ogni iterazione suppongo
  che $x$ è il massimo in $v[0..h-1]$. Dopo l'istruzione di scelta $x$ è il
  massimo in $v[0..h]$, comunque sia andata la scelta. Alla fine
  dell'iterazione, dopo l'incremento di $h$, avrò ancora che $x$ è il massimo in
  $v[0..h-1]$. Ragiono quindi per induzione. Se all'inizio dell'iterazione e
  alla fine ho la stessa asserzione, ed è vera prima di iniziare l'iterazione,
  posso dire che ho una \textbf{proprietà invariante} e vale anche al termine
  dell'ultima iterazione e quindi vale anche alla fine dell'esecuzione del
  programma. 
\end{esempio}
Nell'esempio notiamo in primis l'assenza di formalità. Si introducono quindi
concetti:
\begin{enumerate}
  \item \textbf{precondizione} che nell'esempio è fatta da $n>0$ e
  $v[i]\in\mathbb{Z},\,\,\forall i\in \{0,\ldots, n-1\}$
  \item \textbf{postcondizione} che nell'esempio si ritrova con l'asserzione $x$
  è il massimo in $v[0..h-1]$ e $h=n$, che scritto in modo formale diventa:
  \[
    \begin{rcases}
      v[i]\leq x,\,\,\forall i\in \{0,\ldots, n-1\}\\
      \exists\,i\in\{0,\ldots, n-1\}\mbox{ t.c. } v[i]=x
    \end{rcases}
    x=max(v[0..n-1])
  \]
\end{enumerate}
\textit{Queste formule possono essere rese come formule proposizionali, tramite
  una congiunzione logica:}
\[
  \begin{cases}
    v[0]\leq x \land v[1]\leq x\land\ldots v[n-1]\leq x\\
    v[0]= x \lor v[1]= x\lor\ldots v[n-1]=x
  \end{cases}
\]
Abbiamo studiato lo stato della memoria del programma in un certo istante
tramite formule.
\begin{definizione}
  Definiamo \textbf{stato della memoria} come:
  \[s:V\to\mathbb{Z}\]
  ovvero una funzione che mappa le variabili del programma (poste nell'insieme
  $V$) in $\mathbb{Z}$.
\end{definizione}
Fissato uno stato della memoria e una formula posso validare una formula in
quello stato osservando le variabili e le relazioni aritmetiche della
formula.\\
Data una formula $\phi$ e uno stato $s$ posso sapere se $\phi$ è valida in
$s$.\\
Una formula che gode della \textbf{proprietà invariante} se vera all'inizio
dell'iterazione è vera anche alla fine della stessa. Per capire se è invariante
basta vedere lo stato di una formula ad inizio e fine di una iterazione.\\
L'esecuzione di una istruzione cambia lo stato della memoria. Potrebbe però
accadere che una serie di istruzioni non facciano terminare il programma, perciò
quanto detto sopra è in realtà un'approssimazione della realtà.
\begin{definizione}
  Definiamo la \textbf{specifica di correttezza di un programma} con la tripla:
  \[\alpha\,\, P\,\, \beta\]
  dove:
  \begin{itemize}
    \item $\alpha$ e $\beta$ sono formule (definite con tutte le simbologie
    aritmetiche tra variabili, sia di conto che di relazione).
    \item $P$ è un ``programma'' (anche un frammento o una singola istruzione) che
    modifica lo stato della memoria.
  \end{itemize}
  $\alpha$ è la \textbf{precondizione}, che supponiamo verificata nello stato
  iniziale e $\beta$ è la \textbf{postcondizione}, 
  che supponiamo valida dopo l'esecuzione del programma.
\end{definizione}
Durante il corso useremo un linguaggio imperativo non reale semplificato.
Dovremo anche definire una logica, definendo un apparato deduttivo, un insieme
di regole per costruire dimostrazioni derivando nuove formule da quelle
preesistenti. Useremo la \textbf{logica di Hoare}. Le formule della logica di
Hoare sono triple di tipo $(\alpha P \beta)$ quindi si tratta di una logica di tipo diverso
anche se si appoggia su quella proposizionale.
\section{Linguaggio semplificato}
Definiamo quindi il linguaggio di programmazione imperativo semplificato che
andremo ad utilizzare. Si userà una grammatica formale.\\
L'elemento fondamentale di questo linguaggio è il \textbf{comando}, che indica o
una singola istruzione o un gruppo di istruzioni strutturate. Il simbolo usato
nella grammatica per indicare un comando è ``C''. Un comando viene costruito
tramite le \textbf{produzioni}, introdotte da ``::=''. Il comando più semplice è
l'\textbf{assegnamento}, che usa l'operatore ``:='' per assegnare un valore ad
una variabile. Con il simbolo ``E'' indichiamo un simbolo non terminale della
grammatica che sta per \textit{espressione}. Una volta costruito semplici
espressioni possiamo combinarle, eseguendole in sequenza, inserendo un ``;'' tra
due comandi.\\ 
In merito all'istruzione di scelta abbiamo l'istruzione ``if'', seguito da
un'espressione booleana, seguito da ``then'', seguita da un comando, seguita da
``else'', seguita da un comando, e il tutto viene concluso da ``endif''. Qui
abbiamo una prima semplificazioni dicendo che l'\textit{else} è
obbligatorio. Per l'iterazione abbiamo il ``while'', seguito da un'espressione
booleana, seguito da ``do'' con poi il comando, il tutto concluso da
endwhile. Infine abbiamo una istruzione speciale, chiamata ``skip'', che non fa
nulla e avanza il \textit{program counter} (con essa posso saltare il ramo
alternativo dell'\textit{if-else}).\\
Un'espressione booleana ``B'' può essere la costante ``true'', la costante
``false'' o del tipo ``not B'', ``B and B'', ``B or B'', ``E < E'' (e le altre),
``E = E''. Non si hanno tipi di dato ma supporremo di avere a che fare solo con
\textit{interi}. Non si ha la funzione di nozione o di classe. Questo linguaggio
è comunque \textit{Turing Complete}.
\begin{listing}[H]
  \begin{lstlisting}
    x ~ a; y ~ b;
    while x != y do
    if x < y then
    y ~ y - x;
    else
    x ~ x - y;
    endif
    endwhile  
  \end{lstlisting}
  \caption{Esempio di programma $D$}
  \label{listing:D}
\end{listing}
Cerchiamo di capire se il programma $D$, sopra definito, soddisfa la tripla:
\[\{a>0\land b>0\}\,\,D\,\, \{x=MCD(a,b)\}\]
Quindi mi chiedo se eseguendo il programma con uno stato della memoria dove $a$
e $b$ sono due interi positivi (precondizione) allora, alla fine dell'esecuzione
di $D$, $x$ sarà il massimo comune divisore tra $a$ e $b$
(postcondizione). Dobbiamo dimostrare la tripla e qualora non fosse vera bisogna
confutarla trovando un caso in cui non è verificata (trovando uno stato che
soddisfi la precondizione ma che, una volta eseguito il programma, la postcondizione non 
sia verificata).
\section{Logica di Hoare}
\begin{definizione}
  Una \textbf{dimostrazione}, in una logica data, è una sequenza di formule di
  quella logica che sono o \textit{assiomi} (formule che riteniamo vere a
  priori) o formule derivate dalle precedenti tramite una \textit{regola di
    inferenza}. Una regola di inferenza mi manda da un insieme di formule
  $\alpha_1,\alpha_2,\ldots,\alpha_n$ ad una nuova formula $\alpha$ e si indica
  con: 
  \[\frac{\alpha_1,\alpha_2,\ldots,\alpha_n}{\alpha}\]
  Che si legge come: "\textit{se ho già derivato $\alpha_1, \alpha_2,
    \ldots,\alpha_n$ sono autorizzato a derivare $\alpha$}".\\ 
  Nel nostro caso ogni $\alpha_i$ è una tripla della \textbf{logica di Hoare}.\\
  Una dimostrazione si ottiene quindi applicando le regole di derivazione fino
  ad arrivare, se si riesce, ad una soluzione.
\end{definizione}
\subsection{Regole di derivazione}
Vediamo quindi le regole di derivazione, che sono associate alle regole del
linguaggio sopra definito.
\subsubsection{Skip}
\begin{definizione}
  Partiamo con la regola per l'istruzione \textbf{\textit{skip}}, che non
  facendo nulla 
  non cambia lo stato della memoria e quindi la regola di derivazione non ha
  nessuna premessa:
  \[\frac{}{\{p\}\,\,skip\,\,\{p\}}\]
  con $p$ che è una formula proposizionale. Dopo lo \textit{skip} $p$ vale se
  valeva prima dello \textit{skip}
\end{definizione}
\subsubsection{Implicazione}
\begin{definizione}
  La seconda è una regola che non ha un rapporto diretto con il linguaggio e
  chiameremo \textbf{regola di conseguenza (o dell'implicazione)}. Ha due
  premesse: una tripla appartenente alla logica di Hoare e una implicazione della
  logica proposizionale.
  \[\frac{p\implies p'\,\, \,\,\{p'\}\,\,C\,\,\{q\}}{\{p\}\,\,C\,\,\{q\}}\]
  Ovvero se eseguo $C$ partendo da uno stato in cui vale $p'$ allora dopo varrà
  $q$. Ma sappiamo anche che $p$ implica $p'$. Quindi se nel mio stato della
  memoria vale $p$ e quindi anche $p'$ per l'implicazione. Posso quindi dire che
  se ho $p$ ed eseguo $C$ ottengo $q$.\\
  Ho anche una forma speculare:
  \[\frac{\{p\}\,\,C\,\,\{q'\}\,\, \,\,q'\implies q}{\{p\}\,\,C\,\,\{q\}}\]
\end{definizione}
\subsubsection{Sequenza}
\begin{definizione}
  La terza regola è legata alla struttura di \textbf{sequenza} dei programmi:
  \[\frac{\{p\}\,\,C_1\,\,\{q\}\,\,
      \,\,\{q\}\,\,C_2\,\,\{r\}}{\{p\}\,\,C_1;C_2\,\,\{r\}}\] 
\end{definizione}
\subsubsection{Assegnamento}
\begin{definizione}
  La quarta regola riguarda l'\textbf{assegnamento}. Questa è l'unica regola
  non banale (come lo è lo \textit{skip}) che non ha premesse. È la regola base
  per derivare le triple necessarie alle altre regole. Quindi, avendo $E$ come
  espressione, $x$ una variabile e $p$ come una postcondizione, ovvero una
  formula che contiene gli identificatori di diverse variabili:
  \[\frac{}{\{p[E/x]\}\,\,x\cceq E\,\,\{p\}}\]
  Dove con $p[E/x]$, come precondizione, indichiamo una \textbf{sostituzione} 
  indicante che cerchiamo in $p$ tutte le occorrenze $x$ e le sostituiamo con $E$.
  \begin{esempio}
    Se ho $x \cceq y+1$ con $E$ pari a $y+1$ e con $p$ pari a $\{x>0\}$ come
    postcondizione data e cerco la precondizione. Quindi la tripla completa
    sarebbe: 
    \[\{y+1>0\}\,\,x \cceq y+1\,\,\{x>0\}\]
    E la tripla sappiamo che è \emph{vera} (se so che $y+1$ è positivo, dopo
    che a $x$ assegno $y+1$ posso essere sicuro che anche $x$ è positivo).
  \end{esempio}
  \begin{esempio}
    Se ho $x \cceq x+1$ con $E$ pari a $x+2$ e con $p$ pari a $\{x>0\land x\leq
    y\}$ come postcondizione data e cerco la precondizione. Quindi la tripla
    completa sarebbe:
    \[\{x+2>0\land x+2\leq y\}\,\,x \cceq x+1\,\,\{x>0\land x\leq y\}\]
    e anche questa tripla è garantita dalla regola di sostituzione
  \end{esempio}
\end{definizione}
\subsubsection{Istruzione di scelta}
\begin{definizione}
  La quinta regola è quella relativa all'\textbf{istruzione di scelta} che è
  della forma:
  \[\{p\}\mbox{ if \textit{B} then \textit{C} else \textit{D} endif }\{q\}\]
  Se la condizione $B$ è vera eseguo $C$ altrimenti $D$. In entrambi i casi
  alla fine deve valere la postcondizione $q$. Separiamo i due casi:
  \begin{itemize}
    \item se suppongo vere $p$ e $B$ eseguo $C$ arrivando in $q$:
    \[\{p\land B\}\,\,\,C\,\,\,\{q\}\]
    \item se suppongo vera $p$ ma falsa $B$ avrò:
    \[\{p\land \neg B\}\,\,\,D\,\,\,\{q\}\]
  \end{itemize}
  Ricavo quindi la formula generale:
  \[\frac{\{p\land B\}\,\,\,C\,\,\,\{q\}\,\,\,\,\,\,\,\,\,
      \{p\land \neg B\}\,\,\,D\,\,\,\{q\}}{\{p\}\mbox{ if \textit{B}
        then \textit{C} else \textit{D} endif }\{q\}}\]
\end{definizione}
\begin{shaded}
  Come notazione usiamo che:
  \[\vdash \{p\}\,\,\,C\,\,\,\{q\}\]
  dove $\vdash$
segnala che la tripla è stata \textbf{dimostrata/derivabile} con le regole di
derivazione (si parla quindi di \textit{sintassi}, viene infatti ignorato il
significato ma si cerca solo di applicare le regole, ottenendo al conclusione
come risultato di una catena di regole).\\
Come notazione usiamo anche che:
\[\vDash \{p\}\,\,\,C\,\,\,\{q\}\]
dove $\vDash$
indica che la tripla è \textbf{vera} (si parla quindi di \textit{semantica},
riferendosi al significato).\\
Dato che si ha \textbf{completezza} e \textbf{correttezza} dell'apparato
deduttivo si ha hanno due situazioni.
\begin{itemize}
  \item \textit{ogni tripla \textbf{derivabile} è anche \textbf{vera} in
    qualsiasi interpretazione}
  \item \textit{ogni tripla \textbf{vera} vorremmo fosse anche
    \textbf{derivabile} e il discorso verrà approfondito in seguito per la
    logica di Hoare}
\end{itemize}
$\vdash$ può avere a pedice una sigla per la regola rappresentata, ad esempio
$\vdash_{ass}$ per l'assegnamento.
\end{shaded}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{y\geq 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  uso la \emph{regola di assegnamento} (che non ha premesse) partendo dalla
  postcondizione. Sostituisco e ottengo:
  \[\vdash_{ass}\{2\cdot y+1> 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  Procedo usando la \emph{regola di implicazione} (sapendo che $y\geq 0\implies
  2y+1>0$):
  \[\vdash_{impl}\{y\geq 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  Dimostrando quindi che la tripla è \textbf{vera}.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{z> 0\}\,\,\,x\cceq (y\cdot z)+1\,\,\,\{x>0\}\]
  e vediamo che la tripla non è valida in quanto $y$ potrebbe essere negativo e
  non portare alla positività di $x$. Formalmente cerchiamo un controesempio
  cercando di non soddisfare la postcondizione. Scelgo in memoria $z=1$ e
  $y=-2$. Abbiamo fatto quindi un ragionamento semantico. Provo a anche
  sintatticamente e giungo a:
  \[\vdash\{(y\cdot z+1)> 0\}\,\,\,x\cceq (y\cdot z)+1\,\,\,\{x>0\}\]
  che non è vero, quindi non posso proseguire.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{s=x^i\}\,\,\,i\cceq i+1,\, s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  anche se uguali precondizione e postcondizione fanno riferimento a due momenti
  della memoria diversi e quindi i valori saranno diversi.\\
  Abbiamo a che fare con un \textbf{invariante} in quanto la formula non varia
  tra precondizione e postcondizione anche se i valori saranno diversi (in mezzo
  al processo posso violare comunque l'invarianza).\\
  Ragioniamo in modo puramente sintattico. Dobbiamo applicare due volte
  l'assegnamento (e spesso serve dopo anche la regola di implicazione) e una
  volta la sequenza. Anche qui partiamo dalla postcondizione risalendo via via
  alle precondizioni:
  \[\vdash_{ass}\{sx=x^i\}\,\,\,s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  Ho creato quindi la condizione intermedia tra i due assegnamenti e proseguendo
  ho:
  \[\vdash_{ass}\{sx=x^{i+1}\}\,\,\,i\cceq i+1\,\,\,\{sx=x^i\}\]
  per transitività si può scrivere:
  \[\{sx=x^{i+1}\}\,\,\,i\cceq i+1,\, s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  ma non coincide con quanto voglio dimostrare. Ragiono quindi in modo
  algebrico. In $\{sx=x^{i+1}\}\,\,\,i\cceq i+1\,\,\,\{sx=x^i\}$ ho infatti:
  \[\{s\cancel{x}=x^{i+\cancel{1}}\}\to\{sx=x^i\},\mbox{ se }x\neq 0\]
  e quindi la formula iniziale è dimostrata.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{\top\}\,\,\,\mbox{if } x<0 \mbox{ then }y\cceq -2\cdot x \mbox{ else }
    y\cceq 2*x\mbox{ endif}\,\,\,\{y\geq 0\}\]
  Diciamo che $C$ rappresenta $y\cceq -2\cdot x$ e $D$ rappresenta $y\cceq 2*x$
  per praticità. Indichiamo la condizione booleana $x<0$ con $B$. Tutta la
  condizione di scelta la chiamiamo $S$
  Quindi avremo:
  \[\{\top\}\,\,\,\mbox{if } B \mbox{ then }C\mbox{ else }
    D \mbox{ endif}\,\,\,\{y\geq 0\}\]
  che in modo ancora più compatto sarebbe:
  \[\{\top\}\,\,\,S\,\,\,\{y\geq 0\}\]
  \textbf{Applico quindi la regola di derivazione al primo caso:}
  Ho quindi:
  \[\{\top \land x<0\}\,\,\,C\,\,\,{y\geq 0}\]
  che è uguale a:
  \[\{x<0\}\,\,\,C\,\,\,{y\geq 0}\]
  Procedo ora con l'assegnamento:
  \[\vdash_{ass}\{-2\cdot x\geq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  che però equivale algebricamente a:
  \[\vdash_{ass}\{x\leq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  ma siccome $x<0 \implies x\geq 0$ uso la regola dell'implicazione:
  \[\vdash_{impl}\{x< 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  \textbf{Passo al secondo caso:}
  \[\{\top \land x\geq 0\}\,\,\,D\,\,\,{y\geq 0}\]
  che è uguale a:
  \[\{x\geq 0\}\,\,\,D\,\,\,{y\geq 0}\]
  Procedo ora con l'assegnamento:
  \[\vdash_{ass}\{2\cdot x\geq 0\}\,\,\,y\cceq 2\cdot x\,\,\,\{y\geq 0\}\]
  che però equivale algebricamente a:
  \[\vdash_{ass}\{x\geq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  e quindi è dimostrabile che:
  \[\vdash \{\top\}\,\,\,S\,\,\,\{y\geq 0\}\]
\end{esempio}
\subsubsection{Iterazione}
% Per proseguire bisogna definire meglio il concetto di \textbf{invariante}.
\begin{definizione}
  Definiamo:
  \begin{itemize}
    \item \textbf{correttezza parziale}, dove la tripla viene letta supponendo a
    priori che l'esecuzione termini
    \item \textbf{correttezza totale}, dove la tripla viene letta dovendo anche
    dimostrare che l'esecuzione termini. Si procede quindi prima dimostrando la
    correttezza parziale aggiungendo poi la dimostrazione per l'esecuzione
    finita
  \end{itemize}
\end{definizione}
\begin{definizione}
  La sesta regola è quella relativa all'iterazione. Abbiamo quindi:
  \[\{p\}\mbox{ while \textit{B} do \textit{C} endwhile }\{q\}\]
  ma non posso determinare a priori quante volte si eseguirà $C$, che modifica
  lo stato della memoria. Per comodità $\mbox{ while \textit{B} do \textit{C}
    endwhile }$ la chiameremo $W$, quindi in modo compatto abbiamo:
  \[\{p\}\,\,\,W\,\,\,\{q\}\]
  Partiamo con la correttezza parziale supponendo a priori che l'esecuzione
  termini. Si ha quindi che in $q$ sicuramente $B$ è falsa, altrimenti non si
  avrebbe terminazione. Quindi in realtà abbiamo:
  \[\{p\}\,\,\,W\,\,\,\{q\land \neg B\}\]
  Ipotizziamo che all'inizio $B$ sia vera, quindi la precondizione sarà
  $\{i\land B\}$, con $i$ rappresentante una nuova formula. In questi stati
  eseguiremo $C$. Suppongo di poter derivare, tramite $C$ eseguito una sola
  volta, nuovamente $i$:
  \[\{i\land B\}\,\,\,C\,\,\,\{i\}\]
  Se vale questa tripla significa che $i$ è un \textbf{invariante} per $C$ e
  quindi $i$ viene chiamata \textbf{invariante di ciclo}. Nello stato raggiunto
  dopo $C$ la condizione $B$ può essere valida o meno. Qualora valga dopo la
  singola esecuzione di $C$ allora avrei ancora $\{i\land B\}$ e dovrei eseguire
  nuovamente $C$ e dopo varrà ancora $i$ sicuramente e bisogna ristudiare $B$
  per capire come procedere, in quanto $i$ resterà vera per qualsiasi numero di
  iterazioni di $C$. Si ha che $i$ resterà vera, teoricamente, anche una volta
  ``usciti'' dal ciclo. Qualora non valga $B$ si ha che:
  \[\{i\land \neg B\}\,\,\,W\,\,\,\{i\land \neg B\}\]
  (dove si nota che $i$ resta vera ma $B$ impedisce di tornare nel ciclo).\\
  Si ha quindi che:
  \[\frac{\{i\land B\}\,\,\,C\,\,\,\{i\}}{\{i\}\,\,\,W\,\,\,\{i\land \neg B\}}\]
  che è la \textbf{regola dell'iterazione}. Scritta in modo completo:
  \[\frac{\{i\land B\}\,\,\,C\,\,\,\{i\}}{\{i\}\mbox{ while \textit{B} do
        \textit{C} endwhile }\{i\land \neg B\}}\]
  Una nozione più forte di \textbf{invariante} può essere espressa dicendo che
  \[\{i\}\,\,\,C\,\,\,\{i\}\]
  che si differenzia da quella di \textbf{invariante di ciclo} (dove mi
  interessa sapere che un invariante sia vero prima del ciclo anche se questa
  non è un proprietà intrinseca degli invarianti):
  \[\{i\land B\}\,\,\,C\,\,\,\{i\}\]
  Ricordiamo che stiamo dando per scontata la \textbf{terminazione} tramite la
  \textbf{correttezza parziale}.\\
  Facciamo qualche osservazione:
  \begin{itemize}
    \item nella precondizione della conclusione non si ha $B$, in quanto il
    corpo dell'iterazione può anche non essere mai eseguito
    \item data un'istruzione iterativa posso avere più di un invariante. Si ha
    inoltre che ogni formula iterativa ha l'\textbf{invariante banale}
    $i=\top$. Possiamo avere anche una formula in cui compaiono variabili che
    non sono modificate della funzione iterativa e quindi l'intera formula è un
    \textit{invariante di ciclo}. Studiamo quindi gli invarianti ``più utili''
    \item nei casi pratici non consideriamo ovviamente iterazioni isolate ma
    iterazioni inserite in un programma. In questi casi quindi la scelta di un
    invariante adeguato dipende sia dall'iterazione che dall'intero contesto.
    \begin{esempio}
      Ho un programma su cui voglio dimostrare:
      \[\{p\}\mbox{ \textit{C; W; D} }\{q\}\]
      con $W$ iterazione e $C,D$ comandi.\\
      Spezziamo quindi il programma per fare la dimostrazione, tramite la regola
      della sequenza:
      \[\{p\}\mbox{ \textit{C} }\{r\}\mbox{ \textit{W} }\{z\}\mbox{ \textit{D}
        }\{q\}\]
      $r$ sarà quindi la precondizione dell'iterazione $W$ che porterà a $z$ che
      sarà precondizione di $D$.\\
      Sapendo che la regola di derivazione per $W$ è:
      \[\{i\}\mbox{ \textit{W} }\{i\land\neg B\]
      Confronto la tripla con la ``catena'' sopra espressa. Si nota che $r$
      dovrà implicare l'invariante per $W$.
    \end{esempio}
  \end{itemize}
\end{definizione}
\begin{esempio}
  Vediamo quindi un esempio completo.\\
  Si prenda il seguente programma:
  \begin{listing}[H]
    \begin{lstlisting}
      i ~ 0; s ~ 1;
      while i < N do
        i ~ i + 1;
        s ~ s * x;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:W}
  \end{listing}
  \textit{Per comodità chiamo $A$ i due assegnamenti iniziali,
      $W$ l'iterazione e $C$ il corpo dell'iterazione.}\\
  Ci proponiamo di derivare:
  \[\{N\geq 0\}\mbox{ \textit{P} }\{s\cceq x^N\}\]
  $x$ e $N$ sono variabili, nella realtà costanti non venendo mai modificate da
  $P$, e il loro valore fa parte dello stato della memoria.\\
  Concentriamoci in primis sull'iterazione cercando un invariante. Una strategia
  semplice è quella di simulare i primi passi di una esecuzione. Supponendo $N$
  comunque non nullo abbiamo, seguendo le due variabili $i$ e $s$ nelle varie
  iterazioni (procedendo verso destra nella tabella), procedendo in modo
  \textit{simbolico} per $s$ (usando quindi il simbolo $x$ direttamente):
  \begin{table}[H]
    \centering
    \begin{tabular}[H]{c|ccccc}
      i & 0 & 1 & 2 & 3 & $\ldots$\\
      s & 1 & $x$ & $x^2$ & $x^3$ & $\ldots$
    \end{tabular}
  \end{table}
  Quindi $s= x^i$ è il nostro invariante. Dimostro questa ipotesi è vera,
  dimostrando:
  \[\{s=x^i\land i<N\}\mbox{ \textit{C} }\{s=x^i\}\]
  sapendo che essa è la premessa alla regola di iterazione.\\
  Procedo quindi con la dimostrazione, avendo l'assegnamento:
  \[\vdash_{ass}\{sx=x^{i+1}\}\mbox{ \textit{C} }\{s=x^i\}\]
  infatti i due assegnamenti sono \emph{indipendenti}, permettendo di applicare
  in una sola volta due regole di assegnamento.\\
  Sapendo che $s\cancel{x}=x^{i+\cancel{i}}\implies s=x^i$. Ho quindi mostrato
  che:
  \[\vdash \{s=x^i\}\mbox{ \textit{C} }\{s=x^i\}\]
  dimostrando che ho effettivamente l'invariante $s= x^i$ (e nel dettaglio
  abbiamo trovato un \textbf{invariante forte}, che lo è a priori rispetto alla
  condizione booleana $B$).\\
  Vogliamo però ottenere come precondizione $\{s=x^i\land i<N\}$. Sapendo però
  che $s= x^i\land i<N$ è una regola ``più forte'' di $s= x^i$ (se è vera la
  prima sicuramente è vera anche la seconda) posso usare
  l'implicazione e dire che:
  \[\{s=x^i\land i<N\}\implies \{s= x^i\}\]
  quindi:
  \[\vdash_{impl} \{s=x^i\land i<N\}\mbox{ \textit{C} }\{s=x^i\}\]
  posso quindi applicare la regola dell'iterazione avendo la premessa corretta e
  ottenendo quindi:
  \[\vdash_{iter}\{s=x^i\}\mbox{ \textit{W} }\{s=x^i\land i\geq N\}\]
  ma la postcondizione non ci sta implicando $s=x^N$, per avere:
  \[\{N\geq 0\}\mbox{ \textit{P} }\{s\cceq x^N\}\]
  bisogna quindi \textbf{rafforzare} l'invariante, in modo che l'invariante
  implichi che alla fine dell'esecuzione $i=N$.\\
  Procediamo quindi con una nuova ipotesi, cercando di dimostrare che $i\leq N$
  è un invariante:
  \[\{i\leq N\land i<N\}\mbox{ \textit{C} }\{i\leq N\}\]
  Questa precondizione contiene una formula che è più restrittiva dell'altra.
  Procedo con l'assegnamento (CAPIRE QUESTA PARTE):
  \[\vdash_{ass}\{i+1\leq N\}\mbox{ \textit{C} }\{i\leq N\}\]
  ma sapendo che $i<N\implies i+1\leq N$ ottengo:
  \[\vdash_{impl}\{i< N\}\mbox{ \textit{C} }\{i\leq N\}\]
  Dimostrando quindi che è un invariante.\\
  Passo quindi all'iterazione:
  \[\vdash_{iter}\{i\leq N\}\mbox{ \textit{W} }\{i\leq N\land i\geq N\}\]
  e la postcondizione è uguale a dire che $i=N$.\\
  Considerando quindi i due invarianti ottengo:
  \[\vdash_{iter}\{s=x^i\land i\leq N\}\mbox{ \textit{W} }
    \{s=x^i\land i= N\}\]
  Dobbiamo però dimostrare anche la parte degli assegnamenti iniziali:
  \[\{N\geq 0\}\mbox{ \textit{A} }\{s=x^i\land i\leq N\}\]
  Bisogna infatti vedere se anche le condizioni iniziali permettono
  all'invariante doppio di restare tale.\\
  Applico quindi due volte l'assegnamento (partendo dal fondo); il primo con
  $s\cceq 1$: 
  \[\vdash_{ass}\{1=x^i\land i\leq N\}\mbox{ $s\cceq 1$ }\{s=x^i\land i\leq N\}\]
  Passo quindi a $i\cceq 0$ seguendo il nuovo ordine delle condizioni:
  \[\vdash_{ass}\{1=x^0\land 0\leq N\}\mbox{ $i\cceq 0$ }\{1=x^i\land i\leq N\}\]
  Applico quindi la sequenza con la prima parte (sapendo $1=x^0$ è sempre vero):
  \[\vdash_{seq}\{N\geq 0\}\mbox{ \textit{A} }\{s=x^i\land i\leq N\}\]
  completando la dimostrazioni.\\
  Da notare solo che $x$ deve essere non nullo.
\end{esempio}
\textbf{\textit{Altri esempi sono presenti sulla pagina del corso}}.\\
\subsection{Correttezza totale}
Analizziamo ora il caso in cui non si abbia certezza di terminazione di
un'iterazione:
\[\{p\}\mbox{ while \textit{B} do \textit{C} endwhile }\{q\}\]
Distinguiamo i due tipi di correttezza, dal punto di vista della derivabilità,
tramite: 
\[\vdash^{parz}\{p\}\mbox{ C }\{q\}\]
\[e\]
\[\vdash^{tot}\{p\}\mbox{ C }\{q\}\]
Dal punto di vista semantico non ha invece senso distinguere di due casi e
quindi si ha solo:
\[\vDash\{p\}\mbox{ C }\{q\}\]
In quanto dal punto di vista semantico o si ha terminazione o non si ha, non si
hanno casistiche differenti a seconda di correttezza totale o parziale.\\
Bisognerà quindi dimostrare la terminazione.\\
Studiamo il caso semplice dove:
\[W=\mbox{ while \textit{B} do \textit{C} endwhile }\]
Cerchiamo un'espressione aritmetica $E$, dove compaiono le variabili del
programma, costanti numeriche e operazioni aritmetiche. Cerchiamo anche un
\textbf{invariante di ciclo} $i$ (che quindi è una formula) per $W$. $E$ e $i$
devono soddisfare due condizioni:
\begin{enumerate}
  \item $i\implies E\geq 0$, quindi se vale $i$ allora l'espressione aritmetica
  ha valore $\geq 0$ (il valore di $E$ lo ottengo eseguendo le operazioni sulle
  costanti e sui valori, in quello stato della memoria, delle variabili)
  \item $\vdash^{tot}\{i\land B\land E=k\}\mbox{ C }\{i\land E<k\}$, dove
  nella precondizione abbiamo la congiunzione logica tra l'invariante $i$, la
  condizione di ciclo $B$ e tra $E=k$, avendo prima assegnato a $k$ il valore
  effettivo di $E$ nello stato in cui iniziamo a computare $C$. $k$ quindi non
  deve essere una variabile del programma e non deve apparire in $C$
  (rappresentante il corpo della singola iterazione). Se vale la
  condizione 1 e l'invariante sappiamo che $E\geq 0\implies k\geq 0$. La
  postcondizione è formata dall'invariante $i$ e dal fatto che il valore di $E$
  dopo l'esecuzione sia strettamente minore di $k$ (che è il valore di $E$ prima
  dell'esecuzione). In pratica $E$ decresce ad ogni singola iterazione, ovvero
  ad ogni esecuzione di $C$.\\
  La notazione $\vdash^{tot}$ serve a escludere che $C$ abbia altri cicli
  annidati (portando a dover dimostrare che anche i cicli interni
  terminano). Per praticità quindi supponiamo di non avere cicli interni (stiamo
  partendo dal ciclo più interno)
\end{enumerate}
In pratica, nella seconda condizione, uso $E$ per concludere che una singola
esecuzione dell'iterazione mi porta in uno stato in cui vale l'invariante, dove
può valere o meno $B$ (che potrebbe portare all'uscita dall'iterazione) ma in
cui $E$ ha un valore diverso, minore a quello di partenza ma mai minore di 0 per
la prima condizione (in quanto l'invariante è sempre valido). Quindi, ad un
certo punto, $E$ raggiungerà il valore minimo e in quel momento o $B$ è falsa o
si ha una contraddizione ($E$ dovrebbe diventare negativo), quindi si ha la
terminazione. \\
Quindi se abbiamo dimostrato le due condizioni posso dire:
\[\vdash^{tot}\{i\}\mbox{ W }\{i\land\neg B\}\]
che è anche la conclusione della regola di derivazione dell'iterazione in caso
di correttezza parziale.\\
Possiamo anche osservare due cose:
\begin{enumerate}
  \item $E$ non è una formula logica ma un'espressione aritmetica con valore
  numerico ($E\geq 0$ è una formula logica)
  \item qualora si abbia $E=0$ non si hanno problemi, $0$ è solo un esempio,
  potremmo riscrivere la prima condizione come $E\geq n$, con $n$ qualsiasi
  numero intero, basta che sia ben definito per poter permettere la ripetizione
  finita delle iterazioni
\end{enumerate}
Chiameremo questa espressione aritmetica è \textbf{variante}.\\
\textit{L'invariante della correttezza totale non è sempre quello di quella
  parziale, magari è uguale, magari diverso o anche uguale solo in parte}
\begin{esempio}
  Vediamo un esempio chiarificatore.\\
  Dato il programma (una sorta di conto alla rovescia):
  \begin{listing}[H]
    \begin{lstlisting}
      while x > 5 do
        x ~ x - 1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:t}
  \end{listing}
  studio la correttezza totale della tripla:
  \[\{x>5\}\mbox{ P }\{x=5\}\]
  Innanzitutto cerco un variante $E$, che decresce ad ogni singola iterazione è
  un invariante $i$ come descritti sopra, quindi che, qualora ci sia
  l'invariante $i$ allora  $E\geq 0$.\\
  Un primo invariante ``banale'' è $i=x\geq 5$.\\
  In merito ad $E$ notiamo che nel corpo dell'iterazione abbiamo un solo
  comando, dove il valore della variabile $x$ viene decrementato, quindi un
  primo candidato per $E$ potrebbe essere proprio $x$.\\
  Per comodità scegliamo però come variante $x-5$ per avere una
  corrispondenza diretta con l'invariante $x\geq 5$, essendo derivato dallo
  stesso invariante (basandoci in primis sulla prima condizione).\\
  \textbf{Non sempre posso ottenere una corrispondenza tra variante e
    invariante}.\\
  Presi questo invariante e questo variante verifichiamo le due condizioni:
  \begin{enumerate}
    \item $x\geq 5\implies x-5\geq 0$ è ovviamente corretto perché si ottiene
    che $x\geq 5\implies x \geq 5$ (infatti questo è il motivo per cui si è
    scelto $x-5$ come variante)
    \item $\vdash^{tot}\{x\geq 5\land x>5\land x-5=k\}\,\,\,x\cceq
    x-1\,\,\,\{x\geq 5\land x-5<k\}$\\
    Applico quindi la regola dell'assegnamento (che si applica anche per la
    correttezza totale) e ottengo la precondizione per la postcondizione
    $\{x\geq 5\land x-5<k\}$:
    \[\{x-1\geq 5\land x-1-5<k\}=\{x\geq 6\land x-6 < k\}\]
    che però è diversa da $\{x\geq 5\land x>5\land x-5=k\}$.\\
    Cerco quindi di applicare la regola dell'implicazione. Riguardando la
    precondizione originale noto che $x>5$ è ``più forte'' di $x\geq 5$ e quindi
    quest'ultimo può essere trascurato. Ricordando che stiamo lavorando su
    valori interi si ha che $x>5\implies x\geq 6$. Inoltre, sempre per il fatto
    che lavoriamo su numeri interi, \\
    $x-5=k\implies x-6<k$ e quindi:
    \[\{x\geq 5\land x>5\land x-5=k\}\implies \{x\geq 6\land x-6 < k\}\]
  \end{enumerate}
  Abbiamo quindi dimostrato che il programma termina e vale la tripla
  iniziale.\\
  Qualora avessimo dovuto dimostrare la correttezza parziale si sarebbe dovuto
  procedere riutilizzando lo stesso invariante e procedendo studiando la
  negazione di $B$, ovvero $x\leq 5$ (che insieme danno la postcondizione).
\end{esempio}
\begin{esempio}
  vediamo un esempio non si ha alcuna variabile che decrementa nel corpo
  dell'iterazione:
  \begin{listing}[H]
    \begin{lstlisting}
      while x < 5 do
        x ~ x + 1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:ti}
  \end{listing}
  studio la correttezza totale della tripla:
  \[\{x<5\}\mbox{ P }\{x=5\}\]
  Dal punto di vista della correttezza parziale ragiono come al solito mentre
  per la correttezza totale la situazione è un po' diversa.\\
  Ovviamente non posso usare $x$ come variante ma posso sicuramente usare, per
  esempio, $-x$, che sicuramente decresce visto che $x$ cresce, in entrambi i
  casi ad ogni iterazione. Riprendendo l'esempio sopra quindi posso usare $x\leq
  5$ come invariante e, al posto di $-x$ che comunque andrebbe bene, secondo una
  ragionamento simile allo scorso esempio, $5-x$ come variante (si nota che
  anche questo $E$ è ricavabile da $i$, anche se questo non è sempre attuabile).
  Il resto della dimostrazione è analoga all'esempio precedente.
\end{esempio}
\subsubsection{Correttezza e Completezza}
In generale quando si sviluppa una logica, con un apparato deduttivo e
un'interpretazione delle formule, siamo interessati a due proprietà generali
della logica e dell'apparato deduttivo:
\begin{enumerate}
  \item \textbf{correttezza}, ovvero il fatto che tutto quello che si può
  derivare con l'apparato deduttivo è effettivamente vero, ovvero
  $vdash\implies\vDash$. Quindi se una tripla è derivabile è anche vera
  \item \textbf{completezza}, ovvero il fatto che l'apparato deduttivo sia in
  grado di derivare tutte le formule vere (ovvero tutte le triple) vere. Si ha
  quindi $\vDash\implies\vdash$
\end{enumerate}
Per la logica di Hoare vale la correttezza ma vale una proprietà di completezza
è \textit{relativa} in quanto nel corso di una dimostrazione di completezza
occorre a volte usare deduzioni della logica proposizionale ma tali formule
parlano anche di operazioni aritmetiche. Si ha che l'aritmetica può essere
formalizzata attraverso un linguaggio logico con degli assiomi e delle regole di
inferenza. Si hanno però i \textbf{teoremi di incompletezza dell'aritmetica} di
G\"{o}del che dicono che l'aritmetica come teoria matematica è incompleta in
quanto ci sono formule scritte nel linguaggio dell'aritmetica che sono vere ma
non sono dimostrabili. Questa incompletezza si riverbera sulla logica di Hoare,
che comunque, dal punto di vista deduttivo sulle triple, è completa.
\subsection{Precondizione più debole}
Dato un comando $C$ e una formula $q$ che interpretiamo come post condizione di
$C$. Si cerca $p$ tale per cui:
\[\vdash\{p\}\mbox{ C } \{q\}\]
Sia valida e quindi vera.\\
Ovviamente il caso interessante è quello totale.
\begin{esempio}
  Suppongo di avere come comando un singolo assegnamento $y\cceq 2\cdot x-1$ e
  come postcondizione $\{y>x\}$.\\
  Cerco possibili precondizioni che, dopo l'assegnamento, portino a quella
  postcondizione. Ovviamente si avrebbero infiniti valori di $x$ che consentono
  di arrivare alla postcondizione (nonché altrettanti che non lo permettono). 
\end{esempio}
Si cerca quindi la ``migliore'' precondizione per arrivare ad una data
postcondizione ma per farlo ci serve un criterio di confronto. \\
Introduciamo quindi alcuni simboli e nozioni utili:
\begin{itemize}
  \item $V$ è l'insieme della variabili di $C$
  \item $\Sigma=\{\sigma|\,\sigma:V\to\mathbb{Z}\}$ è l'insieme degli stati
  della memoria (ricordando che posso avere solo valori interi nel nostro
  linguaggio)
  \item $\Pi$ è l'insieme di tutte le formule sull'insieme $V$
  \item $\sigma \vDash p$ significa che la formula $p$ è vera nello stato
  $\sigma$ e si ha che $\vDash\subseteq\Sigma\times \Pi$, con $\vDash$ che
  indica la veridicità di una formula in uno stato
  \item $t(\sigma)=\{p\in \Pi|\,\sigma\vDash p\}$ come la funzione che assegna
  ad uno stato l'insieme delle proposizioni, ovvero tutte le formule, che sono
  vere in $\sigma$
  \item $m(p)=\{\sigma\in \Sigma|\,\sigma\vDash p\}$ come la funzione che
  associa ad una formula l'insieme di tutti gli stati che soddisfano la formula
\end{itemize}
Tra le due funzioni finali si ha una sorta di ``dualità'' e agiscono in modo
speculare tra loro. Se, partendo da $\Sigma$, applico $t(\sigma)$ e scopriamo
che un certo stato $p$ appartiene a $\Pi$ e a tale $p$ associamo l'insieme dato
da $m(p)$ si avrà che $\sigma\in m(p)$.\\
Si possono fare altre osservazioni su queste due funzioni. \\
Prendiamo due formule $p$ e $q$ e cerco di capire se è possibile avere
$m(p)=m(q)$. La risposta è positiva e si parla, in caso, di \textit{equivalenza
  tra formule}. Fissati due stati $\sigma_1$ e $\sigma_2$ mi chiedo se possano
appartenere allo stesso insieme di formule. In questo caso la risposta è
negativa, in quanto sei i due stati sono distinti allora ci deve essere almeno
una variabile $x$ che ha valore diverso nei due stati ma allora è facile trovare
una formula che separa i due stati e tale formula potrà essere vera solo in uno
dei due stati.\\
Dato $S\subseteq \Sigma$ e $F\subseteq\Pi$ (ragiono quindi su sottoinsiemi) si
ha che:
\begin{itemize}
  \item $t(S)=\{p\in \Pi|\,\forall\,\sigma\in S,\,\,\,\sigma\vDash
  p\}=\bigcap_{\sigma\in S}t(\sigma)$ (ragiono quindi su tutti gli stati di $S$
  e quindi sulle formule che sono vere in tutti questi stati) 
  \item $m(F)=\{\sigma\in \Sigma|\,\forall\,p\in F\,\,\,\sigma\vDash
  p\}=\bigcap_{p\in \Pi}m(p)$ (ragiono quindi su tutte le formule di $\Pi$
  e quindi su tutti gli stati in devono essere soddisfatte tutte queste formule) 
\end{itemize}
Abbiamo quindi esteso il dominio delle due funzioni a sottoinsiemi di stati e
sottoinsiemi di formule.\\
Si può anche dimostrare che $S\subseteq m(t(s))$ e che $F\subseteq
t(m(F))$. Inoltre, dati $A\subseteq B$, si può dimostrare che $m(B)\subseteq
m(A)$ (se ho un insieme più grande di formule ho meno stati che le soddisfano
tutte).\\
Si ha un rapporto tra la logica proposizionale (coi suoi connettivi logici) e
l'insieme di stati ($p$ e $q$ sono formule di $\Pi$): 
\begin{itemize}
  \item $m(\neg p)=\Sigma \,\,\backslash \,\,m(p)$, quindi il complemento
  insiemistico di $p$
  \item $m(p\lor q)=m(p)\cup m(q)$, quindi all'unione dei due insiemi  
  \item $m(p\land q)=m(p)\cap m(q)$ (da verificare) quindi all'intersezione dei
  due insiemi
\end{itemize}
L'implicazione merita un discorso a parte in quanto ha due ``nature'':
\begin{itemize}
  \item \textbf{operatore logico}, che comporta che $p\implies q= \neg p\lor q$
  e in tal caso si ha, in linea a quanto detto per gli altri connettivi:
  \[m(p\implies q)= m(\neg p)\cup m(q)\]
  \item \textbf{relazione tra formule}, dove se $p$ implica $q$ (in tutti in
  casi in cui è vera $p$ è vera $q$) allora
  $m(p)\subseteq m(q)$, intendendo che $q$ è ``più debole'' (ovvero come formula
  ci da una conoscenza inferiore essendo $q$ corrispondente ad un insieme più
  grande di stati) di $p$. Si ha a che
  fare con una relazione algebrica tra le due formule. Più ``debole'' non
  significa comunque ``peggiore''
\end{itemize}
Posso quindi capire come definire la precondizione migliore per ottenere una
tripla valida, insieme al comando $C$ e alla postcondizione.\\
Un modo è quello di definire la migliore precondizione come la precondizione più
debole $p$ che presa come precondizione forma una tripla valida:
\[\vdash \{p\}\mbox{ C }\{q\}\]
se $p$ è la precondizione più debole allora corrisponde al più grande insieme di
stati tale che la tripla sia valida.\\
Questa è una scelta ragionevole perché è la precondizione che impone meno
vincoli sullo stato iniziale, infatti determina tutti gli stati iniziali che
garantiscono il raggiungimento della postcondizione. È sempre possibile
calcolare la precondizione più debole.\\
Indichiamo, fissati $C$ comando e $q$ formula di postcondizione, con $wp(C, q)$
la precondizione più debole (\textit{weakest precondition}).
\begin{teorema}
  Si ha che $\vDash\{p\}\mbox{ C }\{q\}$ (quindi la tripla è vera) sse
  $p\implies wp(C,q)$ 
\end{teorema}
Vediamo quindi la \textit{regola di calcolo}.\\
Si hanno diversi casi a seconda dei tipi di comando:
\begin{itemize}
  \item \textbf{assegnamento:} in questo caso la precondizione più debole è
  quella determinata dalla regola di derivazione introdotta per la correttezza
  parziale. Quindi dato un assegnamento del tipo $x\cceq E$ e una postcondizione
  $q$ ho che la precondizione più debole si ottiene sostituendo in $q$ ogni
  occorrenza di $x$ con l'espressione $E$, ottenendo quindi:
  \[\{q[E/x]\}\]
  \item \textbf{sequenza:} in questo caso, se ho la sequenza di due comando
  $C_1$ e $C_2$ allora la precondizione più debole si calcola nel seguente modo:
  calcolo la precondizione più debole per $C_2$, ottenendo $wp(C_2,q)$, che sarà
  quindi la formula usata come postcondizione per calcolare la precondizione più
  debole di $C_1$, ovvero: $wp(C_1,wp(C_2,q))$. Questo non è altro che la
  condizione più debole della sequenza:
  \[wp((C_1;C_2),q)= wp(C_1,wp(C_2,q))\]
  \item \textbf{scelta:} in questo caso, avendo:
  \[S=\mbox{ if \textit{B} then \textit{C} else \textit{D} endif }\]
  fisso la postcondizione $q$ e procedo come per la regola di
  derivazione. Separo i due casi in cui sia vera $B$ o meno. Se $B$ è vera devo
  eseguire $C$ quindi calcolo, eseguendo $C$ la precondizione più debole
  $wp(C,q)$. Qualora $B$ non sia vera calcolo, eseguendo $D$, la precondizione
  più debole $wp(D,q)$. Nel complesso quindi, facendo la disgiunzione dei due
  casi, ottengo: 
  \[wp(S,q)=(B\land wp(C,q))\lor (\neg B\land wp(D,q))\]
\end{itemize}
\textbf{inserire prima parte lezione}\\
Abbiamo sviluppato una tecnica di studio basata su un linguaggio di
programmazione molto semplificato. Innanzitutto potremmo estendere il linguaggio
usato aggiungendo:
\begin{itemize}
  \item il do-while:
  \[\mbox{do \textnormal{C} while \textnormal{B} endwhile}\]
  che nella realtà corrisponde a:
  \[\mbox{\textnormal{C}; while \textnormal{B} do
      \textnormal{C} endwhile}\]
  \item il repeat:
  \[\mbox{repeat \textnormal{C} until \textnormal{B} endrepeat}\]
  che nella realtà corrisponde a:
  \[\mbox{\textnormal{C}; while not \textnormal{B} do
      \textnormal{C} endwhile}\]
  \item il ciclo for:
  \[\mbox{for(\mathnormal{D; B; F}) \mathnormal{C} endfor} \]
  sempre convertendolo in un while (\textit{ipotizzo così}):
   \[\mbox{while \textnormal{B} do
      \textnormal{C; F}  endwhile}\]
  \item procedure, metodi e funzioni
  \item array, anche se solo in lettura se vogliamo applicare la logica di Hoare
  come l'abbiamo vista
\end{itemize}
Un altro limite è dato dal fatto che abbiamo usato solo il tipo intero, ma
possiamo potenzialmente aggiungere anche gli altri senza cambiare le basi della
logica introdotte.
\subsection{Logica di Hoare per sviluppare programmi}
Possiamo vedere le logiche di Hoare come \textbf{contratti} tra chi scrive il
programma e l'utente. In questo contesto l'utente commissiona il programma
specificando la postcondizione e chiede al programmatore di garantire che tale
postcondizione venga garantita al termine dell'esecuzione. Per garantire la
postcondizione $q$ deve essere vera la precondizione $p$.\\
Vediamo un esempio.
\begin{esempio}
  Ci proponiamo di calcolare la radice quadrata intera di un certo $k\geq 0$,
  approssimando per difetto.\\
  Alla fine del programma voglio il risultato nella variabile $x$, quindi
  voglio:
  \[\{k\geq 0\}\mbox{ P }\{0\leq x^2\leq k<(x+1)^2)\}\]
  con $(x+1)^2$ per la correttezza dell'approssimazione.\\
  Ci si propone di fare il calcolo per approssimazioni successive, partendo da
  un valore più piccolo di quello corretto.\\
  Possiamo spaccare $q$ in:
  \[0\leq x^2\leq k \,\,\,\mbox{ e }\,\,\,k<(x+1)^2\]
  la prima possiamo considerarla come \textit{invariante} (anche solo $x^2\leq
  k$).\\
  
  All'inizio possiamo pensare che $x$ dipenda da $k$ per una certa funzione $E$.
  Si ha quindi un prototipo del genere (con un certo $B$, condizione booleana, e
  un certo $F$, incremento nel corpo, dipendenti da $x$ e $k$):
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ E(x)
      while B(x,k) do
        x ~ F(x,k);
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Alla fine deve valere $x^2\leq k \land \neg B$ per la regola
  dell'iterazione.\\
  Come $B$ posso porre $(x+1)^2\leq k$ (ovvero la negazione della seconda parte
  della postcondizione, in modo da ottenere, con la negazione, $q$).\\ Si
  ottiene quindi: 
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ 0
      while (x+1)^2 <= k do
        x ~ x+1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Bisognerebbe comunque dimostrare invariante e terminazione per validare il
  programma. 
\end{esempio}
Supponiamo un programma del tipo:
\[\{p\}\mbox{ A;W;C } \{q\}\]
che si risolve schematicamente con:
\[\{p\}\mbox{ A } \{inv\} \mbox{ W } \{inv\land \neg B\}\mbox{ C }\{q\}\]
Questo schema usa i cosiddetti \textbf{invarianti costruttivi}.\\
\subsection{Ultime considerazioni}
Vediamo qualche ultima considerazione sulla logica di Hoare.
\begin{itemize}
  \item alcune applicazioni pratiche della logica di Hoare:
  \begin{itemize}
    \item \textit{java modelling language}, un linguaggio di specificazione
    scritto in Java, che permette di fare \textit{design by contract} stabilendo
    delle precondizioni e delle postcondizioni 
    \item \textit{Eiffel, programming by contract}
    \item \textit{assert.h} in C
  \end{itemize}
  
  \item alcune applicazioni teoriche della logica di Hoare:
  \begin{itemize}
    \item \textit{semantica del programmi}, ovvero lo studio di cosa significa
    un programma, tramite:
    \begin{itemize}
      \item \textit{semantiche operazionali}
      \item \textit{semantiche assiomatiche}
      \item \textit{semantiche denotazionali}
      \item \textit{semantiche operazionali strutturate}
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{document}
% LocalWords:  Machine Learning dell multicore monocore checking mutex thread
% LocalWords:  race condition graph sottoprocessi Petri Morgan int const while
% LocalWords:  if return ht primis postcondizione Hoare then endif for endwhile
% LocalWords:  endfor program counter skip and not cccccc ccccc derivabilità
% LocalWords:  weakest precondition sse step repeat until endrepeat array java
% LocalWords:  modelling contract programming Eiffel assert language
% LocalWords:  postcondizioni denotazionali
