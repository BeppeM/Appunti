\documentclass[a4paper,12pt, oneside]{book}

% \usepackage{fullpage}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage[safe,extra]{tipa}
% \usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{fontspec}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{cancel}
\usepackage{braket}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{color}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage[cache=false]{minted}
\usepackage{mathtools}
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[space]{grffile} % For spaces in paths
\usepackage{etoolbox} % For spaces in paths
\makeatletter % For spaces in paths
\patchcmd\Gread@eps{\@inputcheck#1 }{\@inputcheck"#1"\relax}{}{}
\makeatother
\usepackage[noend]{algpseudocode}
\makeatletter
\def\cceq{\mathrel{\vcenter{\hbox{:}}{=}}}
\def\Cceq{\mathrel{\vcenter{\hbox{::}}{=}}}
\makeatother
\usepackage{tikz}\usetikzlibrary{er}\tikzset{multi  attribute /.style={attribute
    ,double  distance =1.5pt}}\tikzset{derived  attribute /.style={attribute
    ,dashed}}\tikzset{total /.style={double  distance =1.5pt}}\tikzset{every
  entity /.style={draw=orange , fill=orange!20}}\tikzset{every  attribute
  /.style={draw=MediumPurple1, fill=MediumPurple1!20}}\tikzset{every
  relationship /.style={draw=Chartreuse2,
    fill=Chartreuse2!20}}\newcommand{\key}[1]{\underline{#1}}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,shapes,backgrounds,petri}
\tikzset{
  place/.style={
    circle,
    thick,
    draw=black,
    minimum size=6mm,
  },
  transition/.style={
    rectangle,
    thick,
    fill=black,
    minimum width=8mm,
    inner ysep=2pt
  },
  transitionv/.style={
    rectangle,
    thick,
    fill=black,
    minimum height=8mm,
    inner xsep=2pt
  }
} 
\usetikzlibrary{automata,positioning, calc}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\definecolor{darkgreen}{rgb}{0.18, 0.43, 0.08}
\definecolor{watergreen}{rgb}{0.16, 0.66, 0.60}

\lstdefinelanguage{conc}{
  keywords={C},
  keywordstyle=\color{blue}\bfseries,
  keywords=[2]{skip},
  keywordstyle=[2]\color{watergreen}\bfseries,
  keywords=[3]{if, else, endif, for, while, then, endwhile, endfor, do},
  keywordstyle=[3]\color{darkgreen}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  classoffset=4, % starting new class
  otherkeywords={>,<,.,;,-,!,=, +, /, *},
  morekeywords={>,<,.,;,-,!,=, +, /, *},
  keywordstyle=\color{darkgray},
  classoffset=0,
}
\lstset{
  language=conc,
  extendedchars=true,
  basicstyle=\footnotesize\ttfamily,
  showstringspaces=false,
  showspaces=false,
  tabsize=2,
  breaklines=true,
  literate={./}{{{\color{red}./}}}2 {.^}{{{\color{red}.\^{}}}}2
  {:}{{{\color{red} $\ \Cceq\ $}}}1
  {./}{{{\color{purple}./}}}2 {.^}{{{\color{purple}.\^{}}}}2
  {~}{{{\color{purple} $\ \cceq\ $}}}1
  {./}{{{\color{green}./}}}2 {.^}{{{\color{green}.\^{}}}}2
  {|}{{{\color{green} |}}}1
  {./}{{{\color{orange}./}}}2 {.^}{{{\color{orange}.\^{}}}}2
  {;}{{{\color{orange} ;}}}1, 
  showtabs=false
}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}


\title{Modelli della Concorrenza}
\author{UniShare\\\\Davide Cozzi\\\href{https://t.me/dlcgold}{@dlcgold}}
\date{}

\pgfplotsset{compat=1.13}
\begin{document}
\maketitle

\definecolor{shadecolor}{gray}{0.80}
\setlist{leftmargin = 2cm}
\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}
\tableofcontents
\renewcommand{\chaptermark}[1]{%
  \markboth{\chaptername
    \ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%
\newcommand{\simplies}{{\implies}}
\newcommand{\siff}{{\iff}}
\newcommand{\notimplies}{\;\not\!\!\!\simplies}
\newcommand{\notiff}{\;\not\!\!\!\iff}
\chapter{Introduzione}
\textbf{Questi appunti sono presi a lezione. Per quanto sia stata fatta
  una revisione è altamente probabile (praticamente certo) che possano
  contenere errori, sia di stampa che di vero e proprio contenuto. Per
  eventuali proposte di correzione effettuare una pull request. Link: }
\url{https://github.com/dlcgold/Appunti}.\\
\textbf{Le immagini presenti in questi appunti sono tratte dalle slides del
  corso e tutti i diritti delle stesse sono da destinarsi ai docenti del corso
  stesso}.
\chapter{Introduzione alla Concorrenza}
La \textbf{concorrenza} è presente in diversi aspetti della quotidianità.
Un primo esempio di \textbf{sistema concorrente} non legato all'informatica è 
la \textit{cellula vivente}: può essere vista come un dispositivo che
trasforma e manipola dati per ottenere un risultato. I vari processi all'interno
di una cellula avvengono in modo concorrente, il che la rende un \textit{sistema
  asincrono}. Un secondo esempio può essere
quello dell'\textit{orchestra musicale}: i vari componenti suonano spesso
simultaneamente rappresentando un \textit{sistema sincrono} (ovvero un sistema
che funziona avendo una sorta di ``cronometro'' condiviso dai vari attori). 
Un esempio informatico invece è un \textit{processore multicore} (anche se in
realtà anche se fosse \textit{monocore} sarebbe comunque un sistema concorrente
per ovvie ragioni). Anche una \textit{rete di calcolatori} è un modello
concorrente, nonché i \textit{modelli sociali umani}.
\subsubsection{Caratteristiche comuni}
I modelli concorrenti hanno alcuni aspetti comuni, tra cui:
\begin{itemize}
  \item competizione per l’accesso alle risorse condivise
  \item cooperazione per un fine comune (che può portare a competizione)
  \item coordinamento di attività diverse
  \item sincronia e asincronia
\end{itemize}
\subsubsection{Studio}
Durante lo studio e la progettazione di sistemi concorrenti si hanno diversi problemi
peculiari che rendono il tutto molto complesso. Un sistema
concorrente mal progettato può avere effetti catastrofici.  

Per poter sviluppare modelli concorrenti si necessita innanzitutto di:
\begin{itemize}
  \item \textbf{linguaggi}, per specificare e rappresentare sistemi concorrenti. 
  \begin{itemize}
    \item \textbf{linguaggi di programmazione} (con l'uso di \textit{thread},
    \textit{mutex}, scambio di messaggi, etc$\ldots$ con i vari
    problemi di \textit{race condition}, uso di variabili condivise
    etc$\ldots$).
    
    \item linguaggi rappresentativi, come ad esempio  
    una \textit{partitura musicale} (nella quale si visualizza bene la natura
    \textit{sincrona}).

    \item \textbf{task graph (\textit{grafo delle
        attività})}, nel quale i nodi sono le attività (o eventi) mentre gli archi
    rappresentano una \textit{relazione d'ordine parziale}, come per esempio una
    \textit{relazione di precedenza} sui nodi.

    \item \textbf{algebre di processi}, simile ad un sistema di equazioni, con simboli
    che rappresentano eventi del sistema concorrente e operatori atti a comporre
    fra loro i vari sottoprocessi del sistema concorrente. Ogni ``equazione''
    descrive un processo che costituisce un elemento di un sistema concorrente.
    
    \item \textbf{modelli}, per modellare sistemi concorrenti in astratto. Un
    esempio è dato dalle \textbf{reti di Petri}, che modellano un sistema
    concorrente partendo dalle nozioni di \textit{stato locale} di uno dei
    componenti del sistema e di \textit{evento locale} che ha un effetto su alcune
    componenti (e non tutte). Si ha quindi rappresentato un \textit{sistema
      dinamico} che si evolve nel tempo e la cui evoluzione è rappresentata tramite
    \textit{relazioni di flusso}).

  \end{itemize}
  
  \item \textbf{logica}, per analizzare e specificare sistemi concorrenti.
  \item \textbf{model-checking}, per validare formule relative a proprietà di
  sistemi concorrenti.
\end{itemize}

\chapter{Logica}

\emph{Si ringrazia
  \MYhref{https://github.com/bigboss98/Appunti-1/tree/master/Primo Anno/Fondamenti}{Marco
    Natali}
  \footnote{Link al repository:
    https://github.com/bigboss98/Appunti-1/tree/master/Primo Anno/Fondamenti} per questo
  ripasso}.\\\\   
La logica è lo studio del ragionamento e dell’argomentazione e, in particolare,
dei procedimenti inferenziali, rivolti a chiarire quali	procedimenti di pensiero
siano validi e quali no. Vi sono molteplici tipologie di logiche, come ad
esempio la logica classica e le logiche costruttive, tutte accomunate dall'essere
composte da 3 elementi: 
% Elementi di una Logica
\begin{itemize}
  \item \textbf{Linguaggio}: insieme di simboli utilizzati nella Logica per
  definire le cose.
  \item \textbf{Sintassi}: insieme di regole che determina quali elementi
  appartengono o meno al linguaggio.
  \item \textbf{Semantica}: permette di dare un significato alle formule del
  linguaggio e determinare se rappresentano o meno la verità.
\end{itemize}

\section{Logica proposizionale}
Ci occupiamo della \textit{logica classica} che si compone in \textit{logica proposizionale} 
e \textit{logica predicativa}.
La logica proposizionale è quindi un tipo di logica classica che presenta come
caratteristica principale quella di essere un linguaggio limitato, ovvero caratterizzato dal poter
esprimere soltanto proposizioni senza possibilità di estensione ad una
classe di persone.
\newpage
\subsection{Sintassi}
Il linguaggio di una logica proposizionale è composto dai seguenti elementi:

% Elementi linguaggio logica proposizionale
\begin{itemize}
  \item Variabili Proposizionali atomiche (o elementari): $P,Q,R,p_i, \dots$. 
  \item Connettivi Proposizionali: $\land, \lor, \neg, \implies, \siff$
  \item Simboli Ausiliari: ``(`` e ``)'' (detti delimitatori)
  \item Costanti: $T$ (\textit{True, Vero, $\top$}) e $F$ (\textit{False, Falso,
    $\bot$})
\end{itemize}

La sintassi di un linguaggio è composta da una serie di formule ben
formate ($FBF$) definite induttivamente nel seguente modo:
% definizione formule ben formate
\begin{enumerate}
  \item Le costanti e le variabili proposizionali: $\top,\bot,p_i\in FBF$.
  \item Se $A$ e $B \in FBF$ allora $(A \land B)$,$(A \lor B)$,$(\neg A)$,$(A
  \simplies B)$, $(A \siff B)$ sono delle formule ben formate.
  \item nient'altro è una formula
\end{enumerate}

\textbf{In una formula ben formata le parentesi sono bilanciate.}

\begin{esempio}
  Vediamo degli esempi:
  \begin{itemize}
    \item $(P \land Q) \in FBF$  è una formula ben formata\newline
    \item $(PQ \land R) \not \in FBF$ in quanto non si rispetta la sintassi del
    linguaggio 
    definita. 
  \end{itemize}
\end{esempio}

% Definizione delle sottoformule
\begin{definizione}

  Sia $A \in FBF$, l'insieme delle sottoformule di $A$ è definito come segue:
  \begin{enumerate}
    \item Se $A$ è una costante o variabile proposizionale allora A stessa è la
    sua 
    sottoformula.
    \item Se $A$ è una formula del tipo $(\neg A')$ allora le sottoformule di A
    sono 
    A stessa e le sottoformule di $A'$; 
    $\neg$ è detto connettivo principale e $A'$ sottoformula immediata di A.
    \item Se $A$ è una formula del tipo $B \circ C$, allora le sottoformule di A
    sono A stessa 
    e le sottoformule di B e C; $\circ$ è il connettivo principale e B e C sono
    le due sottoformule immediate di A. 
  \end{enumerate}

\end{definizione}
È possibile ridurre ed eliminare delle parentesi attraverso l'introduzione della
precedenza tra gli operatori, definita come segue: 
$$
\neg, \land, \lor, \simplies,\siff
$$

In assenza di parentesi una formula va parentizzata privilegiando le
sottoformule 
i cui connettivi principali hanno la precedenza più alta.\newline
In caso di parità di precedenza vi è la convenzione di associare da destra a
sinistra. Segue un esempio:
$$
\neg A \land (\neg B \simplies C) \lor D 
\hbox{ diventa }
((\neg A) \land ((\neg B) \simplies C) \lor D)
$$
\subsubsection{Albero Sintattico}
% Definizione di albero sintattico
\begin{definizione}
  Un albero sintattico $T$ è un albero binario coi nodi etichettati da simboli
  di $L$, che rappresenta la scomposizione di una formula ben formata $X$
  definita 
  come segue: 
\end{definizione}
\begin{enumerate}
  \item Se $X$ è una formula atomica, l'albero binario che la rappresenta è
  composto 
  soltanto dal nodo etichettato con $X$
  \item Se $X = A \circ B$, $X$ è rappresentata da un albero binario che ha la
  radice 
  etichettata con $\circ$, i cui figli sinistri e destri sono la
  rappresentazione di $A$ e $B$ 
  \item Se $X = \neg A$, $X$ è rappresentato dall'albero binario con radice
  etichettata 
  con $\neg$, il cui figlio è la rappresentazione di $A$
\end{enumerate}

Poiché una formula è definita mediante un albero sintattico, le proprietà di una
formula 
possono essere dimostrate mediante induzione strutturale sulla formula, ossia
dimostrare 
che la proprietà di una formula soddisfi i seguenti 3 casi:
\begin{itemize}
  \item è verificata la proprietà per tutte le formule atomo $A$
  \item supposta verifica la proprietà per $A$, si verifica che la proprietà è
  verificata per $\neg A$ 
  \item supposta la proprietà verificata per $A_1$ e $A_2$, si verifica che la
  proprietà è verifica per $A_1 \circ A_2$, per ogni connettivo $\circ$.
\end{itemize}
\newpage
\subsection{Semantica}
La semantica di una logica consente di dare un significato e un'interpretazione
alle formule del Linguaggio.\newline
\begin{definizione}
  Sia data una formula proposizionale $P$ e sia ${P_1,\dots,P_n}$, l'insieme
  degli 
  atomi che compaiono nella formula $A$. Si definisce come
  \emph{interpretazione} una 
  funzione $v:\{P_1,\dots,P_n\} \mapsto \{T,F\}$ che attribuisce un valore di
  verità 
  a ciascun atomo della formula $A$.
  \\
  $v:P\to\{0,1\}$ è un'\textbf{assegnazione booleana} 
\end{definizione}

I connettivi della Logica Proposizionale hanno i seguenti valori di verità:
% Tabella di Verità degli operatori
\[
  \begin{array}{ccccccc}
    \toprule
    \text{A} & \text{B} & A \land B & A \lor B & \neg A & A \simplies B & A
                                                                          \siff
                                                                          B \\
    \midrule
    F & F & F & F & T & T & T \\
    F & T & F & T & T & T & F \\
    T & F & F & T & F & F & F \\
    T & T & T & T & F & T & T \\
    \bottomrule
  \end{array}
\]
Essendo ogni formula $A$ definita mediante un unico albero sintattico,
l'interpretazione $v$ 
è ben definita e ciò comporta che data una formula $A$ e un'interpretazione
$v$, 
eseguendo la definizione induttiva dei valori di verità, si ottiene un unico
$v(A)$. 

% Tipologie di formule
\begin{definizione}
  Una formula nella logica proposizionale può essere di diversi tipi:
  \begin{itemize}
    \item \textbf{Valida o Tautologica:} la formula è soddisfatta da qualsiasi
    valutazione della Formula 
    \item \textbf{Soddisfacibile NON Tautologica:} la formula è soddisfatta da
    qualche valutazione 
    della formula ma non da tutte.
    \item \textbf{Falsificabile:} la formula non è soddisfatta da qualche
    valutazione della formula. 
    \item \textbf{Contraddizione:} la formula non viene mai soddisfatta
  \end{itemize}
\end{definizione}

\begin{teorema}
  Si ha che:
  \begin{itemize}
    \item $A$ è una formula valida se e solo se $\neg A$ è insoddisfacibile.
    \item $A$ è soddisfacibile se e solo se $\neg A$ è falsificabile
  \end{itemize}
\end{teorema}


\subsubsection{Modelli e decidibilità}
Si definisce \emph{modello}, indicato con $M \models A$, tutte le valutazioni
booleane 
che rendono vera la formula $A$.
Si definisce \emph{contromodello}, indicato con $\not\models$, tutte le
valutazioni booleane 
che rendono falsa la formula $A$.

La logica proposizionale è decidibile (posso sempre verificare il significato di
una formula). 
Esiste infatti una procedura effettiva che stabilisce la validità o no di una
formula, o se questa 
ad esempio è una tautologia.
In particolare il verificare se una proposizione è tautologica o meno è
l’operazione di decidibilità principale che si svolge nel calcolo
proposizionale. 

\begin{definizione}
  Se $M \models A$ per tutti gli $M$, allora $A$ è una tautologia e si indica
  $\models A$.
\end{definizione}

\begin{definizione}
  Se $M \models A$ per qualche $M$, allora $A$ è soddisfacibile.
\end{definizione}

\begin{definizione}
  Se $M \models A$ non è soddisfatta da nessun $M$, allora $A$ è
  insoddisfacibile.
\end{definizione}
\subsection{Equivalenze Logiche}
\begin{definizione}
  Date due formule $A$ e $B$, si dice che $A$ è \emph{logicamente equivalente} a
  $B$, 
  indicato con $A \equiv B$, se e solo se per ogni interpretazione $v$ risulta
  $v(A) = v(B)$. 
\end{definizione}

Nella logica proposizionale sono definite le seguenti equivalenze logiche,
indicate con $\equiv$: 
\begin{enumerate}
  \item \textbf{Idempotenza:}
  \begin{align*}
    A \lor A  \equiv  A \\
    A \land A  \equiv  A \\
  \end{align*}
  \item \textbf{Associatività:}
  \begin{align*}
    A \lor (B \lor C) \equiv  (A \lor B) \lor C \\
    A \land (B \land C)  \equiv  (A \land B) \land C
  \end{align*}
  \item \textbf{Commutatività:}
  \begin{align*}
    A \lor B  \equiv  B \lor A \\
    A \land B  \equiv  B \land A
  \end{align*}
  \item \textbf{Distributività:}
  \begin{align*}
    A \lor (B \land C)  \equiv & (A \lor B) \land (A \lor C)\\
    A \land (B \lor C)  \equiv & (A \land B \lor (A \land C)
  \end{align*}
  \item \textbf{Assorbimento:}
  \begin{align*}
    A \lor (A \land B)  \equiv  A
    A \land (A \lor B)  \equiv  A
  \end{align*}
  \item \textbf{Doppia negazione:}
  \begin{equation*}
    \neg \neg A \equiv A
  \end{equation*}
  \item\textbf{Leggi di De Morgan:}
  \begin{align*}
    \neg (A \lor B)  \equiv  \neg A \land \neg B \\
    \neg(A \land B)  \equiv  \neg A \lor \neg B
  \end{align*}
  \item \textbf{Terzo escluso:}
  \begin{equation*}
    A \lor \neg A \equiv T
  \end{equation*}
  \item \textbf{Contrapposizione:}
  \begin{equation*}
    A \simplies B \equiv \neg B \simplies \neg A
  \end{equation*}
  \item \textbf{Contraddizione}
  \begin{equation*}
    A \land \neg A \equiv F
  \end{equation*}
\end{enumerate}
\subsubsection{Completezza di insiemi di Connettivi}
Un insieme di connettivi logici è completo se mediante i suoi connettivi si può
esprimere un qualunque altro connettivo.
Nella logica proposizionale valgono anche le seguenti equivalenze, utili per
ridurre il linguaggio: 

\[(A \simplies B)  \equiv  (\neg A \lor B) \]
\[(A \lor B)  \equiv  \neg(\neg A \land \neg B) \]
\[(A \land B)  \equiv  \neg(\neg A \lor \neg B) \]
\[(A \siff B) \equiv  (A \simplies B) \land (B \simplies A) \]

L'insieme dei connettivi $\{ \neg,\lor,\land \}$, $\{ \neg,\land \}$ e $\{
\neg,\lor \}$ sono completi.

\chapter{Correttezza di programmi sequenziali}
Introduciamo l'argomento con un esempio.
\begin{esempio}
  Definiamo una funzione in C che riceve un vettore, un intero (la lunghezza del vettore) e
  e restituisce un ulteriore numero intero.
  \begin{listing}[ht]
    \begin{minted}{c}
      int f(int n, const int v[]) {
        int x = v[0];
        int h = 1;
        while (h < n) {
          if (x < v[h])
          x = v[h];        
          h = h + 1;
        }
        return x;
      }
    \end{minted}
    \caption{Esempio di funzione in C}
    \label{listing:1}
  \end{listing}
  Chiedendoci cosa fa la funzione scopriamo che si occupa di cercare il massimo in
  un vettore.\\
  La strategia della funzione è quella di spostarsi lungo il vettore e
  conservare in $x$ il valore massimo fino ad ora trovato. Arrivati alla fine
  del vettore so che in $x$ avrò il valore massimo.\\
  Più formalmente suppongo che $n$ sia $n > 0$, per dire che ho almeno un
  elemento nel vettore. Suppongo inoltre che $v[i]\in\mathbb{Z},\,\,\forall i\in
  \{0,\ldots, n-1\}$. Abbiamo fissato le \textbf{condizioni iniziali}.\\
  All'inizio $x$ è il massimo del sotto-vettore con solo il primo elemento
  ($v[0..0]$) e dopo l'assegnamento di $h$ in $v[o..h-1]$, che, con $h=1$ mi
  conferma che x è il massimo in $v[0..0]$. \\
  Al termine di una certa iterazione $x$ contiene il massimo 
  tra i valori compresi tra $v[0]$ e $v[h-1]$ (detto altrimenti il massimo in
  $v[0..h-1]$). Inoltre al fine di una certa iterazione mi aspetto che $h\leq
  n$.\\
  Possiamo quindi dire che quando esco dal ciclo $x$ è il massimo in $v[0..h-1]$
  ma in questo momento $h=n$ e quindi $x$ è il massimo del vettore.\\
  Consideriamo ora la parte iterativa. All'inizio di ogni iterazione suppongo
  che $x$ è il massimo in $v[0..h-1]$. Dopo l'istruzione di scelta $x$ è il
  massimo in $v[0..h]$, comunque sia andata la scelta. Alla fine
  dell'iterazione, dopo l'incremento di $h$, avrò ancora che $x$ è il massimo in
  $v[0..h-1]$. Ragiono quindi per induzione. Se all'inizio dell'iterazione e
  alla fine ho la stessa asserzione, ed è vera prima di iniziare l'iterazione,
  posso dire che ho una \textbf{proprietà invariante} e vale anche al termine
  dell'ultima iterazione e quindi vale anche alla fine dell'esecuzione del
  programma. 
\end{esempio}
Nell'esempio notiamo in primis l'assenza di formalità. Si introducono quindi
concetti:
\begin{enumerate}
  \item \textbf{precondizione} che nell'esempio è fatta da $n>0$ e
  $v[i]\in\mathbb{Z},\,\,\forall i\in \{0,\ldots, n-1\}$
  \item \textbf{postcondizione} che nell'esempio si ritrova con l'asserzione $x$
  è il massimo in $v[0..h-1]$ e $h=n$, che scritto in modo formale diventa:
  \[
    \begin{rcases}
      v[i]\leq x,\,\,\forall i\in \{0,\ldots, n-1\}\\
      \exists\,i\in\{0,\ldots, n-1\}\mbox{ t.c. } v[i]=x
    \end{rcases}
    x=max(v[0..n-1])
  \]
\end{enumerate}
\textit{Queste formule possono essere rese come formule proposizionali, tramite
  una congiunzione logica:}
\[
  \begin{cases}
    v[0]\leq x \land v[1]\leq x\land\ldots v[n-1]\leq x\\
    v[0]= x \lor v[1]= x\lor\ldots v[n-1]=x
  \end{cases}
\]
Abbiamo studiato lo stato della memoria del programma in un certo istante
tramite formule.
\begin{definizione}
  Definiamo \textbf{stato della memoria} come:
  \[s:V\to\mathbb{Z}\]
  ovvero una funzione che mappa le variabili del programma (poste nell'insieme
  $V$) in $\mathbb{Z}$.
\end{definizione}
Fissato uno stato della memoria e una formula posso validare una formula in
quello stato osservando le variabili e le relazioni aritmetiche della
formula.\\
Data una formula $\phi$ e uno stato $s$ posso sapere se $\phi$ è valida in
$s$.\\
Una formula che gode della \textbf{proprietà invariante} se vera all'inizio
dell'iterazione è vera anche alla fine della stessa. Per capire se è invariante
basta vedere lo stato di una formula ad inizio e fine di una iterazione.\\
L'esecuzione di una istruzione cambia lo stato della memoria. Potrebbe però
accadere che una serie di istruzioni non facciano terminare il programma, perciò
quanto detto sopra è in realtà un'approssimazione della realtà.
\begin{definizione}
  Definiamo la \textbf{specifica di correttezza di un programma} con la tripla:
  \[\alpha\,\, P\,\, \beta\]
  dove:
  \begin{itemize}
    \item $\alpha$ e $\beta$ sono formule (definite con tutte le simbologie
    aritmetiche tra variabili, sia di conto che di relazione).
    \item $P$ è un ``programma'' (anche un frammento o una singola istruzione) che
    modifica lo stato della memoria.
  \end{itemize}
  $\alpha$ è la \textbf{precondizione}, che supponiamo verificata nello stato
  iniziale e $\beta$ è la \textbf{postcondizione}, 
  che supponiamo valida dopo l'esecuzione del programma.
\end{definizione}
Durante il corso useremo un linguaggio imperativo non reale semplificato.
Dovremo anche definire una logica, definendo un apparato deduttivo, un insieme
di regole per costruire dimostrazioni derivando nuove formule da quelle
preesistenti. Useremo la \textbf{logica di Hoare}. Le formule della logica di
Hoare sono triple di tipo $(\alpha P \beta)$ quindi si tratta di una logica di tipo diverso
anche se si appoggia su quella proposizionale.
\section{Linguaggio semplificato}
Definiamo quindi il linguaggio di programmazione imperativo semplificato che
andremo ad utilizzare. Si userà una grammatica formale.\\
L'elemento fondamentale di questo linguaggio è il \textbf{comando}, che indica o
una singola istruzione o un gruppo di istruzioni strutturate. Il simbolo usato
nella grammatica per indicare un comando è ``C''. Un comando viene costruito
tramite le \textbf{produzioni}, introdotte da ``::=''. Il comando più semplice è
l'\textbf{assegnamento}, che usa l'operatore ``:='' per assegnare un valore ad
una variabile. Con il simbolo ``E'' indichiamo un simbolo non terminale della
grammatica che sta per \textit{espressione}. Una volta costruito semplici
espressioni possiamo combinarle, eseguendole in sequenza, inserendo un ``;'' tra
due comandi.\\ 
In merito all'istruzione di scelta abbiamo l'istruzione ``if'', seguito da
un'espressione booleana, seguito da ``then'', seguita da un comando, seguita da
``else'', seguita da un comando, e il tutto viene concluso da ``endif''. Qui
abbiamo una prima semplificazioni dicendo che l'\textit{else} è
obbligatorio. Per l'iterazione abbiamo il ``while'', seguito da un'espressione
booleana, seguito da ``do'' con poi il comando, il tutto concluso da
endwhile. Infine abbiamo una istruzione speciale, chiamata ``skip'', che non fa
nulla e avanza il \textit{program counter} (con essa posso saltare il ramo
alternativo dell'\textit{if-else}).\\
Un'espressione booleana ``B'' può essere la costante ``true'', la costante
``false'' o del tipo ``not B'', ``B and B'', ``B or B'', ``E < E'' (e le altre),
``E = E''. Non si hanno tipi di dato ma supporremo di avere a che fare solo con
\textit{interi}. Non si ha la funzione di nozione o di classe. Questo linguaggio
è comunque \textit{Turing Complete}.
\begin{listing}[H]
  \begin{lstlisting}
    x ~ a; y ~ b;
    while x != y do
    if x < y then
    y ~ y - x;
    else
    x ~ x - y;
    endif
    endwhile  
  \end{lstlisting}
  \caption{Esempio di programma $D$}
  \label{listing:D}
\end{listing}
Cerchiamo di capire se il programma $D$, sopra definito, soddisfa la tripla:
\[\{a>0\land b>0\}\,\,D\,\, \{x=MCD(a,b)\}\]
Quindi mi chiedo se eseguendo il programma con uno stato della memoria dove $a$
e $b$ sono due interi positivi (precondizione) allora, alla fine dell'esecuzione
di $D$, $x$ sarà il massimo comune divisore tra $a$ e $b$
(postcondizione). Dobbiamo dimostrare la tripla e qualora non fosse vera bisogna
confutarla trovando un caso in cui non è verificata (trovando uno stato che
soddisfi la precondizione ma che, una volta eseguito il programma, la postcondizione non 
sia verificata).
\section{Logica di Hoare}
\begin{definizione}
  Una \textbf{dimostrazione}, in una logica data, è una sequenza di formule di
  quella logica che sono o \textit{assiomi} (formule che riteniamo vere a
  priori) o formule derivate dalle precedenti tramite una \textit{regola di
    inferenza}. Una regola di inferenza mi manda da un insieme di formule
  $\alpha_1,\alpha_2,\ldots,\alpha_n$ ad una nuova formula $\alpha$ e si indica
  con: 
  \[\frac{\alpha_1,\alpha_2,\ldots,\alpha_n}{\alpha}\]
  Che si legge come: "\textit{se ho già derivato $\alpha_1, \alpha_2,
    \ldots,\alpha_n$ sono autorizzato a derivare $\alpha$}".\\ 
  Nel nostro caso ogni $\alpha_i$ è una tripla della \textbf{logica di Hoare}.\\
  Una dimostrazione si ottiene quindi applicando le regole di derivazione fino
  ad arrivare, se si riesce, ad una soluzione.
\end{definizione}
\subsection{Regole di derivazione}
Vediamo quindi le regole di derivazione, che sono associate alle regole del
linguaggio sopra definito.
\subsubsection{Skip}
\begin{definizione}
  Partiamo con la regola per l'istruzione \textbf{\textit{skip}}, che non
  facendo nulla 
  non cambia lo stato della memoria e quindi la regola di derivazione non ha
  nessuna premessa:
  \[\frac{}{\{p\}\,\,skip\,\,\{p\}}\]
  con $p$ che è una formula proposizionale. Dopo lo \textit{skip} $p$ vale se
  valeva prima dello \textit{skip}
\end{definizione}
\subsubsection{Implicazione}
\begin{definizione}
  La seconda è una regola che non ha un rapporto diretto con il linguaggio e
  chiameremo \textbf{regola di conseguenza (o dell'implicazione)}. Ha due
  premesse: una tripla appartenente alla logica di Hoare e una implicazione della
  logica proposizionale.
  \[\frac{p\implies p'\,\, \,\,\{p'\}\,\,C\,\,\{q\}}{\{p\}\,\,C\,\,\{q\}}\]
  Ovvero se eseguo $C$ partendo da uno stato in cui vale $p'$ allora dopo varrà
  $q$. Ma sappiamo anche che $p$ implica $p'$. Quindi se nel mio stato della
  memoria vale $p$ e quindi anche $p'$ per l'implicazione. Posso quindi dire che
  se ho $p$ ed eseguo $C$ ottengo $q$.\\
  Ho anche una forma speculare:
  \[\frac{\{p\}\,\,C\,\,\{q'\}\,\, \,\,q'\implies q}{\{p\}\,\,C\,\,\{q\}}\]
\end{definizione}
\subsubsection{Sequenza}
\begin{definizione}
  La terza regola è legata alla struttura di \textbf{sequenza} dei programmi:
  \[\frac{\{p\}\,\,C_1\,\,\{q\}\,\,
      \,\,\{q\}\,\,C_2\,\,\{r\}}{\{p\}\,\,C_1;C_2\,\,\{r\}}\] 
\end{definizione}
\subsubsection{Assegnamento}
\begin{definizione}
  La quarta regola riguarda l'\textbf{assegnamento}. Questa è l'unica regola
  non banale (come lo è lo \textit{skip}) che non ha premesse. È la regola base
  per derivare le triple necessarie alle altre regole. Quindi, avendo $E$ come
  espressione, $x$ una variabile e $p$ come una postcondizione, ovvero una
  formula che contiene gli identificatori di diverse variabili:
  \[\frac{}{\{p[E/x]\}\,\,x\cceq E\,\,\{p\}}\]
  Dove con $p[E/x]$, come precondizione, indichiamo una \textbf{sostituzione} 
  indicante che cerchiamo in $p$ tutte le occorrenze $x$ e le sostituiamo con $E$.
  \begin{esempio}
    Se ho $x \cceq y+1$ con $E$ pari a $y+1$ e con $p$ pari a $\{x>0\}$ come
    postcondizione data e cerco la precondizione. Quindi la tripla completa
    sarebbe: 
    \[\{y+1>0\}\,\,x \cceq y+1\,\,\{x>0\}\]
    E la tripla sappiamo che è \emph{vera} (se so che $y+1$ è positivo, dopo
    che a $x$ assegno $y+1$ posso essere sicuro che anche $x$ è positivo).
  \end{esempio}
  \begin{esempio}
    Se ho $x \cceq x+1$ con $E$ pari a $x+2$ e con $p$ pari a $\{x>0\land x\leq
    y\}$ come postcondizione data e cerco la precondizione. Quindi la tripla
    completa sarebbe:
    \[\{x+2>0\land x+2\leq y\}\,\,x \cceq x+1\,\,\{x>0\land x\leq y\}\]
    e anche questa tripla è garantita dalla regola di sostituzione
  \end{esempio}
\end{definizione}
\subsubsection{Istruzione di scelta}
\begin{definizione}
  La quinta regola è quella relativa all'\textbf{istruzione di scelta} che è
  della forma:
  \[\{p\}\mbox{ if \textit{B} then \textit{C} else \textit{D} endif }\{q\}\]
  Se la condizione $B$ è vera eseguo $C$ altrimenti $D$. In entrambi i casi
  alla fine deve valere la postcondizione $q$. Separiamo i due casi:
  \begin{itemize}
    \item se suppongo vere $p$ e $B$ eseguo $C$ arrivando in $q$:
    \[\{p\land B\}\,\,\,C\,\,\,\{q\}\]
    \item se suppongo vera $p$ ma falsa $B$ avrò:
    \[\{p\land \neg B\}\,\,\,D\,\,\,\{q\}\]
  \end{itemize}
  Ricavo quindi la formula generale:
  \[\frac{\{p\land B\}\,\,\,C\,\,\,\{q\}\,\,\,\,\,\,\,\,\,
      \{p\land \neg B\}\,\,\,D\,\,\,\{q\}}{\{p\}\mbox{ if \textit{B}
        then \textit{C} else \textit{D} endif }\{q\}}\]
\end{definizione}
\begin{shaded}
  Come notazione usiamo che:
  \[\vdash \{p\}\,\,\,C\,\,\,\{q\}\]
  dove $\vdash$
segnala che la tripla è stata \textbf{dimostrata/derivabile} con le regole di
derivazione (si parla quindi di \textit{sintassi}, viene infatti ignorato il
significato ma si cerca solo di applicare le regole, ottenendo al conclusione
come risultato di una catena di regole).\\
Come notazione usiamo anche che:
\[\vDash \{p\}\,\,\,C\,\,\,\{q\}\]
dove $\vDash$
indica che la tripla è \textbf{vera} (si parla quindi di \textit{semantica},
riferendosi al significato).\\
Dato che si ha \textbf{completezza} e \textbf{correttezza} dell'apparato
deduttivo si ha hanno due situazioni.
\begin{itemize}
  \item \textit{ogni tripla \textbf{derivabile} è anche \textbf{vera} in
    qualsiasi interpretazione}
  \item \textit{ogni tripla \textbf{vera} vorremmo fosse anche
    \textbf{derivabile} e il discorso verrà approfondito in seguito per la
    logica di Hoare}
\end{itemize}
$\vdash$ può avere a pedice una sigla per la regola rappresentata, ad esempio
$\vdash_{ass}$ per l'assegnamento.
\end{shaded}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{y\geq 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  uso la \emph{regola di assegnamento} (che non ha premesse) partendo dalla
  postcondizione. Sostituisco e ottengo:
  \[\vdash_{ass}\{2\cdot y+1> 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  Procedo usando la \emph{regola di implicazione} (sapendo che $y\geq 0\implies
  2y+1>0$):
  \[\vdash_{impl}\{y\geq 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  Dimostrando quindi che la tripla è \textbf{vera}.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{z> 0\}\,\,\,x\cceq (y\cdot z)+1\,\,\,\{x>0\}\]
  e vediamo che la tripla non è valida in quanto $y$ potrebbe essere negativo e
  non portare alla positività di $x$. Formalmente cerchiamo un controesempio
  cercando di non soddisfare la postcondizione. Scelgo in memoria $z=1$ e
  $y=-2$. Abbiamo fatto quindi un ragionamento semantico. Provo a anche
  sintatticamente e giungo a:
  \[\vdash\{(y\cdot z+1)> 0\}\,\,\,x\cceq (y\cdot z)+1\,\,\,\{x>0\}\]
  che non è vero, quindi non posso proseguire.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{s=x^i\}\,\,\,i\cceq i+1,\, s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  anche se uguali precondizione e postcondizione fanno riferimento a due momenti
  della memoria diversi e quindi i valori saranno diversi.\\
  Abbiamo a che fare con un \textbf{invariante} in quanto la formula non varia
  tra precondizione e postcondizione anche se i valori saranno diversi (in mezzo
  al processo posso violare comunque l'invarianza).\\
  Ragioniamo in modo puramente sintattico. Dobbiamo applicare due volte
  l'assegnamento (e spesso serve dopo anche la regola di implicazione) e una
  volta la sequenza. Anche qui partiamo dalla postcondizione risalendo via via
  alle precondizioni:
  \[\vdash_{ass}\{sx=x^i\}\,\,\,s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  Ho creato quindi la condizione intermedia tra i due assegnamenti e proseguendo
  ho:
  \[\vdash_{ass}\{sx=x^{i+1}\}\,\,\,i\cceq i+1\,\,\,\{sx=x^i\}\]
  per transitività si può scrivere:
  \[\{sx=x^{i+1}\}\,\,\,i\cceq i+1,\, s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  ma non coincide con quanto voglio dimostrare. Ragiono quindi in modo
  algebrico. In $\{sx=x^{i+1}\}\,\,\,i\cceq i+1\,\,\,\{sx=x^i\}$ ho infatti:
  \[\{s\cancel{x}=x^{i+\cancel{1}}\}\to\{sx=x^i\},\mbox{ se }x\neq 0\]
  e quindi la formula iniziale è dimostrata.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{\top\}\,\,\,\mbox{if } x<0 \mbox{ then }y\cceq -2\cdot x \mbox{ else }
    y\cceq 2*x\mbox{ endif}\,\,\,\{y\geq 0\}\]
  Diciamo che $C$ rappresenta $y\cceq -2\cdot x$ e $D$ rappresenta $y\cceq 2*x$
  per praticità. Indichiamo la condizione booleana $x<0$ con $B$. Tutta la
  condizione di scelta la chiamiamo $S$
  Quindi avremo:
  \[\{\top\}\,\,\,\mbox{if } B \mbox{ then }C\mbox{ else }
    D \mbox{ endif}\,\,\,\{y\geq 0\}\]
  che in modo ancora più compatto sarebbe:
  \[\{\top\}\,\,\,S\,\,\,\{y\geq 0\}\]
  \textbf{Applico quindi la regola di derivazione al primo caso:}
  Ho quindi:
  \[\{\top \land x<0\}\,\,\,C\,\,\,{y\geq 0}\]
  che è uguale a:
  \[\{x<0\}\,\,\,C\,\,\,{y\geq 0}\]
  Procedo ora con l'assegnamento:
  \[\vdash_{ass}\{-2\cdot x\geq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  che però equivale algebricamente a:
  \[\vdash_{ass}\{x\leq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  ma siccome $x<0 \implies x\geq 0$ uso la regola dell'implicazione:
  \[\vdash_{impl}\{x< 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  \textbf{Passo al secondo caso:}
  \[\{\top \land x\geq 0\}\,\,\,D\,\,\,{y\geq 0}\]
  che è uguale a:
  \[\{x\geq 0\}\,\,\,D\,\,\,{y\geq 0}\]
  Procedo ora con l'assegnamento:
  \[\vdash_{ass}\{2\cdot x\geq 0\}\,\,\,y\cceq 2\cdot x\,\,\,\{y\geq 0\}\]
  che però equivale algebricamente a:
  \[\vdash_{ass}\{x\geq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  e quindi è dimostrabile che:
  \[\vdash \{\top\}\,\,\,S\,\,\,\{y\geq 0\}\]
\end{esempio}
\subsubsection{Iterazione}
% Per proseguire bisogna definire meglio il concetto di \textbf{invariante}.
\begin{definizione}
  Definiamo:
  \begin{itemize}
    \item \textbf{correttezza parziale}, dove la tripla viene letta supponendo a
    priori che l'esecuzione termini
    \item \textbf{correttezza totale}, dove la tripla viene letta dovendo anche
    dimostrare che l'esecuzione termini. Si procede quindi prima dimostrando la
    correttezza parziale aggiungendo poi la dimostrazione per l'esecuzione
    finita
  \end{itemize}
\end{definizione}
\begin{definizione}
  La sesta regola è quella relativa all'iterazione. Abbiamo quindi:
  \[\{p\}\mbox{ while \textit{B} do \textit{C} endwhile }\{q\}\]
  ma non posso determinare a priori quante volte si eseguirà $C$, che modifica
  lo stato della memoria. Per comodità $\mbox{ while \textit{B} do \textit{C}
    endwhile }$ la chiameremo $W$, quindi in modo compatto abbiamo:
  \[\{p\}\,\,\,W\,\,\,\{q\}\]
  Partiamo con la correttezza parziale supponendo a priori che l'esecuzione
  termini. Si ha quindi che in $q$ sicuramente $B$ è falsa, altrimenti non si
  avrebbe terminazione. Quindi in realtà abbiamo:
  \[\{p\}\,\,\,W\,\,\,\{q\land \neg B\}\]
  Ipotizziamo che all'inizio $B$ sia vera, quindi la precondizione sarà
  $\{i\land B\}$, con $i$ rappresentante una nuova formula. In questi stati
  eseguiremo $C$. Suppongo di poter derivare, tramite $C$ eseguito una sola
  volta, nuovamente $i$:
  \[\{i\land B\}\,\,\,C\,\,\,\{i\}\]
  Se vale questa tripla significa che $i$ è un \textbf{invariante} per $C$ e
  quindi $i$ viene chiamata \textbf{invariante di ciclo}. Nello stato raggiunto
  dopo $C$ la condizione $B$ può essere valida o meno. Qualora valga dopo la
  singola esecuzione di $C$ allora avrei ancora $\{i\land B\}$ e dovrei eseguire
  nuovamente $C$ e dopo varrà ancora $i$ sicuramente e bisogna ristudiare $B$
  per capire come procedere, in quanto $i$ resterà vera per qualsiasi numero di
  iterazioni di $C$. Si ha che $i$ resterà vera, teoricamente, anche una volta
  ``usciti'' dal ciclo. Qualora non valga $B$ si ha che:
  \[\{i\land \neg B\}\,\,\,W\,\,\,\{i\land \neg B\}\]
  (dove si nota che $i$ resta vera ma $B$ impedisce di tornare nel ciclo).\\
  Si ha quindi che:
  \[\frac{\{i\land B\}\,\,\,C\,\,\,\{i\}}{\{i\}\,\,\,W\,\,\,\{i\land \neg B\}}\]
  che è la \textbf{regola dell'iterazione}. Scritta in modo completo:
  \[\frac{\{i\land B\}\,\,\,C\,\,\,\{i\}}{\{i\}\mbox{ while \textit{B} do
        \textit{C} endwhile }\{i\land \neg B\}}\]
  Una nozione più forte di \textbf{invariante} può essere espressa dicendo che
  \[\{i\}\,\,\,C\,\,\,\{i\}\]
  che si differenzia da quella di \textbf{invariante di ciclo} (dove mi
  interessa sapere che un invariante sia vero prima del ciclo anche se questa
  non è un proprietà intrinseca degli invarianti):
  \[\{i\land B\}\,\,\,C\,\,\,\{i\}\]
  Ricordiamo che stiamo dando per scontata la \textbf{terminazione} tramite la
  \textbf{correttezza parziale}.\\
  Facciamo qualche osservazione:
  \begin{itemize}
    \item nella precondizione della conclusione non si ha $B$, in quanto il
    corpo dell'iterazione può anche non essere mai eseguito
    \item data un'istruzione iterativa posso avere più di un invariante. Si ha
    inoltre che ogni formula iterativa ha l'\textbf{invariante banale}
    $i=\top$. Possiamo avere anche una formula in cui compaiono variabili che
    non sono modificate della funzione iterativa e quindi l'intera formula è un
    \textit{invariante di ciclo}. Studiamo quindi gli invarianti ``più utili''
    \item nei casi pratici non consideriamo ovviamente iterazioni isolate ma
    iterazioni inserite in un programma. In questi casi quindi la scelta di un
    invariante adeguato dipende sia dall'iterazione che dall'intero contesto.
    \begin{esempio}
      Ho un programma su cui voglio dimostrare:
      \[\{p\}\mbox{ \textit{C; W; D} }\{q\}\]
      con $W$ iterazione e $C,D$ comandi.\\
      Spezziamo quindi il programma per fare la dimostrazione, tramite la regola
      della sequenza:
      \[\{p\}\mbox{ \textit{C} }\{r\}\mbox{ \textit{W} }\{z\}\mbox{ \textit{D}
        }\{q\}\]
      $r$ sarà quindi la precondizione dell'iterazione $W$ che porterà a $z$ che
      sarà precondizione di $D$.\\
      Sapendo che la regola di derivazione per $W$ è:
      \[\{i\}\mbox{ \textit{W} }\{i\land\neg B\]
      Confronto la tripla con la ``catena'' sopra espressa. Si nota che $r$
      dovrà implicare l'invariante per $W$.
    \end{esempio}
  \end{itemize}
\end{definizione}
\begin{esempio}
  Vediamo quindi un esempio completo.\\
  Si prenda il seguente programma:
  \begin{listing}[H]
    \begin{lstlisting}
      i ~ 0; s ~ 1;
      while i < N do
        i ~ i + 1;
        s ~ s * x;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:W}
  \end{listing}
  \textit{Per comodità chiamo $A$ i due assegnamenti iniziali,
      $W$ l'iterazione e $C$ il corpo dell'iterazione.}\\
  Ci proponiamo di derivare:
  \[\{N\geq 0\}\mbox{ \textit{P} }\{s\cceq x^N\}\]
  $x$ e $N$ sono variabili, nella realtà costanti non venendo mai modificate da
  $P$, e il loro valore fa parte dello stato della memoria.\\
  Concentriamoci in primis sull'iterazione cercando un invariante. Una strategia
  semplice è quella di simulare i primi passi di una esecuzione. Supponendo $N$
  comunque non nullo abbiamo, seguendo le due variabili $i$ e $s$ nelle varie
  iterazioni (procedendo verso destra nella tabella), procedendo in modo
  \textit{simbolico} per $s$ (usando quindi il simbolo $x$ direttamente):
  \begin{table}[H]
    \centering
    \begin{tabular}[H]{c|ccccc}
      i & 0 & 1 & 2 & 3 & $\ldots$\\
      s & 1 & $x$ & $x^2$ & $x^3$ & $\ldots$
    \end{tabular}
  \end{table}
  Quindi $s= x^i$ è il nostro invariante. Dimostro questa ipotesi è vera,
  dimostrando:
  \[\{s=x^i\land i<N\}\mbox{ \textit{C} }\{s=x^i\}\]
  sapendo che essa è la premessa alla regola di iterazione.\\
  Procedo quindi con la dimostrazione, avendo l'assegnamento:
  \[\vdash_{ass}\{sx=x^{i+1}\}\mbox{ \textit{C} }\{s=x^i\}\]
  infatti i due assegnamenti sono \emph{indipendenti}, permettendo di applicare
  in una sola volta due regole di assegnamento.\\
  Sapendo che $s\cancel{x}=x^{i+\cancel{i}}\implies s=x^i$. Ho quindi mostrato
  che:
  \[\vdash \{s=x^i\}\mbox{ \textit{C} }\{s=x^i\}\]
  dimostrando che ho effettivamente l'invariante $s= x^i$ (e nel dettaglio
  abbiamo trovato un \textbf{invariante forte}, che lo è a priori rispetto alla
  condizione booleana $B$).\\
  Vogliamo però ottenere come precondizione $\{s=x^i\land i<N\}$. Sapendo però
  che $s= x^i\land i<N$ è una regola ``più forte'' di $s= x^i$ (se è vera la
  prima sicuramente è vera anche la seconda) posso usare
  l'implicazione e dire che:
  \[\{s=x^i\land i<N\}\implies \{s= x^i\}\]
  quindi:
  \[\vdash_{impl} \{s=x^i\land i<N\}\mbox{ \textit{C} }\{s=x^i\}\]
  posso quindi applicare la regola dell'iterazione avendo la premessa corretta e
  ottenendo quindi:
  \[\vdash_{iter}\{s=x^i\}\mbox{ \textit{W} }\{s=x^i\land i\geq N\}\]
  ma la postcondizione non ci sta implicando $s=x^N$, per avere:
  \[\{N\geq 0\}\mbox{ \textit{P} }\{s\cceq x^N\}\]
  bisogna quindi \textbf{rafforzare} l'invariante, in modo che l'invariante
  implichi che alla fine dell'esecuzione $i=N$.\\
  Procediamo quindi con una nuova ipotesi, cercando di dimostrare che $i\leq N$
  è un invariante:
  \[\{i\leq N\land i<N\}\mbox{ \textit{C} }\{i\leq N\}\]
  Questa precondizione contiene una formula che è più restrittiva dell'altra.
  Procedo con l'assegnamento (CAPIRE QUESTA PARTE):
  \[\vdash_{ass}\{i+1\leq N\}\mbox{ \textit{C} }\{i\leq N\}\]
  ma sapendo che $i<N\implies i+1\leq N$ ottengo:
  \[\vdash_{impl}\{i< N\}\mbox{ \textit{C} }\{i\leq N\}\]
  Dimostrando quindi che è un invariante.\\
  Passo quindi all'iterazione:
  \[\vdash_{iter}\{i\leq N\}\mbox{ \textit{W} }\{i\leq N\land i\geq N\}\]
  e la postcondizione è uguale a dire che $i=N$.\\
  Considerando quindi i due invarianti ottengo:
  \[\vdash_{iter}\{s=x^i\land i\leq N\}\mbox{ \textit{W} }
    \{s=x^i\land i= N\}\]
  Dobbiamo però dimostrare anche la parte degli assegnamenti iniziali:
  \[\{N\geq 0\}\mbox{ \textit{A} }\{s=x^i\land i\leq N\}\]
  Bisogna infatti vedere se anche le condizioni iniziali permettono
  all'invariante doppio di restare tale.\\
  Applico quindi due volte l'assegnamento (partendo dal fondo); il primo con
  $s\cceq 1$: 
  \[\vdash_{ass}\{1=x^i\land i\leq N\}\mbox{ $s\cceq 1$ }\{s=x^i\land i\leq N\}\]
  Passo quindi a $i\cceq 0$ seguendo il nuovo ordine delle condizioni:
  \[\vdash_{ass}\{1=x^0\land 0\leq N\}\mbox{ $i\cceq 0$ }\{1=x^i\land i\leq N\}\]
  Applico quindi la sequenza con la prima parte (sapendo $1=x^0$ è sempre vero):
  \[\vdash_{seq}\{N\geq 0\}\mbox{ \textit{A} }\{s=x^i\land i\leq N\}\]
  completando la dimostrazioni.\\
  Da notare solo che $x$ deve essere non nullo.
\end{esempio}
\textbf{\textit{Altri esempi sono presenti sulla pagina del corso}}.\\
\subsection{Correttezza totale}
Analizziamo ora il caso in cui non si abbia certezza di terminazione di
un'iterazione:
\[\{p\}\mbox{ while \textit{B} do \textit{C} endwhile }\{q\}\]
Distinguiamo i due tipi di correttezza, dal punto di vista della derivabilità,
tramite: 
\[\vdash^{parz}\{p\}\mbox{ C }\{q\}\]
\[e\]
\[\vdash^{tot}\{p\}\mbox{ C }\{q\}\]
Dal punto di vista semantico non ha invece senso distinguere di due casi e
quindi si ha solo:
\[\vDash\{p\}\mbox{ C }\{q\}\]
In quanto dal punto di vista semantico o si ha terminazione o non si ha, non si
hanno casistiche differenti a seconda di correttezza totale o parziale.\\
Bisognerà quindi dimostrare la terminazione.\\
Studiamo il caso semplice dove:
\[W=\mbox{ while \textit{B} do \textit{C} endwhile }\]
Cerchiamo un'espressione aritmetica $E$, dove compaiono le variabili del
programma, costanti numeriche e operazioni aritmetiche. Cerchiamo anche un
\textbf{invariante di ciclo} $i$ (che quindi è una formula) per $W$. $E$ e $i$
devono soddisfare due condizioni:
\begin{enumerate}
  \item $i\implies E\geq 0$, quindi se vale $i$ allora l'espressione aritmetica
  ha valore $\geq 0$ (il valore di $E$ lo ottengo eseguendo le operazioni sulle
  costanti e sui valori, in quello stato della memoria, delle variabili)
  \item $\vdash^{tot}\{i\land B\land E=k\}\mbox{ C }\{i\land E<k\}$, dove
  nella precondizione abbiamo la congiunzione logica tra l'invariante $i$, la
  condizione di ciclo $B$ e tra $E=k$, avendo prima assegnato a $k$ il valore
  effettivo di $E$ nello stato in cui iniziamo a computare $C$. $k$ quindi non
  deve essere una variabile del programma e non deve apparire in $C$
  (rappresentante il corpo della singola iterazione). Se vale la
  condizione 1 e l'invariante sappiamo che $E\geq 0\implies k\geq 0$. La
  postcondizione è formata dall'invariante $i$ e dal fatto che il valore di $E$
  dopo l'esecuzione sia strettamente minore di $k$ (che è il valore di $E$ prima
  dell'esecuzione). In pratica $E$ decresce ad ogni singola iterazione, ovvero
  ad ogni esecuzione di $C$.\\
  La notazione $\vdash^{tot}$ serve a escludere che $C$ abbia altri cicli
  annidati (portando a dover dimostrare che anche i cicli interni
  terminano). Per praticità quindi supponiamo di non avere cicli interni (stiamo
  partendo dal ciclo più interno)
\end{enumerate}
In pratica, nella seconda condizione, uso $E$ per concludere che una singola
esecuzione dell'iterazione mi porta in uno stato in cui vale l'invariante, dove
può valere o meno $B$ (che potrebbe portare all'uscita dall'iterazione) ma in
cui $E$ ha un valore diverso, minore a quello di partenza ma mai minore di 0 per
la prima condizione (in quanto l'invariante è sempre valido). Quindi, ad un
certo punto, $E$ raggiungerà il valore minimo e in quel momento o $B$ è falsa o
si ha una contraddizione ($E$ dovrebbe diventare negativo), quindi si ha la
terminazione. \\
Quindi se abbiamo dimostrato le due condizioni posso dire:
\[\vdash^{tot}\{i\}\mbox{ W }\{i\land\neg B\}\]
che è anche la conclusione della regola di derivazione dell'iterazione in caso
di correttezza parziale.\\
Possiamo anche osservare due cose:
\begin{enumerate}
  \item $E$ non è una formula logica ma un'espressione aritmetica con valore
  numerico ($E\geq 0$ è una formula logica)
  \item qualora si abbia $E=0$ non si hanno problemi, $0$ è solo un esempio,
  potremmo riscrivere la prima condizione come $E\geq n$, con $n$ qualsiasi
  numero intero, basta che sia ben definito per poter permettere la ripetizione
  finita delle iterazioni
\end{enumerate}
Chiameremo questa espressione aritmetica è \textbf{variante}.\\
\textit{L'invariante della correttezza totale non è sempre quello di quella
  parziale, magari è uguale, magari diverso o anche uguale solo in parte}
\begin{esempio}
  Vediamo un esempio chiarificatore.\\
  Dato il programma (una sorta di conto alla rovescia):
  \begin{listing}[H]
    \begin{lstlisting}
      while x > 5 do
        x ~ x - 1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:t}
  \end{listing}
  studio la correttezza totale della tripla:
  \[\{x>5\}\mbox{ P }\{x=5\}\]
  Innanzitutto cerco un variante $E$, che decresce ad ogni singola iterazione è
  un invariante $i$ come descritti sopra, quindi che, qualora ci sia
  l'invariante $i$ allora  $E\geq 0$.\\
  Un primo invariante ``banale'' è $i=x\geq 5$.\\
  In merito ad $E$ notiamo che nel corpo dell'iterazione abbiamo un solo
  comando, dove il valore della variabile $x$ viene decrementato, quindi un
  primo candidato per $E$ potrebbe essere proprio $x$.\\
  Per comodità scegliamo però come variante $x-5$ per avere una
  corrispondenza diretta con l'invariante $x\geq 5$, essendo derivato dallo
  stesso invariante (basandoci in primis sulla prima condizione).\\
  \textbf{Non sempre posso ottenere una corrispondenza tra variante e
    invariante}.\\
  Presi questo invariante e questo variante verifichiamo le due condizioni:
  \begin{enumerate}
    \item $x\geq 5\implies x-5\geq 0$ è ovviamente corretto perché si ottiene
    che $x\geq 5\implies x \geq 5$ (infatti questo è il motivo per cui si è
    scelto $x-5$ come variante)
    \item $\vdash^{tot}\{x\geq 5\land x>5\land x-5=k\}\,\,\,x\cceq
    x-1\,\,\,\{x\geq 5\land x-5<k\}$\\
    Applico quindi la regola dell'assegnamento (che si applica anche per la
    correttezza totale) e ottengo la precondizione per la postcondizione
    $\{x\geq 5\land x-5<k\}$:
    \[\{x-1\geq 5\land x-1-5<k\}=\{x\geq 6\land x-6 < k\}\]
    che però è diversa da $\{x\geq 5\land x>5\land x-5=k\}$.\\
    Cerco quindi di applicare la regola dell'implicazione. Riguardando la
    precondizione originale noto che $x>5$ è ``più forte'' di $x\geq 5$ e quindi
    quest'ultimo può essere trascurato. Ricordando che stiamo lavorando su
    valori interi si ha che $x>5\implies x\geq 6$. Inoltre, sempre per il fatto
    che lavoriamo su numeri interi, \\
    $x-5=k\implies x-6<k$ e quindi:
    \[\{x\geq 5\land x>5\land x-5=k\}\implies \{x\geq 6\land x-6 < k\}\]
  \end{enumerate}
  Abbiamo quindi dimostrato che il programma termina e vale la tripla
  iniziale.\\
  Qualora avessimo dovuto dimostrare la correttezza parziale si sarebbe dovuto
  procedere riutilizzando lo stesso invariante e procedendo studiando la
  negazione di $B$, ovvero $x\leq 5$ (che insieme danno la postcondizione).
\end{esempio}
\begin{esempio}
  vediamo un esempio non si ha alcuna variabile che decrementa nel corpo
  dell'iterazione:
  \begin{listing}[H]
    \begin{lstlisting}
      while x < 5 do
        x ~ x + 1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:ti}
  \end{listing}
  studio la correttezza totale della tripla:
  \[\{x<5\}\mbox{ P }\{x=5\}\]
  Dal punto di vista della correttezza parziale ragiono come al solito mentre
  per la correttezza totale la situazione è un po' diversa.\\
  Ovviamente non posso usare $x$ come variante ma posso sicuramente usare, per
  esempio, $-x$, che sicuramente decresce visto che $x$ cresce, in entrambi i
  casi ad ogni iterazione. Riprendendo l'esempio sopra quindi posso usare $x\leq
  5$ come invariante e, al posto di $-x$ che comunque andrebbe bene, secondo una
  ragionamento simile allo scorso esempio, $5-x$ come variante (si nota che
  anche questo $E$ è ricavabile da $i$, anche se questo non è sempre attuabile).
  Il resto della dimostrazione è analoga all'esempio precedente.
\end{esempio}
\subsubsection{Correttezza e Completezza}
In generale quando si sviluppa una logica, con un apparato deduttivo e
un'interpretazione delle formule, siamo interessati a due proprietà generali
della logica e dell'apparato deduttivo:
\begin{enumerate}
  \item \textbf{correttezza}, ovvero il fatto che tutto quello che si può
  derivare con l'apparato deduttivo è effettivamente vero, ovvero
  $vdash\implies\vDash$. Quindi se una tripla è derivabile è anche vera
  \item \textbf{completezza}, ovvero il fatto che l'apparato deduttivo sia in
  grado di derivare tutte le formule vere (ovvero tutte le triple) vere. Si ha
  quindi $\vDash\implies\vdash$
\end{enumerate}
Per la logica di Hoare vale la correttezza ma vale una proprietà di completezza
è \textit{relativa} in quanto nel corso di una dimostrazione di completezza
occorre a volte usare deduzioni della logica proposizionale ma tali formule
parlano anche di operazioni aritmetiche. Si ha che l'aritmetica può essere
formalizzata attraverso un linguaggio logico con degli assiomi e delle regole di
inferenza. Si hanno però i \textbf{teoremi di incompletezza dell'aritmetica} di
G\"{o}del che dicono che l'aritmetica come teoria matematica è incompleta in
quanto ci sono formule scritte nel linguaggio dell'aritmetica che sono vere ma
non sono dimostrabili. Questa incompletezza si riverbera sulla logica di Hoare,
che comunque, dal punto di vista deduttivo sulle triple, è completa.
\subsection{Precondizione più debole}
Dato un comando $C$ e una formula $q$ che interpretiamo come post condizione di
$C$. Si cerca $p$ tale per cui:
\[\vdash\{p\}\mbox{ C } \{q\}\]
Sia valida e quindi vera.\\
Ovviamente il caso interessante è quello totale.
\begin{esempio}
  Suppongo di avere come comando un singolo assegnamento $y\cceq 2\cdot x-1$ e
  come postcondizione $\{y>x\}$.\\
  Cerco possibili precondizioni che, dopo l'assegnamento, portino a quella
  postcondizione. Ovviamente si avrebbero infiniti valori di $x$ che consentono
  di arrivare alla postcondizione (nonché altrettanti che non lo permettono). 
\end{esempio}
Si cerca quindi la ``migliore'' precondizione per arrivare ad una data
postcondizione ma per farlo ci serve un criterio di confronto. \\
Introduciamo quindi alcuni simboli e nozioni utili:
\begin{itemize}
  \item $V$ è l'insieme della variabili di $C$
  \item $\Sigma=\{\sigma|\,\sigma:V\to\mathbb{Z}\}$ è l'insieme degli stati
  della memoria (ricordando che posso avere solo valori interi nel nostro
  linguaggio)
  \item $\Pi$ è l'insieme di tutte le formule sull'insieme $V$
  \item $\sigma \vDash p$ significa che la formula $p$ è vera nello stato
  $\sigma$ e si ha che $\vDash\subseteq\Sigma\times \Pi$, con $\vDash$ che
  indica la veridicità di una formula in uno stato
  \item $t(\sigma)=\{p\in \Pi|\,\sigma\vDash p\}$ come la funzione che assegna
  ad uno stato l'insieme delle proposizioni, ovvero tutte le formule, che sono
  vere in $\sigma$
  \item $m(p)=\{\sigma\in \Sigma|\,\sigma\vDash p\}$ come la funzione che
  associa ad una formula l'insieme di tutti gli stati che soddisfano la formula
\end{itemize}
Tra le due funzioni finali si ha una sorta di ``dualità'' e agiscono in modo
speculare tra loro. Se, partendo da $\Sigma$, applico $t(\sigma)$ e scopriamo
che un certo stato $p$ appartiene a $\Pi$ e a tale $p$ associamo l'insieme dato
da $m(p)$ si avrà che $\sigma\in m(p)$.\\
Si possono fare altre osservazioni su queste due funzioni. \\
Prendiamo due formule $p$ e $q$ e cerco di capire se è possibile avere
$m(p)=m(q)$. La risposta è positiva e si parla, in caso, di \textit{equivalenza
  tra formule}. Fissati due stati $\sigma_1$ e $\sigma_2$ mi chiedo se possano
appartenere allo stesso insieme di formule. In questo caso la risposta è
negativa, in quanto sei i due stati sono distinti allora ci deve essere almeno
una variabile $x$ che ha valore diverso nei due stati ma allora è facile trovare
una formula che separa i due stati e tale formula potrà essere vera solo in uno
dei due stati.\\
Dato $S\subseteq \Sigma$ e $F\subseteq\Pi$ (ragiono quindi su sottoinsiemi) si
ha che:
\begin{itemize}
  \item $t(S)=\{p\in \Pi|\,\forall\,\sigma\in S,\,\,\,\sigma\vDash
  p\}=\bigcap_{\sigma\in S}t(\sigma)$ (ragiono quindi su tutti gli stati di $S$
  e quindi sulle formule che sono vere in tutti questi stati) 
  \item $m(F)=\{\sigma\in \Sigma|\,\forall\,p\in F\,\,\,\sigma\vDash
  p\}=\bigcap_{p\in \Pi}m(p)$ (ragiono quindi su tutte le formule di $\Pi$
  e quindi su tutti gli stati in devono essere soddisfatte tutte queste formule) 
\end{itemize}
Abbiamo quindi esteso il dominio delle due funzioni a sottoinsiemi di stati e
sottoinsiemi di formule.\\
Si può anche dimostrare che $S\subseteq m(t(s))$ e che $F\subseteq
t(m(F))$. Inoltre, dati $A\subseteq B$, si può dimostrare che $m(B)\subseteq
m(A)$ (se ho un insieme più grande di formule ho meno stati che le soddisfano
tutte).\\
Si ha un rapporto tra la logica proposizionale (coi suoi connettivi logici) e
l'insieme di stati ($p$ e $q$ sono formule di $\Pi$): 
\begin{itemize}
  \item $m(\neg p)=\Sigma \,\,\backslash \,\,m(p)$, quindi il complemento
  insiemistico di $p$
  \item $m(p\lor q)=m(p)\cup m(q)$, quindi all'unione dei due insiemi  
  \item $m(p\land q)=m(p)\cap m(q)$ (da verificare) quindi all'intersezione dei
  due insiemi
\end{itemize}
L'implicazione merita un discorso a parte in quanto ha due ``nature'':
\begin{itemize}
  \item \textbf{operatore logico}, che comporta che $p\implies q= \neg p\lor q$
  e in tal caso si ha, in linea a quanto detto per gli altri connettivi:
  \[m(p\implies q)= m(\neg p)\cup m(q)\]
  \item \textbf{relazione tra formule}, dove se $p$ implica $q$ (in tutti in
  casi in cui è vera $p$ è vera $q$) allora
  $m(p)\subseteq m(q)$, intendendo che $q$ è ``più debole'' (ovvero come formula
  ci da una conoscenza inferiore essendo $q$ corrispondente ad un insieme più
  grande di stati) di $p$. Si ha a che
  fare con una relazione algebrica tra le due formule. Più ``debole'' non
  significa comunque ``peggiore''
\end{itemize}
Posso quindi capire come definire la precondizione migliore per ottenere una
tripla valida, insieme al comando $C$ e alla postcondizione.\\
Un modo è quello di definire la migliore precondizione come la precondizione più
debole $p$ che presa come precondizione forma una tripla valida:
\[\vdash \{p\}\mbox{ C }\{q\}\]
se $p$ è la precondizione più debole allora corrisponde al più grande insieme di
stati tale che la tripla sia valida.\\
Questa è una scelta ragionevole perché è la precondizione che impone meno
vincoli sullo stato iniziale, infatti determina tutti gli stati iniziali che
garantiscono il raggiungimento della postcondizione. È sempre possibile
calcolare la precondizione più debole.\\
Indichiamo, fissati $C$ comando e $q$ formula di postcondizione, con $wp(C, q)$
la precondizione più debole (\textbf{weakest precondition (\textit{wp})}).
\begin{teorema}[proprietà fondamentale della condizione più debole]
  Si ha che $\vDash\{p\}\mbox{ C }\{q\}$ (quindi la tripla è vera) sse:
  \[p\implies wp(C,q)\]
\end{teorema}
\begin{teorema}
  L'\textbf{esistenza} della precondizione più debole è garantita.\\
  È garantita inoltre l'\textbf{unicità} della precondizione più debole (a meno
  di eventuali equivalenze logiche).
\end{teorema}
Vediamo quindi la \textit{regola di calcolo}.\\
Si hanno diversi casi a seconda dei tipi di comando:
\begin{itemize}
  \item \textbf{assegnamento:} in questo caso la precondizione più debole è
  quella determinata dalla regola di derivazione introdotta per la correttezza
  parziale. Quindi dato un assegnamento del tipo $x\cceq E$ e una postcondizione
  $q$ ho che la precondizione più debole si ottiene sostituendo in $q$ ogni
  occorrenza di $x$ con l'espressione $E$, ottenendo quindi:
  \[\{q[E/x]\}\]
  \item \textbf{sequenza:} in questo caso, se ho la sequenza di due comando
  $C_1$ e $C_2$ allora la precondizione più debole si calcola nel seguente modo:
  calcolo la precondizione più debole per $C_2$, ottenendo $wp(C_2,q)$, che sarà
  quindi la formula usata come postcondizione per calcolare la precondizione più
  debole di $C_1$, ovvero: $wp(C_1,wp(C_2,q))$. Questo non è altro che la
  condizione più debole della sequenza:
  \[wp((C_1;C_2),q)\equiv wp(C_1,wp(C_2,q))\]
  \item \textbf{scelta:} in questo caso, avendo:
  \[S=\mbox{ if \textit{B} then \textit{C} else \textit{D} endif }\]
  fisso la postcondizione $q$ e procedo come per la regola di
  derivazione. Separo i due casi in cui sia vera $B$ o meno. Se $B$ è vera devo
  eseguire $C$ quindi calcolo, eseguendo $C$ la precondizione più debole
  $wp(C,q)$. Qualora $B$ non sia vera calcolo, eseguendo $D$, la precondizione
  più debole $wp(D,q)$. Nel complesso quindi, facendo la disgiunzione dei due
  casi, ottengo: 
  \[wp(S,q)\equiv(B\land wp(C,q))\lor (\neg B\land wp(D,q))\]
  \item \textbf{iterazione:} in questo caso, avendo:
  \[W=\mbox{while \textit{B} do \textit{C} endwhile}\]
  È il caso più complicato.\\
  Si ha che se $\neg B$ è nello stato iniziale dll'iterazione allora il corpo
  non viene eseguito (arrivando direttamente alla postcondizione), invece se
  vale $B$ si avrà che $W$ equivale a $C;W$, in 
  quanto sicuramente almeno una volta eseguo $C$. Ma a questo punto potrei
  rifare lo stesso discorso se $B$ è ancora vera (questo fino a che non ho $\neg
  B$, uscendo dal ciclo e arrivando a $q$), ragionando su $W$ di $C;W$. Si
  arriva di fatto ad una regola ricorsiva:
  \[wp(W,q)\equiv(\neg B\land q)\lor(B\land wp((C;W),q))\]
  Applico quindi la regola della sequenza per la condizione più debole,
  arrivando a:
  \[wp(W,q)\equiv(\neg B\land q)\lor(B\land wp(C, wp(W,q)))\]
  Si nota che questa definizione ricorsiva ha un problema grave: non si ha un
  \textbf{caso base} che risolva la catena di chiamate ricorsive.\\
  Purtroppo non si può usare il ``trucco'' dell'invariante come nella regola di
  definizione e questo comporta che non si ha un vero e proprio algoritmo unico
  ed effettivo per questa regola di ricerca della precondizione più debole
\end{itemize}
Si hanno dei casi estremi, ad esempio:
\begin{itemize}
  \item precondizioni più deboli sempre false, ad esempio:
  \[wp(x\cceq5,x<0)\equiv\bot\]
  ovvero se assegno a $x$ il valore 5 non avrò mai $x$ negativo,
  ovvero ho un insieme vuoto $\emptyset$ di stati che garantiscono quanto detto
  \item precondizioni più deboli sempre vere, ad esempio
  \[wp(x\cceq 5,x\geq0)\equiv\top\]
  ovvero se assegno a $x$ il valore 5 si avrà che $x$ è sempre
  positivo, quindi è vero per qualunque stato iniziale
\end{itemize}
\subsubsection{Esempi vari}
\begin{esempio}
  Vediamo un primo esempio semplice con solo la sequenza di due assegnamenti:
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ x+1;
      y ~ y*x;
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Dove i due assegnamenti sono rispettivamente $A$ e $B$ e si vuole la
  postcondizione $q=x<y$. Cerco quindi $wp(P,q)$.\\
  Procediamo con la formula della sequenza (che ricordiamo procede ``a
  ritroso'').
  \begin{itemize}
    \item Parto dal secondo assegnamento, $B$, e calcolo $wp(B,q)$.\\
    Procedo con la regola e applico la sostituzione, ottenendo:
    \[r=wp(B,q)=x<y\cdot x\]
    quindi se $x<y\cdot x$ vale prima dell'esecuzione di $B$ allora sicuramente
    si raggiunge uno stato finale dove vale la postcondizione $q$ 
    \item lo stato in cui eseguiamo $B$, che abbiamo sopra chiamato $r$, è
    quello prodotto dall'esecuzione di $A$. Dobbiamo quindi calcolare
    $wp(A,r)$.\\
    Procedo quindi ancora con l'assegnamento, ottenendo:
    \[wp(A,r)\equiv x+1<y(x+1)\]
    che quindi, per composizione, è anche la precondizione più debole per $P$ e
    $q$. Bisogna però fare dei controlli.\\
    Uso quindi le regole delle disequazioni:
    \begin{itemize}
      \item caso 1: $x+1>0\implies y>1$
      \item caso 2: $x+1=0\implies 0<0$ e quindi non vale $q$
      \item caso 3: $x+1<0\implies y<0$
    \end{itemize}
    Costruisco quindi la precondizione più debole prendendo la disgiunzione
    logica dei due casi validi:
    \[wp(P,q)\equiv(x\geq 0\land y>1)\lor (x<-1\land y<1)\]
    (sempre ricordando che lavoriamo con interi)
  \end{itemize}
\end{esempio}
\begin{esempio}
   Vediamo un altro esempio:
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ x+a;
      y ~ y-1;
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
Dove i due assegnamenti sono rispettivamente $A$ e $B$ e si vuole la
postcondizione $q=(x=(b-y)\cdot a)$. Cerco quindi $wp(P,q)$.\\
Nella postcondizione ho una variabile $b$ esterna alla sequenza di interesse
fissato a priori.\\
Come prima ottengo:
\[r=wp(B,q)\equiv(x=(b-y+1)\cdot a)=(x=(b-y)\cdot a)\]
e quindi:
\[wp(A,r)\equiv(x+a=(b-y)\cdot a+a)\]
quindi $x=(b-y)\cdot a$ è un invariante ed è anche la postcondizione, Quindi $q$
è un invariante, accezione più forte del termine, per $P$.
\end{esempio}
\begin{esempio}
  Vediamo un altro esempio:
  \begin{listing}[H]
    \begin{lstlisting}
      if y = 0 then
        x ~ 0;
      else
        x ~ x * y;
      endif
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Dove i due comandi sono rispettivamente $C$ e $D$ e la condizione booleana è
  $B$. Come postcondizione voglio $q=(x=y)$.\\
  Per il caso della scelta bisogna calcolare i due casi e usare poi la
  disgiunzione tra essi.\\
  Calcolo quindi:
  \begin{enumerate}
    \item il caso in cui vale $B$: $wp(C,q)=(0=y)$, usando l'assegnamento
    \item il caso in cui vale $\neg B$: $wp(D,q)=(x\cdot y)=y$, usando
    assegnamento. La formula però comporta che ho due casi possibili: $y=0\lor
    x=1$ (il secondo appunto se ho $y\neq 0$)
  \end{enumerate}
  Applico quindi la regola della scelta, ottenendo:
  \[wp(P,q)\equiv (y=0\land y=0)\lor(y\neq 0\land (y=0\lor x=1))\]
  Semplificando si ha che:
  \[wp(P,q)\equiv(y=0)\lor(x=1\land y\neq 0)\]
\end{esempio}
\begin{esempio}
   Vediamo un altro esempio:
  \begin{listing}[H]
    \begin{lstlisting}
      while x > 0 do
        x ~ x - 1
      endwhile
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Con la condizione $B$ e il comando $C$, nel corpo dell'iterazione $W$. Come
  postcondizione si vuole $q=(x=0)$.\\
  Seguendo le modalità discusse in merito alle tecniche relative al calcolo
  della precondizione più debole per l'iterazione, vediamo che:
  \[(\neg B\land q)\equiv (x\leq 0\land x=0)\equiv (x=0)\]
  Per comodità chiamo $wp(W,q)$ $r$.\\ 
  Dobbiamo ora studiare $B\land wp(C,wp(W,q))$ che quindi per noi è, per l'alias
  appena definito, $B\land wp(C,r)$:
  \[B\land wp(C,r)\equiv x>0\land r[x-1/x]\]
  usando quindi l'assegnamento e quindi si ha che:
  \[B\land wp(C,r)\equiv (x>0)\land (x-1=0\lor (x-1>0\land r[x-2/x]))\]
  Siamo quindi in piena ricorsione.\\
  Allo stato attuale ho quindi (usando $ldots$ per non dover riscrivere tutto):
  \[(\neg B\land q)(B\land wp(C,r))\equiv (x=0 \lor(x>0\land(\ldots)))\]
  in realtà, andando avanti, si otterrebbe uno sviluppo regolare che porterebbe
  alla formula infinita:
  \[(x=0\lor x=1\lor x=2\lor \ldots)\equiv x\geq 0\]
  e quindi al precondizione più debole diventa:
  \[wp(W,q)=(x\geq 0)\]
  Si nota quindi come i casi di iterazione si risolvono solo con tecniche ``non
  rigorose'' \textit{ad hoc}.
\end{esempio}
\subsubsection{Estensioni del linguaggio}
Abbiamo, in conclusione, sviluppato una tecnica di studio basata su un
linguaggio di programmazione molto semplificato. Innanzitutto potremmo estendere
il linguaggio usato, aggiungendo:
\begin{itemize}
  \item il \textbf{do-while}:
  \[\mbox{do \textit{C} while \textit{B} endwhile}\]
  che nella realtà corrisponde a:
  \[\mbox{\textit{C}; while \textit{B} do
      \textnormal{C} endwhile}\]
  \item il \textbf{repeat-until}:
  \[\mbox{repeat \textit{C} until \textit{B} endrepeat}\]
  che nella realtà corrisponde a:
  \[\mbox{\textit{C}; while not \textit{B} do
      \textit{C} endwhile}\]
  \item il \textbf{ciclo for}:
  \[\mbox{for(\textit{D; B; F}) \textit{C} endfor} \]
  sempre convertendolo in un while (\textit{ipotizzo così}):
   \[\mbox{while \textit{B} do
      \textit{C; F}  endwhile}\]
  \item \textbf{procedure, metodi e funzioni}
  \item \textbf{array}, anche se solo in lettura se vogliamo applicare la logica
  di Hoare come l'abbiamo vista
\end{itemize}
Un altro limite è dato dal fatto che abbiamo usato solo il tipo intero, ma
possiamo potenzialmente aggiungere anche gli altri senza cambiare le basi della
logica introdotte.
\subsection{Logica di Hoare per sviluppare programmi}
Possiamo vedere le logiche di Hoare come \textbf{contratti} tra chi scrive il
programma e l'utente. In questo contesto l'utente commissiona il programma
specificando la postcondizione e chiede al programmatore di garantire che tale
postcondizione venga garantita al termine dell'esecuzione. Per garantire la
postcondizione $q$ deve essere vera la precondizione $p$.\\
Vediamo un esempio.
\begin{esempio}
  Ci proponiamo di calcolare la radice quadrata intera di un certo $k\geq 0$,
  approssimando per difetto.\\
  Alla fine del programma voglio il risultato nella variabile $x$, quindi
  voglio:
  \[\{k\geq 0\}\mbox{ P }\{0\leq x^2\leq k<(x+1)^2)\}\]
  con $(x+1)^2$ per la correttezza dell'approssimazione.\\
  Ci si propone di fare il calcolo per approssimazioni successive, partendo da
  un valore più piccolo di quello corretto.\\
  Possiamo spaccare $q$ in:
  \[0\leq x^2\leq k \,\,\,\mbox{ e }\,\,\,k<(x+1)^2\]
  la prima possiamo considerarla come \textit{invariante} (anche solo $x^2\leq
  k$).\\
  
  All'inizio possiamo pensare che $x$ dipenda da $k$ per una certa funzione $E$.
  Si ha quindi un prototipo del genere (con un certo $B$, condizione booleana, e
  un certo $F$, incremento nel corpo, dipendenti da $x$ e $k$):
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ E(x);
      while B(x,k) do
        x ~ F(x,k);
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Alla fine deve valere $x^2\leq k \land \neg B$ per la regola
  dell'iterazione.\\
  Come $B$ posso porre $(x+1)^2\leq k$ (ovvero la negazione della seconda parte
  della postcondizione, in modo da ottenere, con la negazione, $q$).\\ Si
  ottiene quindi: 
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ 0;
      while (x+1)^2 <= k do
        x ~ x+1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Bisognerebbe comunque dimostrare invariante e terminazione per validare il
  programma. 
\end{esempio}
Supponiamo un programma del tipo:
\[\{p\}\mbox{ A;W;C } \{q\}\]
che si risolve schematicamente con:
\[\{p\}\mbox{ A } \{inv\} \mbox{ W } \{inv\land \neg B\}\mbox{ C }\{q\}\]
Questo schema usa i cosiddetti \textbf{invarianti costruttivi}.\\
\subsection{Ultime considerazioni}
Vediamo qualche ultima considerazione sulla logica di Hoare.
\begin{itemize}
  \item alcune applicazioni pratiche della logica di Hoare:
  \begin{itemize}
    \item \textit{java modelling language}, un linguaggio di specificazione
    scritto in Java, che permette di fare \textit{design by contract} stabilendo
    delle precondizioni e delle postcondizioni 
    \item \textit{Eiffel, programming by contract}
    \item \textit{assert.h} in C
  \end{itemize}
  
  \item alcune applicazioni teoriche della logica di Hoare:
  \begin{itemize}
    \item \textit{semantica del programmi}, ovvero lo studio di cosa significa
    un programma, tramite:
    \begin{itemize}
      \item \textit{semantiche operazionali}, dove al programma
      viene associata una macchina astratta e, dato un programma viene associata
      una computazione sulla macchina astratta (come ad esempio la
      \textbf{Macchina di Turing} o le \textbf{Macchine a Registri})
      \item \textit{semantiche assiomatiche}, dove vediamo, mano a mano che
      viene eseguito il programma, le asserzioni che vengono verificate
      \item \textit{semantiche denotazionali}, in cui un programma è visto come
      un trasformatore da input e output. Come una $f:I\to O$. SI basa sul
      \textbf{lambda calcolo}
      \item \textit{semantiche operazionali strutturate}
    \end{itemize}
  \end{itemize}
\end{itemize}
Si hanno quindi i due punti chiave che devono essere garantiti:
\begin{enumerate}
  \item \textbf{terminazione} del programma
  \item \textbf{composizionalità} tra più comandi per ottenere un programma
\end{enumerate}
Le triple di Hoare vivono ancora nella \textbf{programmazione by contract}.
\chapter{Calculus of Communicating Systems}
Fino ad ora abbiamo considerato programmi sequenziali e abbiamo studiato la loro
verifica, tramite le triple di Hoare. Si passa ora dal sequenziale al
\textbf{concorrente}. Dati due comandi $s_1$ ed $s_2$ si ha che essi sono
specificati in esecuzione concorrente con:
\[s_1|s_2\]
L'esecuzione in concorrenza può portare a diverse complicanze qualora non venga
rispettato, per esempio, un certo ordine di esecuzione. Si ha quindi il
\textbf{non determinismo}, potendo avere più risultati a seconda dell'ordine di
esecuzione. Si perde la \textbf{composizionalità}.
\begin{esempio}
  Vediamo un esempio.\\
  Siano:
  \[s_1=\{x=V\}\,\,\,x=2\,\,\,\{x=2\}\]
  \[s_2=\{x=V\}\,\,\,x=3\,\,\,\{x=3\}\]
  con $V$ indicante un valore qualunque.\\
  Posso avere:
  \[\{x=V\}\,\,\,s_1|s_2\,\,\,\{x=2\lor x=3\}\]
  avendo non determinismo.\\
  Definiamo anche:
  \[s_1'=\{x=V_0\}\,\,\,x=1;x=x+1\,\,\,\{x=2\}\]
  e vediamo, a conferma della perdita di composizionalità e determinismo che:
  \[\{x=V\}\,\,\,s_1'|s_2\,\,\,\{x=2\lor x=3\lor x=4\}\]
\end{esempio}
Si studierà quindi la \textbf{semantica della programmazione
  concorrente/parallela (o dei sistemi distribuiti)}.\\
Il problema è stato studiato da Hoare e Milner (secondo punti di vista diversi)
alla fine degli anni '70. Hoare ha introdotto un nuovo paradigma di
programmazione, il paradigma \textbf{CSP (\textit{communicating sequential
    processes})}, il linguaggio macchina dei cosiddetti \textit{transputer}. Non
si ha più una memoria condivisa ma un insieme di processi ciascuno con una sua
memoria privata. Si ha un'interazione tra processi tramite lo scambio di
messaggi del tipo \textit{hand-shacking}, avendo quindi la sincronizzazione, con
lo scambio di informazioni. Viene fatto anche un processo particolare
rappresentante la \textbf{memoria condivisa}. Avremo quindi:
\[x|s_1|s_2\]
con $x$ che rappresenta la memoria condivisa dai due processi.\\
Ad oggi diversi linguaggi di programmazione adottano una ``soluzione mista''
rispetto a questo paradigma.\\
Milner propose in modo completamente indipendente da Hoare qualcosa di molto
analogo. Milner propose infatti il \textbf{lambda calcolo
  (\textit{$\lambda$-calcolo})} per passare dal sequenziale al
concorrente. Studia in modo approfondito la composizionalità, sfruttando la
composizione tra funzioni, cercando di non perderla nel concorrente. Introduce
quindi una sorta di \textit{$\lambda$-calcolo concorrente}, introducendo il
\textbf{Calculus of Communicating Systems (\textit{CCS})}, in cui pensa ad un
calcolo algebrico per sistemi comunicanti. Adotta anche lui un paradigma che
studia un sistema (non solo dal punto di vista della programmazione) formato da
componenti, chiamati \textit{processi}. Questi processi comunicano tramite lo
scambio sincrono di messaggi, con il modello \textit{hand-shacking}. Con $a$
senza nulla indiciamo un processo generico ($a$ potrebbe essere qualsiasi cosa)
che invia e con $\overline{a}$ un processo generico che riceve.
Un sistema quindi è un insieme di processi il cui comportamento è gestito da un
calcolo algebrico, si punta alle \textbf{algebre di processi}, ovvero
linguaggi di specifica di sistemi concorrenti che si ispirano al calcolo dei
sistemi comunicanti. I messaggi di scambio corrispondono ad uno scambio di
valori di variabili e questo è rappresentabile dall'algebra. I processi possono
interagire anche con l'\textbf{ambiente esterno}. Dato un sistema $S$, si
scrive:
\[S=p_1|p_2|p_3\]
se $S$ è formato dai processi $p_1$, $p_2$ e $p_3$, processi che sono
\textit{interagenti} ``a due a due''. Ogni processo ha comunque una memoria
privata.\\ 
Grazie alla comunicazione con l'\textit{ambiente esterno} non si ha più un
\textbf{sistema chiuso}. Pnueli (che creò anche le logiche temporali) e Harvel
hanno definito questi sistemi \textbf{sistemi reattivi}, per indicare che i
sistemi reagiscono all'ambiente esterno.\\
Milner risolve il problema della \textit{composizionalità} tramite l'uso di
diverse \textit{porte} che permettono ad un processo di comunicare con altri o
con l'ambiente esterno. Quindi ogni processo può essere visto come un insieme di
sotto-processi \textit{interagenti} che però interagiscono tramite
sincronizzazione con i processi esterni tramite una porta (un sottoprocesso può
comunicare con più processi esterni tramite più porte). Bisogna comunque
mantenere il comportamento complessivo. Per il processo esterno è come se
sostituissi il processo con cui comunica con il suo sottoprocesso. Si introduce
infatti l'\textbf{equivalenza all'osservazione}, che permette di sostituire un
processo $s_i$ con $s_i'$ se sono equivalenti rispetto
all'\textit{osservazione} ovvero sse un qualsiasi osservatore esterno non è in
grado di distinguere i due processi. In questo caso \textit{osservare} significa
\textbf{interagire con il sistema} dove agisce il processo (questa è un'idea di
``osservare'' derivante dalla fisica moderna). Questo deve essere valido
\textbf{per ogni possibile osservatore}. Se questo è garantito la sostituzione
di un processo non va ad inficiare l'esecuzione complessiva, senza incorrere in
\textit{deadlock} o altre problematiche.
\begin{figure}[H]
  \centering
  \psscalebox{1.0 1.0} % Change this value to rescale the drawing.
  {
    \begin{pspicture}(0,-1.7470312)(10.48,1.7470312)
      \psframe[linecolor=black, linewidth=0.04, dimen=outer]
      (4.8,1.1378711)(0.0,-1.6621289)
      \psframe[linecolor=black, linewidth=0.04, dimen=outer]
      (4.0,0.7378711)(3.2,-0.062128905)
      \psframe[linecolor=black, linewidth=0.04, dimen=outer]
      (1.6,0.7378711)(0.8,-0.062128905)
      \psframe[linecolor=black, linewidth=0.04, dimen=outer]
      (2.8,-0.4621289)(2.0,-1.262129)
      \psframe[linecolor=black, linewidth=0.04, dimen=outer]
      (9.6,0.7378711)(7.6,-1.262129)
      \psline[linecolor=black, linewidth=0.04](8.4,-0.4621289)
      (8.4,-0.8621289)(8.8,-0.8621289)(8.8,-0.4621289)(8.4,-0.4621289)
      \psline[linecolor=black, linewidth=0.04](8.0,0.3378711)
      (8.0,-0.062128905)(8.4,-0.062128905)(8.4,0.3378711)(8.0,0.3378711)
      \psline[linecolor=black, linewidth=0.04](8.8,0.3378711)
      (8.8,-0.062128905)(9.2,-0.062128905)(9.2,0.3378711)(8.8,0.3378711)
      \rput[bl](0.8,1.2378711){$S$}
      \rput[bl](1.05,0.2478711){$p_1$}
      \rput[bl](3.4,0.2378711){$p_2$}
      \rput[bl](2.2,-0.9621289){$p_3$}
      \rput[bl](7.7,0.8378711){$p_2$}
      \psline[linecolor=black, linewidth=0.04]
      (1.6,0.3378711)(3.2,0.3378711)(3.2,0.3378711)
      \psline[linecolor=black, linewidth=0.04]
      (3.6,-0.062128905)(2.8,-0.8621289)
      \psline[linecolor=black, linewidth=0.04]
      (4.0,0.3378711)(5.6,0.3378711)
      \rput[bl](2.4,0.4378711){$x$}
      \rput[bl](3.2,-0.8621289){$y$}
      \rput[bl](5.1,0.5378711){$z$}
      \psline[linecolor=black, linewidth=0.04]
      (8.64,-0.8621289)(8.64,-1.6621289)
      \psline[linecolor=black, linewidth=0.04]
      (8.41,0.097871095)(8.8,0.097871095)
      \psline[linecolor=black, linewidth=0.04]
      (8.96,-0.062128905)(8.64,-0.3821289)
      \psline[linecolor=black, linewidth=0.04]
      (8.16,-0.062128905)(8.48,-0.3821289)
      \psline[linecolor=black, linewidth=0.04]
      (8.0,0.097871095)(7.04,0.097871095)
      \rput[bl](7.2,0.2578711){$x$}
      \rput[bl](8.8,-1.6621289){$y$}
      \psline[linecolor=black, linewidth=0.04]
      (9.18,0.097871095)(10.08,0.097871095)
      \rput[bl](9.72,0.2178711){$z$}
    \end{pspicture}
  }
  \caption{Rappresentazione stilizzata di un sistema $S$ e del suo sottoprocesso
    $p_2$}
  \label{fig:proc}
\end{figure}
Vedremo quindi:
\begin{itemize}
  \item il calcolo ``puro'' di sistemi comunicanti CCS, definendone la semantica
  attraverso \textbf{Labeled Transition System, (\textit{LTS})} (\textit{sistemi
    di transizione etichettati}), (avendo come nodi i processi e archi
  etichettati dalle azioni). Useremo le operazioni ``+'', ``$\cdot$'' e la
  ricorsione, che verranno definite tramite LTS
  \item l'equivalenza all'osservazione e la \textbf{bisimulazione}. Vedremo
  anche una tecnica di verifica della \textit{bisimulazione}, tramite la
  \textbf{teoria dei giochi}
\end{itemize}
\begin{figure}
  \centering
  \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
    \node[state] (q_0) {$s$};
    \node[state] (q_1) [right=of q_0] {$s'$};
    \path[->]
    (q_0) edge  node {a} (q_1);
  \end{tikzpicture}
  \caption{Esempio semplice di LTS}
  \label{fig:l}
\end{figure}
\section{LTS}
\begin{shaded}
  Presa una relazione binaria $R$ su $X$, ovvero $R\subseteq X\times X$.\\
  Si ha che:
  \begin{itemize}
    \item una relazione è \textbf{riflessiva} sse $\forall x\in X$ ho che
    $(x,x)\in R$ o, scritto diversamente, $xRx$ ovvero ogni elemento è in
    relazione con se stesso 
    \item una relazione è \textbf{simmetrica} sse ho $(x,y)\in R$ allora
    $(y,x)\in R$, $\forall\,x,y\in X$
    \item una relazione è \textbf{transitiva} se ho $(x,y)\in R\land (y,z)\in R$
    allora ho $(x,z)\in R$
  \end{itemize}
  Se una relazione $R$ è simmetrica, riflessiva e transitiva allora è una
  \textbf{relazione di equivalenza} e quindi posso ripartire $X$ in classi di
  equivalenza:
  \[[x]=\{y\in X|\,\,(x,y)\in R\}\]
  Date due classi di equivalenza $[x_1]$ e $[x_2]$ si ha che o sono uguali
  (quindi tutti gli elementi di una sono nell'altra) oppure la loro intersezione
  è vuota.\\
  Inoltre si ha che:
  \[U_{x\in X}[x]=X\]
  Si è quindi diviso $X$ in sottoinsiemi disgiunti che coprono tutto $X$.\\
  Siano $R$, $R'$ e $R''$ relazioni binarie su $X$. Si ha che:\\
  $R'$ è la \textbf{chiusura riflessiva/simmetrica/transitiva} di
  $R$ sse: 
  \begin{itemize}
    \item $R'\subseteq R$
    \item $R'$ è riflessiva/simmetrica/transitiva
    \item $R'$ è la più piccola relazione che soddisfa i primi due puntu e
    quindi:
    \[\forall R''\mbox{ se }R\subseteq R''\land
      R''\mbox{riflessiva/simmetrica/transitiva}\]
    \[\mbox{allora } R'\subseteq R''\]
  \end{itemize}
\end{shaded}
I \textbf{Labeled Transition System (\textit{LTS})} sono molto usati per
rappresentare sistemi concorrenti. Questo modello ha origine dal modello degli
automi a stati finiti, che però sono usati come riconoscitori di linguaggi.\\
Negli LTS non si ha l'obbligo di avere un insieme finito di stati.
\begin{definizione}
  Definiamo un LTS come la quadrupla:
  \[(S, Act, T, s_0)\]
  dove:
  \begin{itemize}
    \item $S$ è un insieme, anche infinito, di stati
    \item $Act$ è un'insieme di nomi di \textit{azioni}
    \item $T=\{(s,a,s')\}$, con $s,s'\in S$ e $a\in Act$ oppure $T\subseteq
    S\times Act\times A$, è un insieme di triple rappresentanti le
    transizioni. Come scrittura si ha anche $(s,a,s')\in T$ oppure
    $s\stackrel{a}{\rightarrow} s'$ e quindi ho un arco etichettato con $a$ tra
    due nodi etichettati con $s$ e $s'$, come in figura \ref{fig:l}
    \item $s_0$ stato iniziale
  \end{itemize}
  La transizione $s\stackrel{a}{\rightarrow} s'$ può essere estesa a $w\in
  Act^*$ avendo più azioni sse:
  \begin{itemize}
    \item se $w=\varepsilon$ allora $s\equiv s'$
    \item se $w=x\cdot x,\,\,a\in Act,\,\,x\in Act^*$ sse:
    \[s\stackrel{a}{\rightarrow} s'' \mbox{ e } s''\stackrel{x}{\rightarrow}
      s'\]
  \end{itemize}
  Ho che $s\rightarrow s'$ sse $\exists a \in Act \mbox{ t.c. }
  s\stackrel{a}{\rightarrow} s'$.\\
  Quindi ho:
  \[\rightarrow =\cup_{a\in Act}\stackrel{a}{\rightarrow}\]
  Ho che $s\stackrel{*}{\rightarrow} s'$ sse $\exists w\in Act^*\mbox{ t.c. }
  s\stackrel{w}{\rightarrow} s'$. Si ha che:
  \[\stackrel{*}{\rightarrow}=\cup_{w\in Act^*}\stackrel{w}{\rightarrow}\]
  \[\stackrel{*}{\rightarrow}\subseteq S\times S\]
  La relazione $\stackrel{*}{\rightarrow}$ è la chiusura riflessiva e transitiva
  della relazione $\rightarrow$ (tale relazione non è simmetrica), avendo sempre
  $s\stackrel{*}{\rightarrow}s$ ed essendo garantita la transitività.
\end{definizione}
\section{CCS}
\begin{definizione}
  Per definire il \textbf{Calculus of Communicating Systems (\textit{CCS})}
  ``puro'' (astraendo l'aspetto delle strutture dati etc$\ldots$) dobbiamo
  definire che abbiamo: 
  \begin{itemize}
    \item $K$, ovvero un insieme di nomi di processi (\textit{buffer,
      sender,$ldots$}) che possono anche essere simboli di un alfabeto
    \item $A$, ovvero un insieme di nomi di azioni, che sono o azioni di
    sincronizzazione con l'ambiente o le componenti del sistema (anche interne
    al singolo componente)
    \item $\overline{A}$, ovvero l'insieme di nomi delle \textit{coazioni}
    contenute in $A$, $\forall\, a\in A\,\,\,\exists\,\, \overline{a}\in
    \overline{A}$, quindi:
    \[\overline{A}=\{\overline{A}|\,a\in A\}\]
    ovviamente si ha che:
    \[\overline{\overline{a}}=a\]
    \item $Act=A\cup \overline{A}\cup \{\tau\}$ dove $\tau\not\in A$ corrisponde
    all'azione di sincronizzazione tra $a$ e $\overline{a}$, ovvero la
    sincronizzazione è avvenuta. Le prime due sono \textit{azioni osservabili} e
    si indica con:
    \[\mathcal{L}=A\cup \overline{A}\]
    mentre $\tau$ non è osservabile.\\
    Ricordando che osservare un'azione significa poter interagire con essa.
  \end{itemize}
\end{definizione}
\begin{esempio}
  Vediamo un esempio \textbf{NON} chiarificatore.\\
  Prendiamo un sistema $S$, scuola, che eroga una lezione $\overline{lez}$ e poi
  continua ciclicamente ad erogare lezioni:
  \[S=\overline{lez}\cdot S\]
  Lo studente $ST$ segue la lezione e torna a studiare:
  \[ST=lez\cdot ST\] 
  se $S$ e $ST$ si sincronizzano ($S|ST$) lo studente osserva la lezione e il
  risultato non è più ossrvabile, essendo $\tau$.
\end{esempio}
\begin{definizione}
  I processi CCS sono di fatto operazioni CCS e un sistema CCS è definito da una
  collezione di processi $p\in K$ e si avranno:
  \[p=\mbox{espressione CCS}\]
  e si avrà solo un'equazione $\forall\,p\in K$
\end{definizione}

Definiamo meglio i processi CCS e, ad ogni processo, l'LTS assegnato, che sarà
del tipo:
\[(processi, Act, R, p_0)\]
e, tramite le regole di inferenza (con premesse, eventualmente con condizioni
e con conclusioni).\\
Dare un significato tramite LTS, regole di inferenza e sintassi è detto
\textbf{semantica operazionale strutturale}.\\
Un processo CCS può essere:
\begin{itemize}
  \item \textbf{Nil} o $0$ che ha un solo stato senza transizioni
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
      \node[state] (q_0) {$Nil$};
    \end{tikzpicture}
  \end{center}
  \item \textbf{prefisso}, in cui si ha $\alpha\cdot p,\,\alpha\in Act,\,p\in
  processi$, con la regola di inferenza:
  \[\frac{}{\alpha\cdot p\stackrel{\alpha}{\rightarrow}p}\]
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
      \node[state] (q_0) {$\alpha\cdot p$};
      \node[state] (q_1) [right=of q_0] {$p$};
      \path[->]
      (q_0) edge  node {$\alpha$} (q_1);
    \end{tikzpicture}
  \end{center}
  \item \textbf{Somma}, $p_1+p_2,\,\,p_1,p_2\in processi$, con la regola di
  inferenza (il $+$ è la ``scelta'', come nella regex), avendo $\alpha,\beta\in
  Act$ e $p_1',p_2'\in processi$:
  \[\frac{p_1\stackrel{\alpha}{\rightarrow}p_1'}{p_1+p_2
      \stackrel{\alpha}{\rightarrow}p_1'}\] 
  \[oppure\]
   \[\frac{p_2\stackrel{\beta}{\rightarrow}p_2'}{p_1+p_2
       \stackrel{\beta}{\rightarrow}p_2'}\]
   \begin{center}
      \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
      \node[state] (q_0) {$p_1+p_2$};
      \node[state] (q_1) [below right=of q_0] {$p_1'$};
      \node[state] (q_2) [below left=of q_0] {$p_2'$};
      \path[->]
      (q_0) edge  node {$\alpha$} (q_1)
      (q_0) edge  node [above left] {$\beta$} (q_2);
    \end{tikzpicture}
  \end{center}
  \item posso avere $\sum_{i\in I}p_i$ avendo \textbf{multiple somme di
    processi}: 
  \[\frac{p_j\stackrel{\alpha}{\rightarrow}p_j'}{\sum_{i\in
        I}p_i\stackrel{\alpha}{\rightarrow}p_j0},\,j\in J\]
  e quindi se $I=\emptyset$ avrò che $\sum_{i\in I}p_i=Nil$.\\
  Posso avere non determinismo avendo $p_1=a\cdot p_1'$ e $p_2=a\cdot p_2'$,
  avendo:
   \begin{center}
      \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
      \node[state] (q_0) {$p_1+p_2$};
      \node[state] (q_1) [below right=of q_0] {$p_1'$};
      \node[state] (q_2) [below left=of q_0] {$p_2'$};
      \path[->]
      (q_0) edge  node {$\alpha$} (q_1)
      (q_0) edge  node[above left] {$\alpha$} (q_2);
    \end{tikzpicture}
  \end{center}
  \begin{esempio}
    Vediamo un altro esempio.
    Dati:
    \begin{itemize}
      \item $p_1=\alpha\cdot p_1'$
      \item $p_2=\beta\cdot p_2'$
      \item $p_3=\gamma\cdot p_3'$
    \end{itemize}
    \[p_1+p_2+p_3=\alpha\cdot p_1'+\beta\cdot p_2'+\gamma\cdot p_3'\]
    avendo:
    \[\frac{p_1\stackrel{\alpha}{\rightarrow}p_1'}{p_1+p_2+p_3
        \stackrel{\alpha}{\rightarrow}p_1'}\]
    \[\frac{p_2\stackrel{\alpha}{\rightarrow}p_2'}{p_1+p_2+p_3
        \stackrel{\beta}{\rightarrow}p_2'}\]
    \[\frac{p_3\stackrel{\gamma}{\rightarrow}p_3'}{p_1+p_2+p_3
        \stackrel{\gamma}{\rightarrow}p_3'}\]
    \begin{center}
      \begin{tikzpicture}[shorten >=1pt,node distance=3cm,on grid,auto]
        \node[state] (q_0) {$p_1+p_2+p_3$};
        \node[state] (q_1) [below right=of q_0] {$p_1'$};
        \node[state] (q_3) [below left=of q_0] {$p_3'$};
        \node[state] (q_2) [below =of q_0] {$p_2'$};
        \path[->]
        (q_0) edge  node {$\alpha$} (q_1)
        (q_0) edge  node[above left]{$\gamma$} (q_3)
        (q_0) edge  node[above left] {$\beta$} (q_2);
      \end{tikzpicture}
    \end{center}
  \end{esempio}
  \item \textbf{composizione parallela}: $p_1|p_2$, avendo:
  \[\frac{p_1\stackrel{\alpha}{\rightarrow}p_1'}{p_1|p_2
      \stackrel{\alpha}{\rightarrow}p_1'|p_2}\]
  \[oppure\]
  \[\frac{p_2\stackrel{\alpha}{\rightarrow}p_2'}{p_1|p_2
      \stackrel{\alpha}{\rightarrow}p_1|p_2'}\]
  \[oppure\]
  \[\frac{p_1\stackrel{\alpha}{\rightarrow}p_1'\land p_2
      \stackrel{\overline{\alpha}}{\rightarrow}p_2'}{p_1|p_2
      \stackrel{\tau}{\rightarrow}p_1'|p_2'}\]
  e quindi, in quest'ultimo caso, non potremo più avere altre
  sincronizzazioni. \\
  Quindi avendo avendo $p_1=a\cdot p_1'$ e $p_2=a\cdot p_2'$ ho che $p_1|p_2$
  corrisponde a $a\cdot p_1'|\overline{a}\cdot p_2'$, e, l'LTS corrisponde a:
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=3cm,on grid,auto]
      \node[state] (q_0) {\small{$a\cdot p_1'|\overline{a}\cdot p_2'$}};
      \node[state] (q_1) [below right=of q_0] {\small{$a\cdot p_1'|p_2'$}};
      \node[state] (q_3) [below left=of q_0] {\small{$p_1'|\overline{a}\cdot
          p_2'$}};
      
      \node[state] (q_2) [below right =of q_3] {\small{$p_1'|p_2'$}};
      \path[->]
      (q_0) edge  node {$\overline{\alpha}$} (q_1)
      (q_1) edge  node {$\alpha$} (q_2)
      (q_3) edge  node [below left]{$\overline{\alpha}$} (q_2)
      (q_0) edge  node [above left]{$\alpha$} (q_3)
      (q_0) edge  node [above left] {$\tau$} (q_2);
    \end{tikzpicture}
  \end{center}
  \item \textbf{restrizione}, sia $L\subseteq A$. Si ha che:
  \[P_{\backslash L}\]
  significa che il processo $P$ non può interagire con il suo ambiente con
  azioni in $L\cup \overline{L}$ ma le azioni in $L\cup \overline{L}$ sono
  locali a $P$. Avendo quindi:
  \[\frac{p\stackrel{\alpha}{\rightarrow}p'}{P_{\backslash L}
      \stackrel{\alpha}{\rightarrow}p'_{\backslash L}}
    ,\,\alpha,\overline{\alpha}\not\in L,\,\,L\subseteq A\]
  \item \textbf{rietichettatura}, ovvero il cambiamento di nome ad una
  componente, ad una certa azione, per poter riusare tale nome. Ho quindi una
  funzione $f$ tale che:
  \[F:Act\to Act\]
  inoltre devo avere sempre garantito che:
  \begin{itemize}
    \item $f(\tau)=\tau$, ovvero le $\tau$ non cambiano
    \item $f(\overline{a})=\overline{f(a)}$ quindi un'azione soprassegnata può
    cambiare nome solo in un'altra soprassegnata
  \end{itemize}
  Ho quindi $p_{[f]}$ tale che:
  \[\frac{p\stackrel{\alpha}{\rightarrow}p'}{p_{[f]}
      \stackrel{f(\alpha)}{\rightarrow}p'_{[f]}}\]
  Ho quindi che:
  \[\frac{p\stackrel{\alpha}{\rightarrow}p'}{k
      \stackrel{\alpha}{\rightarrow}p'}\iff k=p\]
\end{itemize}
Quindi dato un CCS posso associare un LTS per quanto riguarda la semantica.\\
Si ha una \textbf{precedenza degli operatori}, da quello con meno precedenza a
quello con più precedenza: 
\begin{enumerate}
  \item restrizione
  \item rietichettatura
  \item prefisso
  \item composizione parallela
  \item somma
\end{enumerate}
\begin{esempio}
  Avendo:
  \[R+a\cdot p\,\,|\,\,b\cdot Q_{\backslash L}\]
  sarebbe:
  \[R+((a\cdot p)\,\,|\,\,b\cdot(Q_{\backslash L}))\]
\end{esempio}
\begin{esempio}
  Esercizio bonus:
  \[((a\cdot p_'+\overline{b}\cdot p_1'')|(\overline{a}\cdot p_2'+b\cdot
    p_2''))_{\{a\}}\]
  costruisco l'LTS associato ad $a$.
\end{esempio}
Analizziamo meglio la \textbf{composizione parallela}.\\
\begin{esempio}
  Si ha:
  \[a\cdot Nil\,\,|\,\,\overline{a}\cdot Nil\]
  ovvero:
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=4cm,on grid,auto]
      \node[state] (q_0) {\footnotesize{$a\cdot Nil\,\,|\,\,\overline{a}\cdot
          Nil$}}; 
      \node[state] (q_1) [below right=of q_0] {\footnotesize{$a\cdot
          Nil\,\,|\,\,Nil$}};  
      \node[state] (q_3) [below left=of q_0]
      {\footnotesize{$Nil\,\,\,\,|\,\,\overline{a}\cdot Nil$}}; 
      \node[state] (q_2) [below right =of q_3] {\footnotesize{$Nil|Nil$}};
      \path[->]
      (q_0) edge  node {$\overline{\alpha}$} (q_1)
      (q_1) edge  node {$\alpha$} (q_2)
      (q_3) edge  node [below left]{$\overline{\alpha}$} (q_2)
      (q_0) edge  node [above left]{$\alpha$} (q_3)
      (q_0) edge  node [above left] {$\tau$} (q_2);
    \end{tikzpicture}
  \end{center}
  Posso quindi eseguire le due operazioni o in una sequenza o nell'altra.\\
  Ho quindi una \textbf{simulazione sequenziale non deterministica} del
  comportamento del sistema dato dalla composizione parallela.
\end{esempio}
Quanto visto in questo esempio è possibile perché nell'ipotesi di Milner si ha
che $a$ e $\overline{a}$ sono \textbf{operazioni atomiche}.\\
Si hanno anche modelli di CCS modellati con:
\begin{itemize}
  \item reti di Petri
  \item strutture ad eventi
\end{itemize}
ovvero due modelli in cui si considera la semantica basata sulla \textbf{true
  concurrency}, a \textit{ordini parziali}, ovvero non si impongono sequenze
quindi due processi o si sincronizzano o vengono eseguiti in modo concorrente.
\begin{esempio}
  Costruisco il sistema di transizioni della specifica:
  \[S=\overline{lez}\cdot S\]
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
      \node[state] (q_0) {$S$};
      \path[->]
      (q_0) edge [loop right]  node {$\overline{lez}$} (q_0);
    \end{tikzpicture}
  \end{center}
  avendo:
  \[Uni=(M|LP)_{\{\backslash coin, caffe\}}\]
  Ho,seguendo nei vari step le regole di inferenza legate alla
  sincronizzazione:
  
  \[(coin\cdot \overline{caffe}\cdot
    M\,\,|\,\,\overline{lez}\cdot\overline{coin}\cdot caffe\cdot
    LP)_{\{\backslash coin, caffe\}}\]
  eseguo $\overline{lez}$ passo a:
  \[(coin\cdot \overline{caffe}\cdot
    M\,\,|\,\,\overline{coin}\cdot caffe\cdot
    LP)_{\{\backslash coin, caffe\}}\]
  sincronizzo con $coin$ e quindi passo con $\tau$:
  \[(\overline{caffe}\cdot
    M\,\,|\,\, caffe\cdot
    LP)_{\{\backslash coin, caffe\}}\]
  sincronizzo con $caffe$ e quindi passo con $\tau$:
   \[(coin\cdot \overline{caffe}\cdot
    M\,\,|\,\,\overline{lez}\cdot\overline{coin}\cdot caffe\cdot
    LP)_{\{\backslash coin, caffe\}}\]
  tornando all'inizio.\\
  (in realtà avrei i nodi per ogni formula e gli archi etichettati prima con
  $\overline{lez}$ e poi con $\tau$ per gli ultimi due)\\
  (\textbf{I due $\tau$ non possono avvenire ottemperantemente}).\\
  
\end{esempio}
\begin{esercizio}
  Costruire LTS relativo a:
  \[X=((a\cdot p_1'+\overline{b}\cdot p_1'')|(\overline{a}\cdot P_2'+b\cdot
    p_2''))_{\{a\}}\]
  (nessuno può quindi interagire con l'ambiente tramite $a$ e $\overline{a}$)
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=5cm,on grid,auto]
     \node[state] (q_0) {$X$}; 
      \node[state] (q_1) [left=of q_0] {$(p_1'|p_2')_{\{a\}}$};
      
      \node[state] (q_2) [below right=of q_0]
      {$(p_1''|(\overline{a}\cdot p_2'+b\cdot p_2''))_{\{a\}}$};
      \node[state] (q_3) [below left=of q_0]
      {$((a\cdot p_1'+\overline{b}\cdot p_1'')|p_2'')_{\{a\}}$};
      \node[state] (q_4) [below left=of q_2]
      {$(p_1''|p_2'')_{\{a\}}$};
      
      \path[->]
      (q_0) edge  node {$\overline{b}$} (q_2)
      (q_0) edge [above] node {$\tau$} (q_1)
      (q_2) edge [above left] node {$b$} (q_4)
      (q_3) edge  node {$\overline{b}$} (q_4)
      (q_0) edge  node {$\tau$} (q_4)
      (q_0) edge [above left] node {$b$} (q_3);
    \end{tikzpicture}
  \end{center}
\end{esercizio}
Posso infine chiedermi, vedendo l'esempio sopra, se $Uni$ implementa $S$, vista
come specifica. Quindi mi chiedo se posso quindi sostituire $S$ con $Uni$.\\
Innanzitutto per poter dire che una certa implementazione soddisfa (indicata con
$implementazione \vbdash specifica$) una certa
specifica o se due implementazioni diverse soddisfano la stessa specifica ci
serve una \textbf{relazione di equivalenza} tra processi CCS, ovvero una $R$:
\[R\subseteq P_{CCS}\times P_{CCS}\]
che sia:
\begin{itemize}
  \item riflessiva
  \item simmetrica
  \item transitiva
\end{itemize}
Bisognerà inoltre astrarre:
\begin{itemize}
  \item gli stati e considerare le azioni $Act$
  \item dalle sincronizzazioni interne, ovvero dalle $\tau$
  \item rispetto al non determinismo
\end{itemize}
Milner poi asserisce che $R$ deve essere inoltre una \textbf{congruenza}
rispetto agli operatori del CCS.\\
\begin{definizione}
  Una relazione di equivalenza $R$ è una \textbf{congruenza} sse:
  \[\forall \,\,p,q\in Proc_{CCS} \land \forall c[\cdot] \mbox{ contesto CCS}\]
  (avendo quindi un contesto CCS sostituibile con qualcosa)\\
  allora:
  \[\mbox{se }pRq\mbox{ allora si ha }c[p]\,R\,\,c[q]\]
\end{definizione}
\begin{esempio}
  Preso $Uni=(M\,\,|\,\,LP)_{\{caffe,coin\}}$ posso dire che prendo il contesto:
  \[(\cdot\,\,|\,\,LP)_{\{caffe,coin\}}\]
  se $M_1\,R\,M_2$ allora deve succedere che:
  \[(M_1\,\,|\,\,LP)_{\{caffe,coin\}}\,R\,\,(M_2\,\,|\,\,LP)_{\{caffe,coin\}}\]
  quindi posso sostituire l'uno con l'altro senza avere conseguenze, essendo
  equivalenti.
\end{esempio}
\begin{teorema}
  Dati due processi $p_1$ e $p_2$ a cui assegniamo i due LTS $v_1$ e $v_2$. I
  due processi sono equivalenti se $v_1$ e $v_2$ sono isomorfi. Possiamo in
  realtà cercare qualcosa di ``meno forte'' rispetto all'isomorfismo ma che
  garantisca lo stesso risultato.\\
  Si va a vedere quindi se i due LTS \textbf{ammettono le stesse sequenze di
    operazioni}, prendendo l'\textbf{equivalenza forte} tra automi a stati
  finiti. Questa è detta \textbf{equivalenza rispetto alle tracce} (che vale
  anche per due programmi, che sono equivalenti se implementano la stessa
  sequenza di istruzioni, lo stesso algoritmo). \textit{È comunque più forte di
    dire che hanno la stessa precondizione e postcondizione.}\\
  \textbf{Bisognerà discutere la cosa rispetto alla congruenza}.\\
   Preso $p\in Proc_{CCS}$ ho l'insieme delle traccie di $p$:
  \[tracce(p)=\{w\in Act^*|\,\exists\,p'\in Proc_{CCS}
    \,\,p\stackrel{w}{\rightarrow} p'\}\]
  \textit{Suppongo, per ora, di non considerare le sincronizzazioni (quindi
    senza $\tau$).} 
\end{teorema}
\begin{teorema}
  Preso $p'\in Proc_{CCS}$ ho che è equivalente rispetto alle tracce a $p''$,
  scritto:
  \[p'\stackrel{T}{\sim} p''\]
  sse:
  \[tracce(p')=tracce(p'')\]
\end{teorema}

\begin{esempio}
  Vediamo quindi un esempio.\\
  Prendo il sistema:
  \[(LP|M_i)\_{\{coin,caffe\}}\]
  \[LP=\overline{lez}\\cdot coin\cdot \overline{coffe}\cdto LP\]
  suppongo di avere due macchinette $M$ che erogano sia caffè che $tea$, per il
  primo basta una moneta e per il secondo due.\\
  La prima macchina è::
  \[M_1=\overline{coin}\cdot(caffe\cdot M_1+\overline{coin}\cdot tea\cdot M_1)\]
  la seconda:
  \[M_2=\overline{coin}\cdot(caffe\cdot M_2+\overline{coin}\cdot coin \cdot
    tes\cdot M_1)\]
  Mi chiedo se:
  \[M_1\stackrel{T}{\sim} M_2\]
  \newpage
  Parto da $M_1$:
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=5cm,on grid,auto]
      \node[state] (q_0) {$M_1$}; 
      \node[state] (q_1) [below=of q_0] {\footnotesize{$caffe\cdot
        M_1+\overline{coin}\cdot tea\cdot M_1$}};  
      \node[state] (q_2) [right=of q_1]
      {$tea\cdot M_1$}; 
      
      \path[->]
      (q_0) edge  node {$\overline{\coin}$} (q_1)
      (q_1) edge  node [above]{$\overline{coin}$} (q_2)
      (q_2) edge  node [above right] {$tes$} (q_0)
      (q_1) edge [bend left= 25] node [above left] {$caffe$} (q_0);
    \end{tikzpicture}
  \end{center}
  Passo ad $M_2$:
   \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=4cm,on grid,auto]
     \node[state] (q_0) {$M_2$}; 
      \node[state] (q_1) [below right=of q_0] {$\overline{coin}\cdot tea\cdot
        M_2$};
      
      \node[state] (q_2) [below left=of q_0]
      {$caffe\cdot M_2$};
      \node[state] (q_3) [above right=of q_0]
      {$tea\cdot M_2$}; 
      
      \path[->]
      (q_0) edge  node {$\overline{coin}$} (q_2)
      (q_0) edge  node {$\overline{coin}$} (q_1)
      (q_1) edge [right] node {$\overline{coin}$} (q_3)
      (q_3) edge  node {$tea$} (q_0)
      (q_2) edge [bend left= 25] node [above left] {$caffe$} (q_0);
    \end{tikzpicture}
  \end{center}
  Si vede quindi che le tracce di $M_1$ sono le stesse di quelle di $M_2$.
  Presi:
  \[(LP|M_1)\_{\{coin,caffe\}}\]
  \[(LP|M_2)\_{\{coin,caffe\}}\]
  Si nota che $M_2$ può andare in deadlock, avendo due processi che iniziano con
  $\overline{coin}$ (non posso sapere dove va e se vado sul nodo del tea la
  macchina si aspetta un'altra moneta, mentre magari si voleva il caffè). Quindi
  anche se le tracce sono le stesse ma non posso sostituire senza problemi le
  due macchinette, in qaunto se sostituisco $M_1$ con $M_2$ rischio di andare in
  deadlock. 
\end{esempio}
Lo studio delle tracce quindi non è più sufficiente nel caso di sistemi
concorrenti. Si necessità quindi di una nozione più restrittiva.\\
Per ovviare al problema sopra descritto si ha la \textbf{bisimulazione}. $M_1$ e
$M_2$ non sono equivalenti rispetto alla \textbf{bisimulazione} e quindi non
posso sostituire l'una con l'altra:
\[M_1\stackrel{Bis}{\not\sim}M_2\]
% aggiungere esercizio 1 quaderno
% se serve anche esercizio 2
\begin{esempio}
  Siano:
  \[P_1=a\cdot b\cdot Nil+a\cdot c\cdot Nil\]
  \[P_2=a\cdot(b\cdot Nil+c\cdot Nil)\]
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=4cm,on grid,auto]
      \node[state] (q_0) {$P_1$}; 
      \node[state] (q_1) [below right=of q_0] {$c\cdot Nil$};  
      \node[state] (q_3) [below left=of q_0]
      {$b\cdot Nil$}; 
      \node[state] (q_2) [below right =of q_3] {$Nil$};
      \path[->]
      (q_0) edge  node {$a$} (q_1)
      (q_1) edge  node {$c$} (q_2)
      (q_3) edge  node [below left]{$b$} (q_2)
      (q_0) edge  node [above left]{$a$} (q_3);
    \end{tikzpicture}
  \end{center}
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=4cm,on grid,auto]
      \node[state] (q_0) {$P_2$}; 
      \node[state] (q_1) [below=of q_0] {$b\cdot Nil+c\cdot Nil$};  
      \node[state] (q_2) [below=of q_1] {$Nil$}; 
      \path[->]
      (q_0) edge  node {$a$} (q_1)
      (q_1) edge [bend right = 25] node[below left] {$b$} (q_2)
      (q_1) edge [bend left = 25] node {$c$} (q_2);
    \end{tikzpicture}
  \end{center}
  Avendo quindi:
  \[T(P_1)=\{\varepsilon,a,ab,ac\}\]
  \[T(P_2)=\{\varepsilon,a,ab,ac\}\]
  quindi:
  \[P_1\sim^T P_2\]
  Ma se ad esempio prendo:
  \[Q_1=(P_1|\overline{a}\cdor \overline{b}\cdot Nil )_{\{a,b,c\}}\]
  \[Q_2=(P_2|\overline{a}\cdor \overline{b}\cdot Nil )_{\{a,b,c\}}\]
  \newpage
  Ho:
   \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=4cm,on grid,auto] 
      \node[state] (q_0) {$Q_1$};  
      \node[state] (q_1) [below right=of q_0] {$(b\cdot Nil+\overline{c} \cdot
        Nil)_{\{a,b,c\}}$};
      
      \node[state] (q_3) [below left=of q_0]
      {$(c\cdot Nil+\overline{c} \cdot Nil)_{\{a,b,c\}}$}; 
      \node[state] (q_2) [below =of q_3] {$Nil|Nil$};
       \node[state] (q_4) [below =of q_1] {$deadlock$};
      \path[->]
      (q_0) edge  node {$\tau$} (q_1)
      (q_0) edge  node [above left] {$\tau$} (q_3)
      (q_3) edge  node {$\tau$} (q_2)
      (q_1) edge  node {$\tau$} (q_4)
      ;
    \end{tikzpicture}
  \end{center}
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=4cm,on grid,auto]
      \node[state] (q_0) {$Q_2$}; 
      \node[state] (q_1) [below=of q_0] {$(b\cdot Nil+c\cdot Nil|c\cdot Nil
        )_{\{a,b,c\}}$}; 
      
      \node[state] (q_2) [below=of q_1] {$Nil|Nil$}; 
      \path[->]
      (q_0) edge  node {$\tau$} (q_1)
      (q_1) edge  node {$\tau$} (q_2);
    \end{tikzpicture}
  \end{center}
  Quindi il primo va in deadlock e il secondo no, quindi vedremo non sono
  equivalenti per la bisimulazione.
  \label{bi}
\end{esempio}

\begin{definizione}
  data una relazione binaria $R\subseteq Proc_{CCS}\times Proc_{CCS}$ è una
  relazione di \textbf{bisimulazione (forte)} sse:
  \[\forall\,p,q\in Proc_{CCS}:\,\,\, pRq \mbox{ vale che }\]
  \begin{itemize}
    \item $\forall \alpha\in Act=A\cup \overline{A}\cdot \tau$ se ho
    $p\stackrel{\alpha}{\rightarrow}p'$ allora deve esistere un \[\exists
    q'\mbox{ t.c }q\stackrel{\alpha}{\rightarrow}q'\mbox{ e si ha }p'Rq'\]
  \item  $\forall \alpha\in Act=A\cup \overline{A}\cdot \tau$ se ho
    $q\stackrel{\alpha}{\rightarrow}q'$ allora deve esistere un \[\exists
    p'\mbox{ t.c }p\stackrel{\alpha}{\rightarrow}p'\mbox{ e si ha }p'Rq'\]
\end{itemize}
Due processi $p$ e $q$ sono fortemente bisimili:
\[p\sim^{Bis}q\]
sse $\exists\,\,R\subseteq Proc_{CCS}\times Proc_{CCS}$, relazione di
bisimulazione forte tale che $pRq$.\\
SI ha che:
\[\sim^{Bis}=\cup\{R\subseteq Proc_{CCS}\times Proc_{CCS}|\,\,R \mbox{ è una
    relazione di bisimulazione forte}\}\]
\end{definizione}
\begin{teorema}
  Se prendo $\sim^{Bis}\subseteq Proc_{CCS}\times Proc_{CCS}$ si dimostra che è:
  \begin{itemize}
    \item riflessiva
    \item simmetrica
    \item transitiva
  \end{itemize}
  e quindi è una \textbf{relazione di equivalenza}.\\
  Quindi:
  \[p\sim^{Bis}q\iff \forall\alpha\in Act \mbox{ se }
    p\stackrel{\alpha}{\rightarrow}p'\]
  \[\mbox{ allora }\]
  \[\exists q': q\stackrel{\alpha}{\rightarrow}q'\land p'\sim^{Bis}q'\]
  \[\mbox{ e se }\]
  \[q\stackrel{\alpha}{\rightarrow}q' \]
  \[\mbox{ allora }\]
  \[\exists p': p\stackrel{\alpha}{\rightarrow}p'\land p'\sim^{Bis}q'\]
  
\end{teorema}
\begin{teorema}
  Se due processi sono fortemente bisimili allora sono sicuramente equivalenti
  rispetto alle tracce. \\
  \textbf{NON VALE IL VICEVERSA} (come visto negli esempi sopra).
\end{teorema}
Per vedere che due processi sono bisimili devo quindi, per ogni esecuzione,
ottenere due processi ancora bisimili (potendo quindi fare le azioni
corrispondenti da entrambe le parti).
\begin{esempio}
  esempio \ref{bi} ad esempio dopo $a$ arrivo in $c\cdot Nil$ da una parte e
  $b\cdot Nil+ c\cdot Nil$ ma banalmente dal secondo posso eseguire sia $b$ che
  $c$, cosa che non posso fare nel primo, che può eseguire solo $c$.
\end{esempio}
\begin{esempio}
  Siano:
  \[P_1=a\cdot b\cdot Nil+a\cdot P_1'\]
  \[P_1'=b\cdot P_1'\]
  \[Q_1=a\cdot Q_1'\]
  \[Q_1'=b\cdot Q_1'\]
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto] 
      \node[state] (q_0) {$P_1$};  
      \node[state] (q_1) [below left=of q_0] {$b\cdot P_1'$};
      
      \node[state] (q_2) [below right=of q_0]
      {$P_1'$}; 
      \path[->]
      (q_0) edge   [above left]node {$a$} (q_1)
      (q_0) edge node {$a$} (q_2)
      (q_1) edge  node {$b$} (q_2)
      (q_2) edge [loop right]  node {$b$} (q_2)
      ;
    \end{tikzpicture}
  \end{center}
  \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto] 
      \node[state] (q_0) {$Q_1$};  
      \node[state] (q_2) [below =of q_0]
      {$Q_1'$}; 
      \path[->]

      (q_0) edge  node {$a$} (q_2)
      (q_2) edge [loop right]  node {$b$} (q_2)
      ;
    \end{tikzpicture}
  \end{center}
  Quindi ho:
  \[P_1\sim^{Bis}Q_1\]
  Vedendo che i passi che posso fare nell'uno li posso fare anche
  nell'altro.
\end{esempio}
\begin{esempio}
  Siano:
  \[P_1=a\cdot b\cdot Nil\]
  \[P_2=a\cdot \cdot \tau b\cdot Nil\]
   \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto] 
      \node[state] (q_0) {$P_1$};  
      \node[state] (q_1) [below =of q_0] {$b\cdot Nil$};
      
      \node[state] (q_2) [below =of q_1]
      {$Nil$}; 
      \path[->]
      (q_0) edge node {$a$} (q_1)
      (q_1) edge  node {$b$} (q_2)
      ;
    \end{tikzpicture}
  \end{center}
   \begin{center}
    \begin{tikzpicture}[shorten >=1pt,node distance=2.5cm,on grid,auto] 
      \node[state] (q_0) {$P_2$};  
      \node[state] (q_1) [below =of q_0] {$\tau\cdot b\cdot Nil$};
       \node[state] (q_2) [below =of q_1] {$b\cdot Nil$};
      \node[state] (q_3) [below =of q_2]
      {$Nil$}; 
      \path[->]
      (q_0) edge node {$a$} (q_1)
      (q_1) edge  node {$\tau$} (q_2)
      (q_2) edge  node {$b$} (q_3)
      ;
    \end{tikzpicture}
  \end{center}
  Ho che:
  \[P_1\not\sim^T P_2\]
  e che:
  \[P_1\not\sim^{Bis} P_2\]
  Ma si vede che i comportamenti sono simili.
\end{esempio}
Cerco quindi di astrarre dalle interazioni interne ($\tau$), introducendo
l'\textbf{equivalenza debole}, volendo astrarre rispetto alle azioni $\tau$,
introducendo la \textbf{bisimulazione debole} (e nell'esempio precedente i due
processi sono debolmente equivalenti sia per le tracce che per la
bisimulazione).

\end{document}
% LocalWords:  Machine Learning dell multicore monocore checking mutex thread
% LocalWords:  race condition graph sottoprocessi Petri Morgan int const while
% LocalWords:  if return ht primis postcondizione Hoare then endif for endwhile
% LocalWords:  endfor program counter skip and not cccccc ccccc derivabilità of
% LocalWords:  weakest precondition sse step repeat until endrepeat array java
% LocalWords:  modelling contract programming Eiffel assert language Calculus
% LocalWords:  postcondizioni denotazionali Calculus of Communicating Systems
% LocalWords:  Communicating Systems CCS composizionalità Milner communicating
% LocalWords:  sequential CSP transputer hand shacking process processes System
% LocalWords:  Pnueli Harvel sottoprocesso all deadlock Labeled Transition LTS
% LocalWords:  bisimulazione sender ossrvabile  Nil rietichettatura concurrency
% LocalWords:  rinominazione true LocalWords bisimili
