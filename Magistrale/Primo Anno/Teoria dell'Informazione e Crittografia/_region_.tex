\message{ !name(critto.tex)}\documentclass[a4paper,12pt, oneside]{book}

% \usepackage{fullpage}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage[safe,extra]{tipa}
%\usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{fontspec}
\usepackage{enumerate}
\usepackage{physics}
\usepackage{braket}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage[cache=false]{minted}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}

\usepackage{tikz}\usetikzlibrary{er}\tikzset{multi  attribute /.style={attribute
    ,double  distance =1.5pt}}\tikzset{derived  attribute /.style={attribute
    ,dashed}}\tikzset{total /.style={double  distance =1.5pt}}\tikzset{every
  entity /.style={draw=orange , fill=orange!20}}\tikzset{every  attribute
  /.style={draw=MediumPurple1, fill=MediumPurple1!20}}\tikzset{every
  relationship /.style={draw=Chartreuse2,
    fill=Chartreuse2!20}}\newcommand{\key}[1]{\underline{#1}}
  \usetikzlibrary{arrows.meta}
  \usetikzlibrary{decorations.markings}
  \usetikzlibrary{arrows,shapes,backgrounds,petri}
\tikzset{
  place/.style={
        circle,
        thick,
        draw=black,
        minimum size=6mm,
    },
  transition/.style={
    rectangle,
    thick,
    fill=black,
    minimum width=8mm,
    inner ysep=2pt
  },
  transitionv/.style={
    rectangle,
    thick,
    fill=black,
    minimum height=8mm,
    inner xsep=2pt
    }
  } 
\usetikzlibrary{automata,positioning,chains,fit,shapes}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[space]{grffile} % For spaces in paths
\usepackage{etoolbox} % For spaces in paths
\makeatletter % For spaces in paths
\patchcmd\Gread@eps{\@inputcheck#1 }{\@inputcheck"#1"\relax}{}{}
\makeatother

\title{Teoria dell'Informazione e Crittografia}
\author{UniShare\\\\Davide Cozzi\\\href{https://t.me/dlcgold}{@dlcgold}}
\date{}

\pgfplotsset{compat=1.13}
\begin{document}

\message{ !name(critto.tex) !offset(-3) }

\maketitle

\definecolor{shadecolor}{gray}{0.80}
\setlist{leftmargin = 2cm}
\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}
\tableofcontents
\renewcommand{\chaptermark}[1]{%
  \markboth{\chaptername
    \ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%
\chapter{Introduzione}
\textbf{Questi appunti sono presi a lezione. Per quanto sia stata fatta
  una revisione è altamente probabile (praticamente certo) che possano
  contenere errori, sia di stampa che di vero e proprio contenuto. Per
  eventuali proposte di correzione effettuare una pull request. Link: }
\url{https://github.com/dlcgold/Appunti}.\\
\chapter{Introduzione agli argomenti del corso}
Si ha una sorgente che emette messaggi e li vuole mandare tramite un canale di
comunicazione, che contiene del rumore che agisce sui messaggi e li rovina. Il
destinatario per capire che il messaggio è rovinato ha varie tecniche. Una
prima cosa che potrebbe fare è chiedere di rimandare il messaggio ma sarebbe
meglio correggere \textit{in loco} e si hanno algoritmi per farlo (tra cui lo
\textbf{schema di Hamming} usando il modello del \textbf{rumore bianco}).\\
Un'altra tematica è la codifica stessa del sorgente, comprimendo flussi di dati,
senza perdere informazioni.\\
Si parlerà anche dei canali di comunicazione, delle capacità e dei
\textbf{teoremi di Shannon}. \\
Si vedranno poi le basi della crittografia, i crittosistemi storici, vari
standard etc$\ldots$
\chapter{Teoria dell'informazione}
Si hanno due sottoparti principali:
\begin{itemize}
  \item \textbf{teoria dei codici}, per individuare e correggere gli errori. Si
  studia il canale di trasmissione cercando di contrastare il rumore che c'è nel
  canale
  \item \textbf{teoria dell'informazione}, in cui il focus è la sorgente delle
  informazioni
\end{itemize}
Vediamo il classico schemino della teoria dell'informazione:
\begin{figure}[H]
  \centering
  \includegraphics[width = \textwidth]{img/teo.pdf}
  \caption{Schema di un sistema generale di comunicazione tipico della teoria
    dell'informazione} 
  \label{fig:teo}
\end{figure}
Avendo:
\begin{itemize}
  \item la \textbf{sorgente} che produce segnali, dei simboli, che potrebbero
  essere continui (come la corrente), anche se noi li assumeremo come simboli di
  un alfabeto finito, avendo quindi una \textbf{sorgente discreta}
  \item i simboli, per poter essere spediti all'interno di un canale, vanno
  codificati, avendo una parte di \textbf{codifica}
  \item una volta codificati i simboli vanno nel \textbf{canale di
    trasmissione}, dove si ha del \textbf{rumore}. Tale \textit{rumore} prende
  un simbolo di quelli inseriti e lo cambia
  \item dal canale esce o il simbolo che è entrato o il simbolo modificato dal
  rumore e, tipicamente, non è immediatamente utilizzabile ma deve passare per
  una fase di \textbf{decodifica}
  \item il simbolo decodificato arriva al \textbf{destinatario}
\end{itemize}
Si hanno
alcune assunzioni sulla sorgente:
\begin{itemize}
  \item è \textbf{discreta}, i simboli emessi appartengono ad un alfabeto
  finito. Normalmente tali simboli sono $S=\{s_1, s_2,\ldots,s_q\}$
  \item i simboli vengono emessi uno alla volta ad ogni \textbf{colpo di
    clock}. Non si ha mai che in un colpo di clock non escano simboli o che ne
  escano più di uno solo 
  \item la sorgente è \textbf{senza memoria (\textit{memoryless})}, avendo che
  i simboli già usciti non influenzano per nulla il simbolo che sta per
  uscire. Ogni simbolo che esce non tiene conto del passato, \textit{è come se
    fosse il primo}
  \item è \textbf{probabilistica e randomizzata}. Si ha quindi che i simboli
  $S=\{s_1, s_2,\ldots,s_q\}$ escono con le probabilità
  $(p_1,p_2,\ldots,p_q)$. Deve valere che, ovviamente, che $p_i\in
  [0,1],\forall
  i=1,\ldots,q$. Si ha inoltre che le varie probabilità, nel loro insieme,
  devono formare una distribuzione di probabilità, avendo che:
  \[\sum_{i=1}^q p_i=1\]
  Potrei avere simboli con probabilità nulla di comparire ma nella pratica non
  è qualcosa di sensato. La sorgente la costruisco o da zero (e a quel punto
  un simbolo con probabilità nulla non lo metterei) o ho una sorgente che devo
  studiare (e qui potrei avere simboli con probabilità bassissime se non
  nulle, in tal caso bisognerebbe rivalutare l'assunzione dei simboli di
  quella sorgente). Possiamo quindi meglio dire che $p_i\in (0,1],\forall
  i=1,\ldots,q$.\\
  D'altro canto vedo se posso avere probabilità pari a 1 per un simbolo ma in
  tal caso avrei solo quello e non sarebbe interessante. Si ha quindi che:
  \[p_i\in (0,1),\forall i=1,\ldots,q\]
  \[\sum_{i=1}^q p_i=1\]
  \textit{Le sorgenti che emettono i simboli secondo  
    uno schema prefissato, deterministico, sono poco interessanti, avendo un
    comportamento banale}
\end{itemize}
Il concetto di \textit{spedire in un canale} può anche essere generalizzato in
altre ``idee'', come il disco su cui salvo dei dati e il tempo per cui li
salvo.\\ 
La parte di \textit{codifica e decodifica} può essere approfondita. Nello schema
in figura \ref{fig:teo} ci si è infatti concentrati sul canale, avendo che la
codifica serve a fare in modo che il simbolo trasmesso vada bene per essere
trasmesso nel canale. La \textbf{codifica} è a sua volta suddivisa in due parti:
\begin{enumerate}
  \item \textbf{codifica di sorgente}, che ha come obiettivo rappresentare nel
  modo più efficiente e compatto i simboli emessi dalla \textit{sorgente}. Si
  vuole quindi comprimere la sequenza di simboli (messaggi) emessi dalla
  sorgente, per impegnare meno banda possibile quando andremo a spedire. Si deve
  considerare che ogni bit in un file 
  compresso è essenziale per permettere di poter recuperare il contenuto
  compresso 
  \item \textbf{codifica di canale}, che ha quasi uno scopo opposto rispetto
  alla \textit{codifica di sorgente}, infatti ha come obiettivo quello di
  contrastare il rumore e per farlo aggiunge ridondanza al messaggio (da qui il
  discorso sull'obiettivo opposto)
\end{enumerate}
Si cerca quindi di comprimere il più possibile nella prima fase, quella di
\textit{codifica di sorgente} e di ridondare il meno possibile nella seconda,
quella di \textit{codifica di segnale}.\\
Per capire meglio quanto detto diamo alcune formalità.
\begin{definizione}
  Una \textbf{codifica} è una funzione $cod$ che prende i simboli della sorgente
  $S=\{s_1, s_2,\ldots,s_q\}$ e ad ogni simbolo $s_i$ gli assegna una stringa
  formata coi caratteri di un certo alfabeto $\Gamma$, l'\textbf{alfabeto
    della codifica}. Le stringhe di $\Gamma^*$ sono tutte quelle costruite
  sull'alfabeto $\Gamma$ di lunghezza arbitraria e finita, compresa la stringa
  vuota $\varepsilon$, che posso quindi formare coi simboli di $\Gamma$. Quindi
  ad ogni $s_i\in S$ assegno un $\gamma_i\in Gamma^*$, avendo che:
  \[cod:S\to \Gamma^*\]
  generalmente si ha che:
  \[|\Gamma|<|\Sigma|\]
  e quindi i simboli di $\Sigma$ sono mappati da $cod$ in  sequenze  di  simboli
  di $\Gamma$, a meno che non si ritenga accettabile il fatto che due o più
  simboli di $\Sigma$ vengano mappati nellostesso simbolo di $\Gamma$. \\
  \textit{Più avanti nel corso vedremo casi in cui $|\Gamma|>|\Sigma|$}
\end{definizione}
Si hanno quindi i simboli $S=\{s_1, s_2,\ldots,s_q\}$ che escono con
probabilità $(p_1,p_2,\ldots,p_q)$ e che vengono codificati con le stringhe
$\gamma_1,\gamma_2,\ldots,\gamma_q$. Le varie $gamma_i$ sono dette
\textbf{codeword}. Chiamando $l_i=|\gamma_i|$ la lunghezza 
di tali stringhe si ha che tali stringhe hanno associati i vari
$l_1,l_2,\ldots,l_q$.\\
L'obiettivo quindi della \textit{codifica di sorgente} è quello di minimizzare
la lunghezza media $L$ delle stringhe, avendo quindi una media pesata (pesata
sulle probabilità):
\[L=\sum_{i=1}^q p_i\cdot l_i\]
Tenendo conto delle probabilità, per minimizzare $L$, si deve, avendo a che fare
con termini che sono tutti $>0$ (avendo supposto che non si hanno probabilità
nulla e avendo che una codeword pari alla stringa vuota ha poco senso), fare in
modo che i termini siano tutti il più piccolo possibile. Dato che le probabilità
sono date mentre la codifica la sto costruendo, calcolando le codeword e di
conseguenza le loro lunghezze, devo fare in modo che se la probabilità è grande
la lunghezza deve essere piccola. Se invece la probabilità è piccola posso
permettermi una lunghezza più grande. Parto quindi dai simboli con probabilità
più grande e inizio a usare codeword più piccole possibili, usando via via
quelle più lunghe. \\
Un'idea simile è usata nel \textit{codice Morse} dove le
lettere meno comuni hanno le sequenze più lunghe di punti, linee e spazi (avendo
una codifica ternaria). La lettera più comune, la ``e'', ha infatti solo con un
punto, la codifica più breve mentre le meno comuni hanno sequenze multiple di
punti, linee e spazi che le separano (e gli spazi contano nella lunghezza di
queste codeword).\\
Noi non sappiamo in anticipo che messaggi verranno prodotti dalla sorgente e
quindi le codeword vanno studiate passo a passo, valutando i simboli più
probabili per associare le codeword più brevi e i meno probabili per le
codeword più lunghe.\\
Analizziamo meglio i codici, le codeword. Possono essere:
\begin{enumerate}
  \item \textit{a lunghezza fissa}, ovvero si ha che $l_1=l_2=\cdots=L_q$
  \item \textit{a lunghezza variabile}, avendo che ogni codeword può avere
  lunghezza diversa
\end{enumerate}
Ne segue quindi che il discorso di minimizzare $L$ ha senso solo in presenza di
\textit{codeword a lunghezza variabile} (potendo decidere per ogni simbolo che
codeword associare), avendo la \textbf{codifica a lunghezza variabile}.\\
Con \textit{codeword a lunghezza fissa} avrei tutte le $l_i$ uguali e quindi
avrei, avendo $l_i=l,\forall i$:
\[L=\sum_{i=1}^q p_i\cdot l_i \sum_{i=1}^q p_i\cdot l =l\cdot\sum_{i=1}^q
  p_i=l\cdot 1 = l\]
avendo, come facilmente intuibile, che la lunghezza media è la lunghezza fissa
stessa. Si hanno codifiche a lunghezza fissa, come banalmente numeri a 64bit
etc$\ldots$ in tal caso si parla di \textbf{codici a blocchi}.\\
Usando codifiche a lunghezza fissa si hanno anche esempi interessanti come
quello del \textit{codice pesato}, detto \textbf{codice pesato 01247}. Il nome
deriva dal fatto che si possono codificare le cifre da 0 a 9 (da 1 a 9 con poi
lo 0 dopo il 9) sotto forma di stringhe di 5 bit usando i pesi 0,1,2,4,7
associati a ciascun bit. Vediamo la tabella con la codifica di questo codice:
\begin{table}[H]
  \centering
  \begin{tabular}{c|ccccc} 
    & 0 & 1 & 2 & 4 & 7 \\
    \hline
    1 & 1 & 1 & 0 & 0 & 0 \\
    2 & 1 & 0 & 1 & 0 & 0 \\
    3 & 0 & 1 & 1 & 0 & 0 \\
    4 & 1 & 0 & 0 & 1 & 0 \\
    5 & 0 & 1 & 0 & 1 & 0 \\
    6 & 0 & 0 & 1 & 1 & 0 \\
    7 & 1 & 0 & 0 & 0 & 1 \\
    8 & 0 & 1 & 0 & 0 & 1 \\
    9 & 0 & 0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0 & 1 & 1
  \end{tabular}
\end{table}
Si nota che ogni codeword ha sempre 3 bit pari a 0 e due bit pari a 1, avendo un
\textbf{codice 2-su-5}. Banalmente i pesi si associano ai numeri 0,1,2,4,7 in
modo tale che essi, sommati, formino il numero voluto (ad esempio per 1 avrò i
pesi su 0 e 1, per 9 su 2 e 7 etc$\ldots$). L'unico caso è il caso dello 0, che
non può essere ottenuto come somma di due pesi (spesso si hanno nei codici casi
speciali da gestire a parte). Per lo 0 viene quindi presa una codeword non usata
per altri numeri e quindi l'unica scelta possibile è avere i pesi su 4 e 7
(visto che farebbe 11).\\
Su un totale ci 5 bit, avendo due bit a 1 e tre bit a 0, posso avere un numero
di codeword pari a:
\[n={{5}\choose{2}}=10\]
avendo che i 5 bit sono associati ai 5 elementi dove 1 segnala che ``sto usando
quel peso'', prendendo quindi i sottoinsiemi di due elementi a partire da un
insieme di cinque elementi, ovvero ``in quanti modi posso formare sottoinsiemi
che contengono due elementi a partire da un insieme di cinque elementi'' o detto
altrimenti ``quanti sono i modi in cui posso disporre due uni all'interno di una
stringa di cardinalità cinque''.\\
In un linguaggio di programmazione privo di una struttura dati dedicata posso
simulare un insieme di questo tipo tramite un vettore di bit (con 1 se
l'elemento associato all'indice c'è).\\
\textit{Il codice a barre è detto \textbf{codice 39} ed è un \textbf{codice
    3-su-9}}. \\
In merito alla \textbf{decodifica} si ha che anch'essa sarà di due tipi:
\begin{enumerate}
  \item \textbf{decodifica di canale}, vedendo e c'è stato un errore di
  trasmissione ed eventualmente correggendolo in automatico se il codice mi
  consente di farlo
  \item \textbf{ulteriore decodifica} che non è esattamente una \textit{codifica
    di sorgente} ma quanto una \textit{trasformazione}, dove le
  \textit{codeword} vengono trasformate nel formato leggibile dal
  \textbf{ricevente} 
\end{enumerate}
\section{Codici per individuare errori}
Ci concentriamo ora sulla \textit{codifica di canale} ignorando per ora la
\textit{codifica di sorgente}, avendo come obiettivo l'aggiunta di ridondanza a
simboli, che si suppongono già codificati con codeword, in modo tale che in
queste codeword, spedite nel canale dove eventualmente si possono avere
modifiche causate dal rumore, vengano eventualmente riconosciuti (ed
eventualmente corretti) errori in fase di \textit{decodifica}.\\
\textit{Parlando di codici per individuare errori solitamente, nei disorsi, si
  ha che $|\Gamma|<|\Sigma|$}
Qualora il ricevente con la sua decodifica si accorga che è successo qualcosa ma
non si è in grado di 
correggere quel qualcosa si hanno i cosiddetti \textbf{codici per individuare
  gli errori (\textit{error detection codes})}. Nel caso in cui il ricevente con
la sua decodifica si accorga dell'errore ci si chiede anche se può correggerlo
autonomamente senza chiedere che la sorgente spedisca nuovamente il
messaggio. Non sempre questa cosa si può fare ma quanto accade si parla di
\textbf{codici a correzione d'errore (\textit{error correction codes})}.
\subsection{Controllo di parità semplice}
Vediamo come capire se un messaggio ricevuto è valido.\\
Si supponga di spedire un pacchetto di $n$ bit (ma potrebbe essere qualsiasi
altra cosa ma per praticità prendiamo un bit) nel canale e che da esso esca un
certo pacchetto sempre di $n$ bit (per il rumore potrebbe non essere lo stesso).
\begin{definizione}
  Definiamo il \textbf{controllo di parità}. \\
  Avendo una sequenza di $n$ bits in
  cui si ha $n-1$ bits, dette \textbf{cifre di messaggio} di
  \textbf{messaggio vero}, che chiameremo
  \textbf{\textit{msg}} e un bit che è la \textbf{cifra di controllo}, che
  chiameremo \textbf{\textit{check}}. Le cifre di messaggio si indicano con
  $\circ$ mentre la cifra di controllo con $\times$ e quindi il messaggio è del
  tipo:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
      $\circ$&$\circ$&$\circ$&$\circ$&$\cdots$&$\circ$&$\times$\\
      \hline
    \end{tabular}
  \end{table}
  avendo $n-1$ $\circ$ e un solo $\times$ (che potrebbe anche non essere in
  fondo, basta avere coscienza della posizione nel pacchetto, concordando la
  cosa tra mittente e ricevente).\\
  Si ha che:
  \begin{itemize}
    \item chi spedisce ha le cifre di messaggio e deve calcolare la cifra di
    controllo
    \item chi riceve controlla che la cifra di controllo sia coerente con le
    cifre di messaggio
  \end{itemize}
  Nell'\textbf{error detection code} il ricevente è solo in grado di capire che
  la sequenza non è valida ma per farlo bisogna assumere di avere limitazioni
  nella sequenza di $n$ bit che è entrata nel canale. Questa limitazione è che
  gli $n$ bit entranti nel canale siano una \textbf{codeword valida}, avendo
  che, preso un sottoinsieme $M$ di tutto l'insieme di $n$ bit, ovvero
  $M\subseteq\{0,1\}^n$, $M$ è un insieme di \textit{codeword valide}. Quindi
  solo un messaggio appartenente a $M$ può entrare nel canale. Fatta questa
  premessa, quando esce un messaggio, ho che, a causa del rumore, questo
  messaggio viene rovinato, non avendo più un messaggio
  valido (cosa che viene capita dal ricevente). \textit{Purtroppo può succedere
    che il rumore trasformi un messaggio valido in un altro messaggio valido ma
    non considereremo questa opzione per ora}. \\
  Nel \textbf{controllo di parità semplice} il pacchetto di $n$ bit, come visto
  è formato da $n-1$ bit di messaggio e un bit di controllo, la \textbf{check
    digit}. Si procede quindi, ricordando che siamo in un caso binario, a
  contare il numero di $1$ nei primi $n-1$ bit e se questo è dispari setto il
  bit di check a $1$ e si nota che così il numero di $1$ nel pacchetto intero
  di $n$ bit diventa pari, avendo la cosiddetta \textbf{parità pari} (ovvero
  ogni sequenza valida ha un numero pari di $1$). Controllando il numero di $1$
  il ricevente capisce se il rumore ha modificato il messaggio anche se non può
  capire cosa è successo (avendo quindi che il \emph{controllo di parità
    semplice} è solo un \emph{error detecting code} e non un \emph{error
    correcting code}). Non potendo fare nulla, in caso di errore identificato,
  il ricevente può solo chiedere al mittente di inviare nuovamente il messaggio.
  Qualora il rumore modificasse il messaggio in modo tale che si abbia comunque
  un numero apri di $1$ si rientrerebbe nella casistica sopra descritta in cui
  il rumore forma ancora un messaggio valido. Questa cosa può succedere se, nel
  caso binario, il rumore modifica un numero pari di cifre e quindi il
  \emph{controllo di parità semplice} funziona solo se viene modificato un
  \emph{numero dispari di cifre}.
  
\end{definizione}
Vediamo quindi meglio come calcolare la \textbf{check digit}.\\
Si rinominiamo gli $n$ bit come:
\[x_1x_2\cdots x_{n-1}\,y\]
quindi con $y$ \textit{check digit}.\\
Si ha che, con $\oplus$ \textit{xor}:
\[y=x_1\oplus x_2\oplus\cdots\oplus x_{n-1}\]
Ricordando che:
\begin{table}[H]
  \centering
  \begin{tabular}{c|c|c|c}
    $a$& $b$ & $a\land b$& $a\oplus b$\\
    \hline
    0 & 0 & 0 &0\\
    0 & 1 & 0 & 1\\
    1 & 0 & 0 &1\\
    1 & 1 & 1 & 0\\    
  \end{tabular}
\end{table}
quindi vale $1$ sse i due bit in input sono diversi ma questo non ci aiuta su
$n-1$ input. Altrimenti si ha che vale $1$ sse il numero di $1$ in input è
dispari e questo ci aiuta su $n-1$ input infatti la generalizzazione dello
\textit{xor} a più di due input è detta \textbf{funzione di parità}. Un altro
punto di vista per considerare lo \textit{xor} è quello della \textbf{somma a
  modulo 2} usando la notazione:
\[y=\bigoplus_{i=1}^{n-1}x_i=\sum_{i=1}^{n-1}x_i\bmod\,\, 2\]
avendo che faccio prima la somma e poi il modulo 2 mi dice 0 se è pari e 1 se è
dispari. \\
Si ha inoltre una relazione interessante tra le formule scritte usando solo
$\oplus$ e $\land$ nella cosiddetta \textbf{forma algebrica normale
  (\textit{ANF})}. Queste formule booleane possono essere trasformate in formule
aritmetiche con \textit{modulo 2}, quindi in $\mathbb{Z}_2$ dicendo che lo
\textit{xor} equivale alla \textit{formula modulo due} e l'\textit{and} al
\textit{prodotto modulo due} (la cosa vale in entrambi i versi).\\
La funzione di parità è così usata che in tutti i microprocessori, fin dagli
anni settanta, si ha un \textit{flag di parità} tra i flag della CPU, che viene
settato come appena visto a seconda dei bit caricati su un registro particolare
(a volte detto \textit{accumulatore}). Tale calcolo è facilmente mappabile in
un circuito, avendo che lo \textit{xor} gode della proprietà associativa (avendo
un circuito che fa un albero di porte \textit{xor}).

Posso anche simulare lo \textit{xor} con un \textbf{automa a stati finiti}, con
due stati ``pari'' e ``dispari'', con il ``pari'' stato iniziale (diciamo che
input vuoto è pari):
\begin{center}
  \begin{tikzpicture}[shorten >=1pt,node distance=3cm,on grid,auto]
    \node[state, initial, accepting] (q_0) {$pari$};
    \node[state, accepting] (q_1) [right=of q_0] {$dispari$};
    \path[->]
    (q_0) edge [bend left = 25] node {1} (q_1)
    edge [loop above] node {0} ()
    (q_1) edge [bend left = 25] node {1} (q_0)
    edge [loop right] node {0} ();
  \end{tikzpicture}
\end{center}
Questo metodo ha senso se il canale è pochissimo rumoroso, avendo pochissima
probabilità di avere la modifica di un bit e ancora meno di due (due modifiche
si ricorda che non verrebbero rilevate essendo pari), così poca da poter
ipotizzare che non avvengano mai due errori (e se mai dovesse succedere
bisognerà valutare l'impatto del problema e le conseguenze). Stiamo assumendo
quindi che la \textbf{probabilità d'errore} può essere \textbf{trascurabile}
infatti canali di buona qualità dovrebbero sbagliare non più di un bit su un
milione, per canali più affidabili anche uno su un miliardo. Possiamo quindi
trascurare che possano accadere due errori e dire che il \textbf{controllo di
  parità} va bene. \\
Parliamo ora meglio di \textbf{ridondanza}, definendola formalmente.
\begin{definizione}
  La \textbf{ridondanza} $R$ è definita come:
  \[R=\frac{\mbox{il numero totale di simboli/cifre spediti}}{\mbox{numero di
        simboli/cifre che sono effettivamente parte del messaggio}}\] 
  Nel caso del \emph{controllo di parità} i simboli che vogliamo spedire sono
  $n$ bit a fronte di $n-1$ bit di vero messaggio. Si ha quindi:
  \[R=\frac{n}{n-1}=\frac{(n-1)+1}{n-1}=1+\frac{1}{n-1}\]
  Mettendo in evidenza che la ridondanza è sempre $R\geq 1$, visto che a
  numeratore abbiamo almeno una cifra in più (quella della \emph{check digit}) e
  che quindi è sicuramente maggiore del denominatore. In realtà per avere $R=1$
  dovrei avere numeratore e denominatore uguali che non ha molto senso parlando
  di ridondanza, quindi nei casi \emph{interessanti} si ha che $R>1$. Guardando
  la formula la cosa è confermata da $1+\frac{1}{n-1}$ con $\frac{1}{n-1}$ che
  viene detto \textbf{eccesso di ridondanza}.
\end{definizione}
\begin{definizione}
  Possiamo \textbf{generalizzare} la definizione di \textbf{ridondanza}:
  \[R=\frac{msg+check}{msg}\]
  avendo:
  \begin{itemize}
    \item $msg$ numero di simboli/cifre di messaggio
    \item $check$ numero di cifre di controllo
  \end{itemize}
  Ma allora (avendo $check<msg$ per avere qualcosa di sensato):
  \[R=\frac{msg+check}{msg}=1+\frac{check}{msg}\]
  che è la forma ``generale'' della ridondanza. Si ha che $\frac{check}{msg}$ è
  \textbf{eccesso di ridondanza}. 
\end{definizione}
\textit{Si può dire di non avere necessità di ``proteggere'' di più il bit di
  parità in quanto, per la macchina, conta come tutti gli altri. Tutti vanno
  ``protetti'' nello stesso modo.}\\
\textit{Come ho la \textbf{parità pari} potrei avere la \textbf{parità dispari},
dove i messaggi validi hanno un numero dispari di $1$. I vari ragionamenti sono
analoghi, essendo tutto uguale dal punto di vista matematico, avendo un
isomorfismo tra le due tecniche. La scelta tra i due dipende dai casi è dalla
celta di cosa rappresentiamo con $0$ e $1$ (pensiamo con $0$ che rappresenta
assenza di segnale, in questo caso meglio usare la parità dispari, mentre se $0$
e $1$ rappresentassero diverse quantità di Volt andrebbe bene la parità
pari)}. \\ 
\textbf{Nel corso si userà comunque solo la \textit{parità pari}}.
\subsection{Il rumore bianco}
Introduciamo ora un primo \textit{modello di rumore}, il \textbf{modello del
  rumore bianco}.
\begin{definizione}
  Un \textbf{modello di rumore} è un modello matematica che descrive cosa
  succede nel canale quando il rumore rovina i bit.
\end{definizione}
\begin{definizione}
  Il \textbf{modello del rumore bianco} consiste nell'avere il messaggio con i
  bit $x_1x_2\cdots x_n$ (con magari $x_n$ come controllo di parità ma dato che
  ``i bit non sono colorati'' la cosa non ci interessa davvero) e avere una
  certa probabilità $p$. Si hanno due condizioni:
  \begin{enumerate}
    \item si ha che $p\in(0,1)$ che è la probabilità che avvenga
    un errore in ogni posizione $i\in [1,n]$ del messaggio. Si ha quindi che la
    probabilità $p$ è uguale in tutte le posizioni
    \item le posizioni sono tutte indipendenti, ovvero il fatto che magari si ha
    un errore nella posizione $i$ non influisce sulle altre. Avendo quindi
    l'eventi casuale $E_i$ con:
    \[E_i=\mbox{è avvenuto un errore in i}\]
    allora:
    \[E_i\mbox{ ed }E_j \mbox{ sono indipendenti, }\forall i\neq j\]
  \end{enumerate}
  \textbf{Le due proprietà sopra elencate rendono molto semplice il modello}.\\
  Questo però non è molto realistico, basti pensare al rumore dovuto ad uno
  sbalzo di corrente, dove da un bit in poi e per diversi bit si avranno alte
  probabilità d'errore. Quando l'errore influisce su una certa porzione di bit
  si dice che si ha un \textbf{burst di errori} (che non può essere gestito con
  le tecniche per il \textit{rumore bianco}, anche se si riesce con qualche
  workaround).\\
  Si è visto che $p\in(0,1)$ infatti:
  \begin{itemize}
    \item se si avesse $p=0$ si avrebbe che ogni bit arriverebbe sempre
    corretto, ma questo può avvenire solo in un mondo utopico e non in quello
    reale/fisico. Non esiste un canale reale non affetto da errori, quindi si ha
    $p\neq 0$
    \item se si avesse $p=1$ si avrebbe che ogni bit del messaggio arriverebbe
    errato ma questa non sarebbe una brutta situazione, anzi sarebbe ottima
    infatti mi basterebbe avere una porta logica \textit{not} della linea di
    trasmissione per riottenere il messaggio corretto, ottenendo un canale $p=0$
    d'errore. Anche questo però è irrealistico quindi $p\neq 1$ 
  \end{itemize}
  Supponiamo ora che $p>\frac{1}{2}$ quindi ho più probabilità che un bit arrivi
  sbagliato che giusto. Anche in questo caso una porta logica \textit{not} alla
  fine della linea di trasmissione per ottenere un canale con $1-p$ come
  probabilità d'errore. Quindi anche questo non ha molto senso quindi si
  considera che:
  \[p\in(0,\frac{1}{2})\]
  Manca solo da valutare $p=\frac{1}{2}$. \\
  Con $p=\frac{1}{2}$ si ha che il bit
  di output è completamente causale e indipendente da cosa sia stato spedito. È
  come se il canale generasse $n$ bit casuali con probabilità uniforme
  ($\frac{1}{2}$), avendo un cosiddetto \textbf{canale completamente
    rumoroso}. Dal punto di vista pratico sarebbe interessante un tale canale,
  per altri punti di vista (come quello della crittografia), avendo infatti un
  \textbf{generatore di bit completamente casuali}. Purtroppo questo non si può
  fare quindi si assume $p\neq \frac{1}{2}$.
\end{definizione}
Cerchiano di capire quale sia la probabilità che avvengano $k$ errori con $0\leq
k\leq n$ (quindi da nessun errore a tutti gli $n$ bit errati), che indichiamo
con: 
\[p[\mbox{ k errori }]\]
Valutiamo i vari casi:
\begin{itemize}
  \item partiamo con 1 errore, quindi $k=1$, avendo $p[\mbox{ 1 errori }]$.\\
  Questo significa che per il messaggio di $n$ bit si immagina un vettore di bit
  associato con $0$ e $1$ come
  ``bandierine'' che indicano se è avvenuto un errore o no in una certa
  posizione. Quindi se in un 
  certa posizione ho $0$ diciamo che significa che non ho un errore di
  trasmissione mentre se ho $1$ ho un errore. Il messaggio di $n$ bit diventa
  quindi una sorta di \textit{maschera} che con gli $1$ mi dice dove è avvenuto
  l'errore. Se suppongo che ne è avvenuto uno solo avrò un solo $1$ e bisogna
  calcolare la  probabilità che questo avvenga.\\
  Supponga che l'errore sia al primissimo bit, quindi in posizione $i=1$, avendo
  quindi, per il discorso delle ``bandierine'' che $msg=10000\ldots 0$ e quindi
  si ha, avendo che la probabilità che avvenga la trasmissione avvenga
  correttamente è $1-p$ (cosa che avviene $n-1$ volte), mentre $p$ che avvenga
  sbagliata (cosa che avviene una sola volta):
  \[p[\mbox{ 1 errori }]=p^1\cdot(1-p)^{n-1}=p\cdot(1-p)^{n-1}\]
  \textbf{Posso fare $\cdot$ in quanto si è supposta l'\textit{indipendenza}
    (non avendo intersezioni tra gli eventi)}.\\
  Ma questo non sta considerando tutto ma solo la prima posizione. Completando
  il calcolo avendo di volta in volta in somma la probabilità di un errore nella
  posizione $i$ ho che:
  \[p[\mbox{ 1 errori }]=p^1\cdot(1-p)^{n-1}+(1-p)^1\cdot
    p^1\cdot(1-p)^{n-2}+\cdots\]
  ma questo conto si può semplificare, avendo sempre gli stessi termini che si
  ripetono:
  \[p[\mbox{ 1 errori }]=n\cdot p^1\cdot(1-p)^{n-1}=n\cdot p\cdot(1-p)^{n-1}\]
  Infatti so che $p\cdot(1-p)^{n-1}$ è la probabilità di avere un errore in una
  certa 
  posizione fissata. Mi chiedo dove posso mettere questa posizione in tutti i
  modi possibili nel pacchetto di $n$ bit e ho che, avendo un solo errore, ho
  $n$ modi per posizionarlo, ciascuno con probabilità $p\cdot(1-p)^{n-1}$
  
  \item passiamo a due errori, avendo $p[\mbox{ 2 errori }]$.\\
  Ho un ragionamento analogo. Parto supponendo di avere i due errori nelle prime
  due posizioni del messaggio/pacchetto, avendo quindi 1 nelle prime due
  posizioni della maschera. Abbiamo comunque già visto che poi il ragionamento
  si generalizza per qualsiasi posizione, in questo caso coppie (anche non
  consecutive) di posizioni. Si ha che, ipotizzando che le prime due siano
  errate:
  \[p[\mbox{ 2 errori }]=p^1\cdot p^1\cdot(1-p)^{n-2}=p^2\cdot(1-p)^{n-2}\]
  Ma anche qui dobbiamo vedere la probabilità per qualsiasi coppia, facendo
  variare le due posizioni d'errore in tutti i modi possibili ma questo è come
  prendere un qualsiasi sottoinsieme di due elementi a partire da un insieme di
  $n$ elementi ma questo altro non è che il calcolo che si fa tramite il
  coefficiente binomiale, avendo quindi:
  \[p[\mbox{ 2 errori }]={{n}\choose{2}}\cdot p^2\cdot(1-p)^{n-2}\]
  \item analogamente a quanto fatto per due errori potrei fare con tre,
  quattro, etc$\ldots$
  \item possiamo generalizzare con $k$ errori, avendo $p[\mbox{
    k errori }]$.\\
  Si hanno quindi $k$ uni da disporre in tutti i modi possibili nel vettore di
  $n$ bit. Si ha quindi:
  \[p[\mbox{ k errori }]={{n}\choose{k}}\cdot p^k\cdot(1-p)^{n-k}\]
  E quindi posso valutare la cosa nei due casi estremi:
  \begin{itemize}
    \item $k=0$, avendo 0 errori. Ho un solo modo per mettere zero $1$ nella
    maschera di bit (da nessuna parte) e infatti (avendo poi tutti gli $n$ bit
    la stessa probabilità di uscire corretti): 
    \[p[\mbox{ 0 errori }]={{n}\choose{0}}\cdot p^0\cdot(1-p)^{n-0}=1\cdot
      1\cdot (1-p)^n=(1-p)^n\]
    \item $k=n$, avendo $n$ errori\footnote{Su dispense del prof grafico con
      $n=8$, $p=0.1$ e $k$ che varia tra 0 e 8}. Ho un solo modo per mettere
    tutti $1$ nella  maschera di bit (ovunque) e infatti (avendo poi tutti gli
    $n$ bit la stessa probabilità di uscire errati):
    \[p[\mbox{ n errori }]={{n}\choose{n}}\cdot p^n\cdot(1-p)^{n-n}=1\cdot
      p^n\cdot 1=p^n\]
  \end{itemize}
  \textit{Si nota che i due casi estremi sono ``speculari''}.
\end{itemize}
\textit{In questo elenco puntato si è quindi ragionato sulle celle della
  maschera di bit associata al pacchetto e non del pacchetto in se, anche se
  spesso risulti ambiguo}.\\
Consideriamo ora nuovamente $p[\mbox{ 1 errore }]$, si ha che, dalla
generalizzazione è:
\[p[\mbox{ 1 errore }]={{n}\choose{1}}\cdot p^1\cdot(1-p)^{n-1}=n\cdot p\cdot
  (1-p)^{n-1}\]
Introduciamo un'approssimazione interessante dell'analisi matematica che vale
per $\alpha\in \mathbb{R}$ e $|X|<1$, ovvero $-1<x<1$:
\[(1+x)^\alpha\simeq 1+\alpha\cdot x\]
Ovvero $1+\alpha\cdot x$ sono i primi due termini dello sviluppo in serie di
$(1+x)^\alpha$.\\
Tratto quindi la formula per un errore in base a questa approssimazione:
\[p[\mbox{ 1 errore }=n\cdot p\cdot(1-p)^{n-1}\simeq n\cdot p\cdot
  [1-p\cdot(n-1)]=n\cdot p-n^2\cdot p^2+n\cdot p^2\]
Ma so che $p\in(0,1)$ e quindi $p^2<p$, infatti (\textit{grafico
  approssimativo}): 
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.8]{img/p.pdf}
\end{figure}
ma quindi, sempre approssimando (avendo quindi già due approssimazioni):
\[p[\mbox{ 1 errore }=n\cdot p\cdot(1-p)^{n-1}\simeq n\cdot p-n^2\cdot
  p^2+n\cdot p^2\simeq n\cdot p\]
Quindi:
\[p[\mbox{ 1 errore }\simeq n\cdot p\]
Analogamente ragiono per due errori:
\[p[\mbox{ 2 errori }]={{n}\choose{2}}\cdot
  p^2\cdot(1-p)^{n-2}\simeq{{n}\choose{2}}\cdot
  p^2\cdot[1-p\cdot(n-2)]={{n}\choose{2}}\cdot (p^2-n\cdot p^3+2p^3)\]
Ma anche qui si ha che $p\in(0,1)$ e quindi $p^3<p$, e quindi si ha:
\[p[\mbox{ 2 errori }]\simeq{{n}\choose{2}}\cdot p^2=\frac{n\cdot
    (n-1)}{2}\cdot p^2\]
In generale, per $k$ errori, con gli stessi passaggi:
\[p[\mbox{ k errori }]\simeq{{n}\choose{k}}\cdot p^k\]
\textbf{I conti diventano molto più semplici}.\\
Si è detto che se la probabilità di due errori è piccoli si può decidere di
trascurarla (usando poi solo il \textit{controllo di parità semplice}). Da
queste approssimazioni vediamo che la probabilità di un errori è $\simeq n\cdot
p$ mentre per due $\simeq\frac{n\cdot (n-1)}{2}\cdot p^2$. Se ipotizziamo $p\sim
10^{-6}$, quindi uno su un milione, si ha che $p^2=10^{-12}$ quindi assolutamente
trascurabile. Si segnala comunque che questa non è una pratica standard.
Normalmente si hanno, ad esempio, \textbf{low density parity codes (LDPC)} dove
si sparano a caso vari controlli di qualità nell'ottica di mantenere le
proprietà di correzione degli errori usando meno controlli possibile, avendo,
tornando alla ridondanza $R=1+\frac{check}{msg}$, che si vuole usare il minor
numero di cifre di controllo per abbassare l'\textit{eccesso di ridondanza},
abbassando la ridondanza stessa, avvicinandosi quindi a $1^+$ (ci si avvicina
da destra ovviamente). Riducendo l'\textit{eccesso di ridondanza} si ha che
ogni cifra di controllo \textit{copre/protegge} il maggior numero ci
simboli/cifre del messaggio. \textit{A parità di simboli inviati si vuole quindi
  ridurre il numero di cifre di controllo}.\\
Approfondiamo e usiamo quindi il\textit{ modello del rumore bianco} per vedere
qual è a probabilità che il ricevente non riesca a capire che c'è stato un
errore utilizzando il \textit{controllo di parità semplice}.\\
Ricordiamo che il messaggio è della forma:
\[x_1x_2\cdots x_{n-1}y\]
con $y$ \textit{controllo di parità semplice} calcolato come:
\[y=\bigoplus_{i=1}^{n-1}x_i\]
Un numero dispari di errore mi segnala che ci sono stati problemi, usando la
\textit{parità pari} (i pacchetti inseriti nel canale hanno un numero apri di
$1$).\\ 
Vogliamo quindi la probabilità che il controllo di parità fallisca, ovvero:
\[p[\mbox{ controllo }\bigoplus \mbox{ fallisce }]\]
ma questo è uguale alla probabilità che avvenga un numero pari di errori
(\textbf{zero escluso}, ovviamente):
{\footnotesize{\[p[\mbox{ controllo }\bigoplus \mbox{ fallisce }]=p[\mbox{ numero
      pari di errori }]=p[\mbox{ 2 errori }]+p[\mbox{ 4 errori }]+\cdots \]}}
Diciamo che, per comodità:
\[1=(1-p)+p=[(1-p)+p]^n\]
Ma so che $(a+b)^n=\sum_{k=0}^n{{n}\choose{k}}a^{n-k}b^k$, quindi:
\[1=[(1-p)+p]^n=\sum_{k=0}^n{{n}\choose{k}}(1-p)^{n-k}p^k\]
ma questa è ${{n}\choose{k}}(1-p)^{n-k}p^k=p[\mbox{ k errori }]$ (infatti la
somma di tutte le probabilità è appunto 1), quindi:
\[1=[(1-p)+p]^n=\sum_{k=0}^n{{n}\choose{k}}(1-p)^{n-k}p^k=\sum_{k=0}^np[\mbox{
    k errori }]\]
D'altro canto posso anche dire che, sempre applicando l'espansione di $(a+b)^n$,
scomponendo però $(-p)^k$ in $(-1)^k\cdot p^k$ (dove $(-1)^k$ vale $1$ per k
pari e $-1$ per $k$ dispari). Si ha quindi:
\[(1-2\cdot p)^n=[(1-p)-p]^n=\sum_{k=0}^n(-1)^k\cdot {{n}\choose{k}}\cdot
  p^k\cdot(1-p)^{n-k}\]
Ma quindi ho:
\[(1-2\cdot p)^n=
  \begin{cases}
    {{n}\choose{k}}\cdot  p^k\cdot(1-p)^{n-k} &\mbox{sse $k$ è pari}\\
    -{{n}\choose{k}}\cdot  p^k\cdot(1-p)^{n-k} &\mbox{sse $k$ è dispari}\\
  \end{cases}
\]
Quindi se $k$ è dispari le espansioni di $1$ e $(1-2\cdot p)^n$ sono uguali ma
di segno opposto mentre se $k$ è pari sono uguali con lo stesso segno. Ma quindi
questa somma delle due espansioni mi lascia col doppio dei soli termini con $k$
pari che ci aiuta volendo calcolare proprio le probabilità con un numero di
errori pari. Si ha quindi, dividendo già per due avendo il discorso del doppio:
\[\frac{1+(1-2\cdot p)^n}{2}=\sum_{t=0}^{\lfloor \frac{n}{2}\rfloor}
  {{n}\choose{2\cdot t}}\cdot p^{2t}\cdot(1-p)^{n-2t}\]
La somma va quindi da 0 alla parte intera di $\frac{n}{2}$. Nel coefficiente
binomiale ho $2\cdot r$ che è una quantità sicuramente pari. In generale è come
se avessi $k=2\cdot t$ ciclando solo sui $k$ pari. Ho quindi ottenuto:
\[p[\mbox{ 0 errori }]+p[\mbox{ 2 errori }]+p[\mbox{ 4 errori }]+\cdots \]
Non vogliamo però $t=0$ quindi:
{\footnotesize{\[p[\mbox{ controllo }\bigoplus \mbox{ fallisce }]=p[\mbox{
        numero pari di errori }]=\frac{1+(1-2\cdot 
    p)^n}{2}-p[\mbox{ 0 errori }]\]}}
\[=\frac{1+(1-2\cdot p)^n}{2}-  {{n}\choose{0}}\cdot
  p^{0}\cdot(1-p)^{n-0}=\frac{1+(1-2\cdot p)^n}{2}-(1-p)^n\]
E quindi:
{\small{\[p[\mbox{ controllo }\bigoplus \mbox{ fallisce }]=p[\mbox{ numero pari
        di errori }]=\frac{1+(1-2\cdot p)^n}{2}-(1-p)^n\]}}
D'altro canto potrei anche calcolare $p[\mbox{ numero dispari di errori }]$:
\[p[\mbox{ numero dispari di errori }]=1-p[\mbox{ numero pari di errori }]\]
(\textit{o anche modificando la sommatoria per ciclare sui $k$ dispari}).\\
Facendo qualche conto\footnote{i calcoli
  per il numero dispari di errore sono materiale extra sulle dispense del
  docente} si ottiene che: 
\[p[\mbox{ numero dispari di errori }]=1-(p[\mbox{ numero pari di errori
  }]+p[\mbox{ 0 errori }])\]
ovvero:
\[p[\mbox{ numero dispari di errori }]=\frac{1-(1-2\cdot p)^n}{2}\]
\textit{In generale il numero dispari di errori è meno interessante}.\\
\subsection{Gestione dei burst}
Come abbiamo introdotto con il solo \textit{controllo di parità} e con
il \textit{rumore bianco} non si possono gestire i \textbf{burst di
  errori}. Vediamo quindi un modo semplice per gestirli.\\
Si supponga di voler spedire dei messaggi formati da lettere, ad esempio:
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    c&i&a&o\\
    \hline
  \end{tabular}
\end{table}
Posiamo di rappresentare ogni lettera tramite l'\textbf{ASCII standard} a 7 bit:
\begin{table}[H]
  \centering
  \begin{tabular}{c|c}
    char & bit\\
    \hline
    c&1000011\\
    i&1001001\\
    a&1000001\\
    o&1001111
  \end{tabular}
\end{table}
Si supponga di avere dei burst di errori di lunghezza $L$ e per semplicità
assumo $L$ di lunghezza pari alle singole word, quindi $L=7$. Per gestire il
burst spedisco prima i 7 bit della prima lettera poi quelli della seconda
etc$\ldots$\\
Infine spedisco un intero pacchetto di bit di controllo di 7 bit dove ogni bit
viene calcolato controllando quella posizione di bit in tutti i pacchetti
precedenti, sempre tramite lo \textit{xor}. Nel caso d'esempio si ha quindi, con
$x$ per indicare il \textbf{check} (se nella colonna sopra ho un numero apri di
$1$ metto $0$ altrimenti $1$):
\begin{table}[H]
  \centering
  \begin{tabular}{c|c}
    c&1000011\\
    i&1001001\\
    a&1000001\\
    o&1001111\\
    \hline
    x&0000100\\
  \end{tabular}
\end{table}
Suppongo un burst che rovini dalla posizione 2 alla 4 incluse (avendo che quindi
molto probabilmente non ci torneranno i conti facendo il check su
$x[2,4]=000$).\\
Ovviamente anche qui un numero pari di errori inganna il sistema avendo comunque
un \textit{controllo di parità semplice} e anche in caso d'errore il ricevente
non sa comunque dove sia avvenuto e quindi fa \textbf{detection} ma non può fare
\textbf{correction}.
\subsection{Codici pesati}
Abbiamo già parlato del \textbf{codice 01247} vediamo ora un codice pesato più
interessante e utilizzato.\\
\begin{definizione}
  Definiamo questo \textbf{codice pesato} come un codice per cui si hanno alcune cifre
  di messaggio $msg=m_1m_2\cdots m_nc$ alle quali associamo dei pesi che
  dipendono dalla posizione in cui si trovano le varie cifre. In particolare si
  ha peso:
  \begin{itemize}
    \item $1$ per la \textbf{check digit} $c$
    \item $2$ per $m_n$
    \item si prosegue sempre aumentando di 1 per le altre cifre
    \item $n$ per $m_2$
    \item $n+1$ per $m_1$
  \end{itemize}
\end{definizione}
Questo si fa perché la cifra di controllo è calcolata per far ottenere:
\[m_1\cdot (n+1)+m_2\cdot n+\cdots+m_n\cdot 1+c\cdot 1=0\]
ma ovviamente questo non sembra possibile e infatti i conti sono fatti in
\textbf{modulo numero primo}, avendo per esempio, se scegliamo come numero
prima $37$:
\[m_1\cdot (n+1)+m_2\cdot n+\cdots+m_n\cdot 1+c\cdot 1\equiv 0\bmod\,\, 37\]
La scelta di 37 non è causale, infatti volendo:
\begin{itemize}
  \item rappresentare le 21 lettere dell'alfabeto inglese
  \item rappresentare dieci cifre da 0 a 9
  \item un simbolo per lo spazio
\end{itemize}
e quindi siamo a 32 simboli e ci serve un numero primo $\geq 31$ e quindi va
bene 37.\\
Vogliamo un numero prima perché se vogliamo fare i conti con le congruenze è
più semplice farle in \emph{modulo numero primo}.\\
Lavoriamo quindi nella classe dei resti:
\[[0]_{37},[1]_{37},\ldots, [36]_{37}\]
e in questo modo se facciamo le varie operazioni è tutto uguale al solito fino
a 36 (cosa che non succede per le classi dei resti in modulo non numero
primo).\\
La classe dei resti in modulo numero primo è un \textbf{campo} mentre se non
fosse primo si avrebbe un \textbf{anello}. In un campo se $x\cdot y=0$ o che
$x=0$ oppure $y=0$ (cosa che non succede negli anelli). Inoltre in un campo ho
che se $x\cdot y =z$ allora $x=z\cdot y^{-1}$ (in un anello non per tutti gli
$y$ esiste un $y^{-1}$ mentre in un campo sì).\\
Facendo dipendere il calcolo del peso della \textbf{check digit} da tutti gli
altri pesi perché, così facendo, sopratutto nelle comunicazioni di tipo
\textbf{seriale} (dove si spedisce una cifra alla volta), ci si accorge subito
se una cifra è andata persa oppure se si è aggiunta cifra o se due cifre si
sono scambiate (cosa comunque difficile in un sistema di comunicazione
elettronico ma è utile in altre situazioni, sopratutto di conti ``a
mano'').\\
Si supponga di avere delle cifre $b$ e $a$, la prima con peso $k+1$ e la
seconda con peso $k$, avendo una scrittura del tipo $cifra(peso)$:
\[b(k+1)+a(k)\]
Ipotizziamo di scambiare $a$ e $b$ (ora $a$ pesa $k+1$ e $b$ pesa $k$),
avendo:
\[a(k+1)+b(k)\]
Ma facendo la differenza si nota che non è nulla:
\[[b(k+1)+a(k)]-[a(k+1)+b(k)]\neq 0\]
infatti ho:
\[b\cdot k+b+a\cdot k-a\cdot k-a-b\cdot k=b-a\]
ma $b-a=0$ sse $b=a$ e quindi l'unico caso in cui non ci si accorge dello
scambio è avere lo scambio di due cifre uguali che non fa cambiare il
risultato.\\
Questa idea viene usata anche nei codici a barre. Vediamo quindi un algoritmo
per calcolare la cifra di controllo:
\begin{algorithm}[H]
  \begin{algorithmic}
    \Function{CheckCalc}{}
    \State $sum \gets 0$
    \State $ssum \gets 0$
    \While {\textit{not} EOF}
    \State \textbf{read} \textit{sym}
    \State $sum \gets sum+sym\,\,\, (\bmod\,\, 37)$
    \State $ssum \gets ssum+sum\,\,\, (\bmod\,\, 37)$
    \EndWhile
    \State $temp \gets ssum+sum\,\,\, (\bmod\,\, 37)$
    \State $c \gets 37-temp\,\,\, (\bmod\,\, 37)$
    \State \textbf{return} $c$ 
    \EndFunction
  \end{algorithmic}
  \caption{Algoritmo di calcolo dei pesi per codice pesato}
\end{algorithm}
Dove:
\begin{itemize}
  \item $sum$ tiene conto della somma numerica della nostro calcolo, accumulando
  i vari termini 
  \item $ssum$ che è una \textit{somma delle somme} e tiene conto implicitamente
  dei vari persi che crescono spostandoci da destra a sinistra come visto
  sopra. Si accumulano i termini e i loro pesi
\end{itemize}
\textit{I $\bmod 37$ nel ciclo sono in realtà superflui ma conviene farli per
  non far diventare i numeri troppo grossi. Le ultime due operazioni servono a
  risolvere alcune problematiche che non vediamo qui.}\\
Vediamo una più chiara simulazione.
\begin{esempio}
  Avendo, simulando per un messaggio $wxyzc$, con $c$ \textit{check digit}:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      msg & sum & ssum \\
      \hline
      $w$ & $w$ & $w$ \\
      $x$ & $w+x$ & $2\cdot w+x$ \\
      $y$ & $w+x+y$ & $3\cdot w+2\cdot x+y$ \\
      $z$ & $w+x+y+z$ & $4\cdot w+ 3\cdot x+2\cdot y+z$ \\
      \hline
      $c$ & $w+x+y+z+c$ & $5\cdot w+4\cdot x+3\cdot y+2\cdot z+c$ \\
      \hline
    \end{tabular}
  \end{table}
  Arrivato alla fine voglio calcolare $c$ in modo che:
  \[5\cdot w+4\cdot x+3\cdot y+2\cdot z+c \equiv 0 \bmod\,\, 37\]
\end{esempio}
Il mittente ha un messaggio e ci calcola la \textbf{check digit}.
Chi riceve fa lo stesso calcolo e alla fine controlla la \textbf{check
  digit}. Un altro modo per il ricevente è quello di fare solo l'ultimo calcolo
se farli tutti step by step.\\
Introduciamo un particolare tipo di codice, quello \textbf{ISBN
  (\textit{(International Standard BookNumber})} dei libri, che 
sono legati, per il codice a barre, allo standard europeo \textbf{EAN13} (negli
USA si usa lo standard 
\textbf{UPC}). In questo codice si hanno 10 cifre (con al più il carattere $X$)
che un identificano in modo univoco ad un libro. Nel dettaglio:
\begin{itemize}
  \item la prima cifra rappresenta lo stato in cui è stampato il libro. Questo
  ha problemi non avendo solo 9 stati che producono libri
  \item le successive 2 cifre sono le prime due per l'editore, anche questo è un
  problema in quanto alcuni stati hanno più di 100 case editrice
  \item le successive 6 sono il numero del libro
  \item l'ultima cifra è il checksum, la check digit
\end{itemize}
In realtà ho trattini dopo la prima cifra, dopo la quinta e dopo la nona ma non
contano nulla ai fini del calcolo ma sono aiutati solo per facilitare la
leggibilità dello stesso.\\
\textbf{A causa dei problemi sopra descritti i codici ISBN vengono assegnati
  ormai con libertà dalle case editrici, usando il primo codice libero}.\\
ISBN è un codice pesato dove i conti sono
fatti in $\bmod\,\,\,11$ (il più piccolo numero primo più grande di 10). Potrei
avere come risultato 10 avendo $\bmod\,\,\,11$ ma in quel caso uso $X$ come
checksum, come ultima cifra.
\begin{esempio}
  Prendiamo l'ISBN:
  \[0\mbox{-}1315\mbox{-}2447\mbox{-} x\]
  e vogliamo verificare che sia effettivamente $X$. Si ha (con $\equiv$ indico i
  conti $\bmod\,\,\,11$):
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      msg & sum & ssum \\
      \hline
      0 & 0 & 0\\
      1 & 1 & 1\\
      3  & 4 & 5\\
      5 & 10 & $20\equiv 9$\\
      2& $12\equiv 1$ & $21\equiv 10$\\
      4 & 5 & $15\equiv 4$\\
      4 & 9 & $13\equiv 2$\\
      7 & $16\equiv 5$ & 7\\
      \hline
      $X\equiv 10$ & $15\equiv 4$ & $11\equiv 0$\\
      \hline
    \end{tabular}
  \end{table}
  Avendo che effettivamente la somma $\bmod\,\,\,11$ fa 0 e quindi è
  verificato.\\ 
  Avrei inoltre, per l'algoritmo, controllando la check digit:
  \[temp = 7+5=12\equiv 1\]
  \[c=11-1=10\equiv X\]
\end{esempio}
\section{Codici per correggere errori}
In questo caso il ricevente non solo deve capire dove è l'errore ma deve anche
correggerlo. 
\subsection{Codici rettangolari}
Il modo più semplice per correggere errori è usare i \textbf{codici a correzione
  rettangolari}. \\
In questo caso il codice viene organizzato in modo logico in forma di
rettangolo, ovviamente solo in modo logico/astratto.
Indichiamo con $\circ$ i bit di messaggio e con $x$ le check digit. In questa
prima soluzione penso ai codici come se fossero a forma di rettangolo, con $m-1$
righe e $n-1$ colonne, con extra una colonna di check digit e una riga extra di
check digit. Contando la riga di check digit e la colonna di check digit
arriviamo a $m$ righe e $n$ colonne.
\begin{table}[H]
  \centering
  \begin{tabular}{cccccc}
    $\circ$ & $\circ$ & $\circ$ &$\ldots$ & $\circ$ & x\\
    $\circ$ & $\circ$ & $\circ$ &$\ldots$ & $\circ$ & x\\
    $\circ$ & $\circ$ & $\circ$ &$\ldots$ & $\circ$ & x\\
    $\vdots$ & $\vdots$ & $\vdots$ &$\ddots$ & $\circ$ & x\\
    $\circ$ & $\circ$ & $\circ$ &$\ldots$ & $\circ$ & x\\
    x&x&x&x&x&(x)
  \end{tabular}
\end{table}
\textit{L'ultima check digit in fondo a destra è superflua, anche se a volte è
  lo xor di tutte le cifre di controllo, richiedendo che abbia numero pari di
  1}.\\ 
I codici rettangolari riescono a correggere un errore, uno e uno solo ma ovunque
avvenga.\\
La check digit della prima riga fa la parità della prima riga, analogamente la
seconda lo fa per la seconda riga etc$\ldots$\\
Faccio un discorso analogo sulle colonne (la colonna 1 ha la check digit come
prima cifra della riga di check digit etc$\ldots$).\\
La check digit mi dice sempre se ho cifre pari tra cifre di messaggio e check
digit.\\ 
Tramite la check digit a fine riga capisco che in una riga si ha un errore, e
posso controllare lo stesso per la colonna. Identifico quindi il punto preciso
in cui è avvenuto l'errore e semplicemente lo inverto, essendo binario.\\
Identificare l'errore singolo è dato dal fatto che solo una riga e solo una
colonna avranno la rispettiva check digit ``rotta'' (nella colonna di check
digit identifico la riga dell'errore mentre nella riga di check digit identifico
la colonna dell'errore) e quindi posso riconoscere il preciso elemento che è
errato. Da questo discorso si capisce che la check digit in fondo a destra è in
realtà inutile.\\ 
Ovviamente questo è garantito funzionare solo per un errore in quanto due errori
potrebbero avere in comune riga o colonna e non saprei più capire dove siano gli
errori. Potrebbe funzionare per più di un errore ma non sempre causa ambiguità e
quindi questo metodo è \textbf{garantito per uno e un solo errore, ovunque si
  trovi}.\\ 
Studiamo quindi la ridondanza, che ricordiamo essere in generale:
\[R=\frac{msg+check}{msg}=1+\frac{check}{msg}\]
Per il codice rettangolare si ha quindi:
\[R_{\sqsubset \! \sqsupset}=\frac{m\cdot n}{(n-1)\cdot
    (m-1)}=1+\frac{1}{m-1}+\frac{1}{n-1}+\frac{1}{(n-1)\cdot (m-1)}\]
Con quindi $\frac{1}{m-1}+\frac{1}{n-1}+\frac{1}{(n-1)\cdot (m-1)}$ che è il
nostro \textit{eccesso di ridondanza} e si ha che è $\geq 0$, quindi in
generale:
\[R_{\sqsubset \! \sqsupset}\geq 1\]
Ricordando che ``i bit non sono colorati'' potrei avere errori anche nelle
check digit. Supponendo però che si ha al massimo un errore e supponendo di non
avere, in quanto inutile, la check digit in basso a destra,  si ha che al più
trovo ``rotta'' o una riga (se ho errore un errore nella colonna di check digit)
o su una colonna (se ho errore un errore nella riga di check digit), capendo
così che ho ``rotta'' una check digit, sapendo anche quale.
\begin{esempio}
  Se devo spedire 24 bit posso rappresentarli in vari modi, secondo vari
  rettangoli (contando anche la check digit in basso a destra):  
  \begin{itemize}
    \item una riga per 24 colonne (più la riga di controllo), con quindi 26
    check digit
    \item 2 righe per 12 colonne (più la riga di controllo), con quindi 15
    check digit
    \item 3 righe per 8 colonne (più la riga di controllo), con quindi 12
    check digit
    \item 4 righe per 6 colonne (più la riga di controllo), con quindi 11
    check digit
    \item le simmetriche di quelle dette sopra
  \end{itemize}
  
\end{esempio}
Man mano che il numero di righe tende a quello di colonne (posto che, come
nell'esempio precedente, non sempre può diventare uguale) si ha che le cifre di
controllo diminuiscono. Tornando quindi alla formula della ridondanza vediamo
che al diminuire delle check digit diminuisce l'eccesso di ridondanza
$\frac{check}{msg}$ e quindi la ridondanza tende ad avvicinarsi a 1.
Quindi per scegliere $n$ e $m$ si cerca di fare si che siano il più uguali
possibili, avendo meno cifre di controllo.
Formalizzando quanto appena detto posso vedere la ridondanza la posso vedere
come una funzione: 
\[f(n,m)\]
avendo che $\frac{1}{m-1}+\frac{1}{n-1}+\frac{1}{(n-1)\cdot (m-1)}$ è quanto
fatto dalla funzione.
Si può quindi cercare il punto minimo di questa funzione trovando che è in
$m=n$.\\
Si può anche ipotizzare di poter aggiungere una cifra di messaggio per poter
avere una rappresentazione con $n=m$, ma ovviamente dipende da caso a
caso. Questo bit aggiuntivo a seconda del caso sarebbe 0 o 1. Si nota che
aumentare il messaggio aumenta $msg$ riducendo, a parità di check digit,
$\frac{check}{msg}$, aiutando ad arrivare ad una ridondanza prossima a
1. \textbf{Non sempre ho modo di aggiungere tale bit}.\\
Il caso migliore è quindi con $n=m$ e si ha quindi, avendo un quadrato, il
calcolo più preciso per la ridondanza:
\[R_\square=\frac{n^2}{(n-1)^2}=1+\frac{2}{(n-1)}+\frac{1}{(n-1)^2}\]
che è la ridondanza migliore che si può ottenere con i codici rettangolari.
\subsection{Codici triangolari}
Vediamo quindi i \textbf{codici a correzione triangolari}, dove la
rappresentazione è appunto triangolare (a occhio una sorta di matrice
triangolare).\\ 
In questo caso si ha un triangolo con un cateto di lunghezza $n$, ad esempio
(con $n=4$): 
\begin{table}[H]
  \centering
  \begin{tabular}{ccccc}
    $\circ$ & $\circ$ & $\circ$ &$\circ$  & x\\
    $\circ$ & $\circ$ & $\circ$  & x&\\
    $\circ$ & $\circ$ & x&&\\
    $\circ$ &x&&&\\
    x&&&&
  \end{tabular}
\end{table}
Ho quindi $n$ check digit per un pacchetto contenente $n-1$ cifre di messaggio e
una check digit. Si ha infatti che le ci cifre di messaggio sono (per la
formula di Gauss):
\[msg=\sum_{i=1}^{n-1}i=\frac{(n-1)\cdot n}{2}\]
(in altre parole il numero di diagonali che ho nella matrice meno uno).\\
Ho inoltre che:
\[tot=msg+check=\frac{(n-1)\cdot n}{2}+n\]
Le check digit vengono calcolate dal mittente in base alle cifre sulla riga e
sulla colonna della check digit.\\
Il ricevente, per capire dove è il bit errato, verifica in quali check digit è
coinvolto, che sono due check digit, e incrociando scopro quale sia il simbolo
errato. Si ha che:
\[R_\triangle=\frac{\frac{n(n+1)}{2}}{\frac{(n-1)n}{2}}=\frac{n+1}{n-1}=
  \frac{(n-1+2)}{n-1}=1+\frac{2}{n-1}\]
Confronto questa ridondanza con quella dei migliori codici rettangolari, ovvero
quella dei codici quadrati, dove vedo che si ha un termine positivo in più. Si
ha quindi che:
\[R_\triangle<R_\square\]
e quindi i codici triangolari sono assolutamente migliori di quelle
rettangolari, anche delle migliori di quelle rettangolari anche se devo avere un
messaggio di $n$ cifre che permette l'espressione $\frac{(n-1)\cdot n}{2}$, per
fare tornare i conti posso comunque aggiungere uno 0, per esempio.
\subsection{Codici cubici}
Per cercare di abbassare ancora meno la ridondanza cerchiamo la disposizione
geometrica più conveniente. Si è provato quindi ad aumentare le dimensioni dello
spazio in cui ci troviamo, pensando ad un cubo, ipotizzando di avere una cifra
di controllo (posta in basso a destra per il piano) che controlla tutto un piano
(contando che ho piani per ognuna delle tre dimensioni).
\textbf{to be continued}
\end{document} 
% LocalWords:  clock burst

\message{ !name(critto.tex) !offset(-1154) }
