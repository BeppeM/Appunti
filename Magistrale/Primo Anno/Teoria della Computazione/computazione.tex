\documentclass[a4paper,12pt, oneside]{book}

% \usepackage{fullpage}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage[safe,extra]{tipa}
%\usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{fontspec}
\usepackage{enumerate}
\usepackage{braket}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage[cache=false]{minted}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}

\usepackage{tikz}\usetikzlibrary{er}\tikzset{multi  attribute /.style={attribute
    ,double  distance =1.5pt}}\tikzset{derived  attribute /.style={attribute
    ,dashed}}\tikzset{total /.style={double  distance =1.5pt}}\tikzset{every
  entity /.style={draw=orange , fill=orange!20}}\tikzset{every  attribute
  /.style={draw=MediumPurple1, fill=MediumPurple1!20}}\tikzset{every
  relationship /.style={draw=Chartreuse2,
    fill=Chartreuse2!20}}\newcommand{\key}[1]{\underline{#1}}
  \usetikzlibrary{arrows.meta}
  \usetikzlibrary{decorations.markings}
  \usetikzlibrary{arrows,shapes,backgrounds,petri}
\tikzset{
  place/.style={
        circle,
        thick,
        draw=black,
        minimum size=6mm,
    },
  transition/.style={
    rectangle,
    thick,
    fill=black,
    minimum width=8mm,
    inner ysep=2pt
  },
  transitionv/.style={
    rectangle,
    thick,
    fill=black,
    minimum height=8mm,
    inner xsep=2pt
    }
  } 
\usetikzlibrary{automata,positioning,chains,fit,shapes}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[space]{grffile} % For spaces in paths
\usepackage{etoolbox} % For spaces in paths
\makeatletter % For spaces in paths
\patchcmd\Gread@eps{\@inputcheck#1 }{\@inputcheck"#1"\relax}{}{}
\makeatother

\title{Teoria della Computazione}
\author{UniShare\\\\Davide Cozzi\\\href{https://t.me/dlcgold}{@dlcgold}}
\date{}

\pgfplotsset{compat=1.13}
\begin{document}
\maketitle

\definecolor{shadecolor}{gray}{0.80}
\setlist{leftmargin = 2cm}
\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}
\tableofcontents
\renewcommand{\chaptermark}[1]{%
  \markboth{\chaptername
    \ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%
\chapter{Introduzione}
\textbf{Questi appunti sono presi a lezione. Per quanto sia stata fatta
  una revisione è altamente probabile (praticamente certo) che possano
  contenere errori, sia di stampa che di vero e proprio contenuto. Per
  eventuali proposte di correzione effettuare una pull request. Link: }
\url{https://github.com/dlcgold/Appunti}.\\
\chapter{Prerequisiti di computazione}
L'informatica è costruita su una logica matematica. Il punto di partenza è stato
dettato da Turing (con la \textbf{macchina di Turing} (\textit{TM})) e questo
pensiero si è poi sviluppato nel tempo. Turing, con la sua macchina logica, ha
dimostrato che ci sono funzioni non calcolabili, verità logiche non
dimostrabili.\\ Subito dopo la macchina di Turing nasce la teoria della
\textbf{complessità   computazionale}, col fine di classificare il problemi in
base alla difficoltà delle soluzioni mediante macchine di calcolo. Tale
\textit{difficoltà} viene stimata rispetto a \textbf{spazio e tempo}. La teoria
della \textbf{complessità computazionale} si riferisce a varie \textbf{classi
  di complessità} che 
classificano, in un primo approccio, \textit{problemi decisionali} descritti da
funzioni binarie che hanno in input una stringa sull'alfabeto $\{0,1\}$ e
restituiscono un bit (o 0 o 1). Questo perché le macchine di Turing ragionano in
binario. Si ha quindi:
\[f:\{0,1\}^*\to {0,1}\]
\textbf{Esistono problemi che si è dimostrato non essere risolvibili in tempo
  efficiente.}\\
Tra le classi abbiamo i \textbf{problemi NP} e \textbf{problemi P}. Inoltre i
problemi NP sono a loro volta classificabili tra loro cercando i più difficili,
ottenendo \textbf{problemi NP-hard} e \textbf{problemi NP-complete} (esistono
varie dimostrazioni per la \textit{NP-completezza}).
\section{Tempo di calcolo di una TM}
\begin{definizione}
  Sia $T:\mathbb{N}\to \mathbb{N}$ una funzione calcolabile da TM e $L\pi$ un
  linguaggio di decisione (dove $\pi$ sta per ``problema'' e ``di decisione'' ci
  ricorda che il risultato sarà binario) allora una \textbf{TM deterministica}
  $M$ accetta L$\pi$ in tempo $T(n)$ se, $\forall x\in L\pi$, con $|x|=n$, $M$
  accetta $x$ in $T(n)$ mosse o configurazioni
\end{definizione}
\begin{definizione}
  Un \textbf{problema di decisione} $\pi$ riceve in input un'istanza $x$ e
  l'output è:
  \begin{itemize}
    \item 0 che vuole dire \textit{no}
    \item 1 che vuole dire \textit{yes}
  \end{itemize}
  Un linguaggio $L\pi$ restituisce 1 per tutti gli $x$ che appartengono al
  linguaggio. Quindi $L\pi$ è l'insieme degli input di $\pi$ su cui l'output è
  1 (è l'analogo della \emph{funzione caratteristica} di un insieme, ovvero la
  funzione che risponde 1 sse un certo elemento appartiene all'insieme di
  riferimento).\\
  La \textbf{funzione associata al problema} si chiama $f\pi$ ed è la funzione
  che dato un input restituisce 1 sse l'input appartiene al $L\pi$.
\end{definizione}
Approfondiamo ora lo studio della \textbf{classe P}.
\begin{definizione}
  La classe dei linguaggi di decisione accettati in tempo
  $T(n)=cn^p,\,p\in\mathbb{N},\, p\neq 0$ da una TM deterministica è detta
  \textbf{classe P}, quindi in un tempo polinomiale sulla dimensione dell'input
  $n$, è detta \textbf{classe P}. Quindi P è una classe di \textbf{problemi di
    decisione}. \\
  Potenzialmente $p$ potrebbe anche non essere un intero in quanto si potrebbero
  avere tempi frazionari.
\end{definizione}
\begin{definizione}
  Si definisce che $L\pi$ è accettato da una TM in tempo $T(n)$ se $\exists
  \,\,T :\mathbb{N}\to \mathbb{N}$ calcolabile da TM e $\forall x\in L\pi$, con
  $|x|=n$, la TM accetta $x$ e risponde 1 (\textit{yes}) in al più $T(n)$ mosse
  di calcolo (dette anche configurazioni).\\
  Nel caso del modello della macchina RAM si ha la stessa situazione con però
  $T(n)$ \textbf{istruzioni RAM} e si dice che $L\pi$ è accettato dalla macchina
  RAM (si può dire che è anche deciso dell'algoritmo A della macchina RAM). In
  caso contrario la macchina RAM restituisce \textit{no}, in quanto si parla di
  ``decisione'' oltre che di ``accettazione'' (a differenza della TM, dove però
  si può ottenere lo stesso discorso parlando di TM complementare $M'$, che in
  $T(n)$ mi risponderà yes alla richiesta che un input non appartenga a
  $L\pi$, altrimenti bisogna fissare un limite di tempo per ottenere yes).\\
  È dimostrabile che se $L\pi$ è accettabile in tempo polinomiale allora nello
  stesso tempo è anche decidibile.\\
  La differenza tra accettazione e decisione sarà fondamentale nel
  \textbf{modello non deterministico}.
\end{definizione}
\begin{shaded}
  Si ricordi che il \textbf{modello RAM (\textit{Random Access Machine})} è
  usato per studiare il tempo di calcolo di 
  uno pseudocodice. È un modello teorico (una macchina teorica ``simile'' a
  quelle reali) dotato di istruzioni come
  \textit{load, store, add, etc$\ldots$} dove un codice (ipoteticamente in
  qualsiasi linguaggio incluso lo pseudocodice) viene tradotto in una sorta di
  linguaggio macchina (linguaggio RAM), dove $n$ è un intero rappresentante il
  numero di istruzioni RAM necessarie per ottenere l'output ($n$ è detto
  \textbf{tempo uniforme}). Sul linguaggio RAM si può studiare anche lo spazio
  calcolato come numero di bit necessari per la computazione (è detto
  \textbf{costo logaritmico}). In questo secondo punto il costo di
  un'istruzione, come ad esempio \textit{load(n)}, è logaritmico rispetto
  all'operando $n$ ($\,\log_2 n$), studia quindi la \emph{dimensione}
  dell'input.
\end{shaded}
Consideriamo ora un modello basato su algoritmi.
\begin{definizione}
  Sia $L\pi$ un linguaggio di decisione e $t:\mathbb{N}\to\mathbb{N}$ una
  funzione calcolabile, allora un algoritmo $A$ accetta $L\pi$ in tempo $T(n)$
  sse $\forall x\in L\pi$, con $|x|=n$, $A$ termina su input $x$ dopo $T(|x|)$
  passi di calcolo, ovvero istruzioni esguite, producendo 1 come output. Quindi
  P è la classe dei linguaggi di decisione accettati in tempo
  $T(n)=cn^p,\,p\in\mathbb{N}$ (con lo stesso discorso di sopra su $p$) da un
  algoritmo $A$ in tempo polinomiale.
\end{definizione}
\chapter{Complessità computazionale}
Si cerca di catalogare dal punto di vista computazionale i \textbf{problemi
  intrattabili}, ovvero problemi risolvibili ma non in modo \textbf{efficiente}
(ovvero in tempo polinomiale). In alcuni casi si pensa che non esista una
soluzione ma non si hanno dimostrazioni in merito mentre in altri casi è
addirittura dimostrato. Abbiamo quindi delle categorie informali per i problemi:
\begin{itemize}
  \item \textbf{facili}, so risolverli in modo efficiente. È la \textbf{classe P}
  \item \textbf{difficili} o, più formalmente, \textbf{intrattabile}, so
  risolverli ma non in modo efficiente e non ho una 
  dimostrazione che mi assicuri che non siano risolvibili in modo efficiente. È
  la \textbf{classe NP} e la sua sottoclasse \textbf{NP-complete}
  \item \textbf{dimostrabilmente difficili}, so risolverli ma so che non esiste
  un algoritmo efficiente in quanto è stato dimostrato che non può esistere
  \item \textbf{impossibili}, non so risolverli sempre neanche in modo non
  efficiente (esiste almeno un input che manda in crisi l'algoritmo ma esiste
  almeno un caso in cui funzioni)
\end{itemize}
Come formalismo useremo la \textbf{Macchina di Turing (\textit{TM})},
\textit{deterministica} e \textit{non deterministica}. Analizzeremo in primis
\textbf{problemi sui grafi}. Un grafo è definito come $G=(V,E)$, con $V$ insieme
dei vertici e $E$ insieme degli archi. Un grafo può essere \textit{orientato} o
\textit{non orientato}. Un \textbf{cammino} tra due vertici è una sequenza di
archi che mi porta da un vertice all'altro. Un cammino è detto \textbf{ciclo} se
il vertice sorgente coincide con quello di destinazione. Due vertici sono
\textbf{connessi} se esiste un cammino che li collega. Un \textbf{grafo
  connesso} è un grafo dove per ogni coppia di vertici si ha che essi sono
connessi. Se questo cammino è di un solo arco si parla di \textbf{grafo
  completo}, ovvero ogni vertice è \textbf{adiacente} ad ogni altro. Si parla di
\textbf{grafo pesato} se si ha una funzione $W$ che associa un peso ad ogni
arco. \\
Useremo anche la teoria dei linguaggi formali con $V$ alfabeto e stringhe
costruite su $V$. Con $\varepsilon$ abbiamo la stringa vuota e con $V^*$ è
l'insieme di tutte le possibili stringhe costruibili con quell'alfabeto, inclusa
la stringa vuota. $V^*$ è un insieme infinito. Con $V^+$ indico
$V^*/\varepsilon$, ovvero senza la stringa vuota. Un \textbf{linguaggio} $L$ è
un sottoinsieme di $V^*$, quindi $L\subseteq V^*$, che comprende tutti gli
elementi di $V^*$ che seguono una certa \textbf{proprietà} (o più
proprietà). Anche $L$ è un insieme infinito.\\
Un'altra nozione è quella di \textbf{problema}. Un problema computazionale è una
``questione'' a cui si cerca risposta. Più formalmente un problema è specificato
da \textbf{parametri} (l'input del problema) e le \textbf{proprietà} che deve
soddisfare la \textbf{soluzione} (l'output). L'\textbf{istanza} di un problema
specificando certi parametri in input al problema (input che devono essere
coerenti ai parametri richiesti).\\
Cominciamo con degli esempi di problemi comunque risolvibili.
\begin{esempio}
  Considero il problema \emph{arco minimo}. Come parametro ho un grafo pesato
  sugli archi $G=(V,E)$. Le proprietà della soluzione è che voglio l'arco con
  peso minimo.\\
  Per risolvere guardo tutti gli archi è vedo quello di peso minimo. Dato che
  basta iterare su tutti gli archi quindi la soluzione è in $O(n)$ (in realtà
  $\Theta(n)$), quindi in \textbf{tempo lineare} sul numero di archi (è quindi
  in \textbf{tempo polinomiale})
\end{esempio}
\begin{esempio}
  Considero il problema \emph{raggiungibilità}. Come parametro ho un grafo non
  pesato $G=(V,E)$ e due vertici, uno sorgente e uno destinazione, tali che
  $v_s,v_d\in V$. Le proprietà della soluzione è che voglio sapere se posso
  arrivare a $v_d$ partendo da $v_s$.\\
  Per risolvere studio tutti i cammini che partono da $v_s$ e posso dare la
  risposta. Una soluzione del genere è in tempo $O(2^{|E|})$. Il tempo quindi
  cresce in \textbf{modo esponenziale}. Una soluzione migliore è quella di usare
  un \textbf{algoritmo di visita} che richiede tempo $O(|V|+|E|)$, ovvero un
  \textbf{tempo polinomiale}. Quindi per quanto all'inizio si pensi
  che sia un \textbf{problema intrattabile} si scopre che è un \textbf{problema
    facile} 
\end{esempio}
\begin{esempio}
  Considero il problema \emph{TSP}. Come parametro ho un grafo pesato sugli
  archi e completo $G=(V,E)$. Le proprietà della soluzione è che voglio sapere
  il \emph{cammino minimo} (in realtà un ciclo) che tocca tutti i vertici una e
  una sola volta (una volta trovata la soluzione non mi interessa la sorgente
  essendo il grafo completo). \\
  Sarebbe facile determinare \textbf{un} ciclo ma non quello di peso minimo e
  per farlo devo trovare tutti i cicli e trovare quello di peso minimo. Ho
  quindi un algoritmo che è $O(2^n)$ (nella realtà è circa $O(n!)$ che è
  comunque esponenziale per l'\textbf{approssimazione di Stirling}). In questo
  caso non si riesce a pensare ad una soluzione che non sia esponenziale nel
  tempo (anche se per alcuni input sia di facile risoluzione, basti pensare ad
  avere tutti gli archi di peso 1, ma mi basta avere un input problematico). Non
  potendo però dimostrare che sia irrisolvibile si dice che è un
  \textbf{problema intrattabile}. \textit{TSP} è uno dei 10 problemi famosi per
  i quali ti danno un milione di dollari se dimostri che è o \emph{facile} o
  \emph{impossibile}
\end{esempio}
Per completezza definiamo un \textbf{algoritmo} come una sequenza di
\textbf{istruzioni elementari} (supportate dal calcolatore) che, eseguite in
sequenza, mi portano alla soluzione di un problema. Si ha quindi che un
algoritmo $A$ risolve un problema $\Pi$ se per ogni possibile istanza di $\Pi$
l'algoritmo $A$ mi da la risposta corretta. Distinguo però:
\begin{itemize}
  \item \textbf{algoritmo efficiente}, che mi da la soluzione in \textbf{tempo
    polinomiale} rispetto alla \textbf{dimensione dell'input}. Ho un
  \textit{caso peggiore} limitato superiormente da un \textbf{polinomiale}:
  $O(p(n))$. Ho una crescita di tempo accettabile all'aumentare
  dell'input. Diciamo comunque che è dura anche solo raggiungere $O(n^{10})$
  quindi anche se dire polinomiale potrebbe voler dire $O(n^{10000000})$ non si
  hanno casi reali di questo tipo
  \item \textbf{algoritmo non efficiente}, che mi da la soluzione ma in tempo
  superiore a quello \textbf{polinomiale}. Ho un \textit{caso peggiore} limitato
  superiormente da un \textbf{esponenziale}: $O(2^n)$. Ho una crescita di tempo
  assolutamente non accettabile (esponenziale appunto) all'aumentare dell'input 
\end{itemize}
\textit{Se ho anche solo un caso di input che porta a tempo esponenziale ho
  comunque un algoritmo non efficiente}.\\
Spesso \textbf{problemi intrattabili} vengono risolti tramite approssimazioni
per arrivare ad una soluzione accettabile anche se non la migliore ma non sempre
è possibile effettuare delle approssimazioni.\\
Anche se avessi ha che fare con un computer mille volte migliore di quelli
attuali, un problema esponenziale avrà comunque tempi non accettabili in
proporzione ad un problema polinomiale. Quindi non sarà il miglioramento
hardware a permettere di rendere accettabile la soluzione di problemi
esponenziali.\\
Un \textbf{algoritmo intrattabile} quindi non risolve in modo efficiente tutti
gli input. Bisognerà trovare un modo per capire se un algoritmo è
\textbf{intrattabile} per davvero.\\
Studiamo ora il tempo di calcolo di una \textbf{macchina di Turing non
  deterministica (\textit{NTDM})}:
\begin{definizione}
  Sia $T:\mathbb{N}\to\mathbb{N}$ una funziona calcolabile da TM. Dato $L\pi$ un
  linguaggio di decisione allora una $NDTM$ $M$. $M$ accetta $L\pi$ in tempo
  $T(n)$ se per ogni $x$ in $L\pi$ ,con $|x|=n$,  $M$ accetta $x$ in $T(n)$
  mosse (o configurazioni).\\
  Quindi la \textbf{classe NP} è la classe dei linguaggi di decisione accettati
  in tempo $T(n)=cn^p,\,\,\,p\in \mathbb{N}$ da una $NDTM$
\end{definizione}
Diamo una definizione alternativa di NP:
\begin{definizione}
  Sia $L\pi$ un linguaggio di decisione, $N:n\to n$ una funzione calcolabile,
  $y$ una stringa di lunghezza polinomiale nell'input, allora un algoritmo $A$
  con ``certificato'' accetta $L\pi$ in tempo $t(n)$ se per ogni $x$ in $L\pi$,
  con $|x|=n$, A termina su input $(x,y)$ dopo $t(|x|)$ passi di calcolo
  (istruzioni eseguite) producendo 1 in output.
  Quindi la \textbf{classe NP} è la classe dei linguaggi (o problemi) di
  decisione accettati in tempo $T(n)=cn^p,\,\,\,p\in\mathbb{N}$ da un algoritmo
  $A$ ``certificato''
\end{definizione}
Resta da capire il significato del termine ``certificato''.
\begin{definizione}
  Preso un algoritmo $A$ definiamo cosa significa che sia ``certificato'' si
  compone di un input $(x,y)$ con $y$ che è una stringa, nel dettaglio una
  \textbf{dimostrazione}, che garantisce che $x\in L\pi$.
\end{definizione}
Vediamo un esempio:
\begin{esempio}
  Uso un problema NP come esempio, quindi $\pi\in NP$, per avere un algoritmo
  che ammette certificato (non ho alternative). \\
  Prendo il problema $\pi$ \emph{vertex-cover}. Come input si ha un grafo
  $G=(V,E)$ e un intero $k$. Come output ho o 1 o 0, essendo un problema di
  decisione, e si cerca di capire se $\exists\, V'\subseteq V$ tale che $V'$ è
  una copertura di $G$. Il sottoinsieme è copertura di un grafo quando $forall\,
  e\in E$ almeno un estremo dell'arco $e=(u,v)$ è in $V'$. La copertura con il
  minor numero di vertici è detta \textbf{minima copertura}.\\
  Il problema \emph{vertex-cover} chiede se esiste una copertura del grafo di
  dimensione $k$. È quindi un \textbf{problema di decisione}. Qualora trovassi
  una copertura di cardinalità minore di $k$ mi basterà aggiungere vertici
  arbitrari fino al raggiungimento di $k$. Ovviamente può non esistere una
  copertura di cardinalità $k$.\\
  Il problema di \emph{vertex-cover} è il problema di decisione del problema di
  trovare la minima copertura di un grafo, che è un \textbf{problema di
    ottimizzazione (o problema di ottimo)} (ogni problema di ottimizzazione può
  essere trasformato in uno di decisione aggiungendo un parametro $k$ e
  richiedendo un risultato booleano).\\
  \emph{vertex-cover} decisionale è nella classe \textbf{NP} con ``certificato''
  e, in questo caso, il parametro $y$ è la dimostrazione che posso dare risposta
  affermativa, per esempio la certezza di avere un sottoinsieme di vertici che
  effettivamente copre tutto il grafo, quindi $y=V''\subseteq V$ tale per cui
  $V''$ copre $G$ con cardinalità $k$. Bisogna capire come trovare e come usare
  $y$, sapendo che $A$ lavora in tempo polinomiale e che la lunghezza di $y$ è
  polinomiale in dimensione di $x$.\\ 
  Vedendo che la cardinalità di $y$ è minore di $k$ l'algoritmo A verifica che
  con i vertici passati con $y$ si è in grado di rispondere al problema in modo
  affermativo, problema che ha in input $G$ e $k$. In poche parole $A$, per ogni
  arco, esamina se ogni estremo è in $y$ e, se tutti gli archi hanno un estremo
  in $y$ allora si ha che usando $y$ si è in grado di verificare l'input $G,
  k$ è \textbf{accettato}. Questa verifica è in tempo polinomiale e si ha che
  $|y|=O(|G,k|)$, soprattutto se $|y|$ è una costante.\\
  Ad oggi non si è dimostrato che \emph{vertex-cover} si risolvibile in tempo
  polinomiale. È quindi un problema \textbf{NP-hard}.
\end{esempio}
\begin{esempio}
  Per l'algoritmo TSP la versione di ottimo è trovare il \emph{tour} di costo
  minimo. Il problema di decisione è se esiste un \emph{tour} di massimo peso
  $k$. In questo caso già il problema di decisione è già in \textbf{NP} e lo è
  anche il problema di ottimo. La verifica con ``certificato'' ha comunque costo
  polinomiale nel caso di richiesta di verifica di un dato \emph{tour} con costo
  minore di $k$. 
\end{esempio}
\textbf{Il problema di decisione è una restrizione di quello di ottimo.}\\
Per notazione dato un algoritmo $A$ chiamiamo $A_d$ la sua versione di
decisione.\\

\textbf{Senza ``certificato'' non potrei accettare un problema NP}.\\
Un problema di \textbf{classe NP} quindi ammette verifica in tempo polinomiale,
ammettendo un ``verificatore'' in tempo polinomiale per i problemi
specificati.\\
Non è così scontato trovare ``certificato'', di dimensione polinomiale
nell'input, e trovare il ``verificatore''. Per esempio la classe
\textbf{exptime} non esiste $A$ con ``certificato'' che sia polinomiale (la
verifica potrebbe richiedere tempo esponenziale).\\
Quindi per un problema \textbf{NP} con ``certificato'' non posso trovare
soluzione in tempo polinomiale ma posso verificare una soluzione data in tempo
polinomiale.\\
Posso quindi dimostrare che un problema $\pi$, con in input una struttura
combinatoria discreta (come un grafo) e un intero, è in \textbf{NP}.\\
Si ha quindi che \textbf{P} è vista come sottoclasse la \textbf{NP} dove i
problemi di decisione sono risolvibili in tempo polinomiale anche senza
``certificato''. Per dimostrare che $P\subseteq NP$ devo far vedere che ogni
$L\pi\in P$ ammette un algoritmo $A$ con ``certificato'' in tempo
polinomiale. Ma questo è vero perché $L\pi$ è accettato da un algoritmo $A$, in
tempo polinomiale con $y=\emptyset$ e quindi $y$ non è necessario.\\
Si ha quindi che $P\subseteq NP$. Pensando poi, per esempio,
all'\textit{isomorfismo di grafi} nessuno sa se sia $P$ o $NP$. Questo
problema ha in input due grafi e come output se sono isomorfi tra loro.
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.3]{img/problem.png}
  \caption{Diagramma di \emph{Eulero Venn} per le classi di complessità}
  \label{fig:complexity}
\end{figure}
\begin{shaded}
  \textit{Tratto dagli appunti di Metodi Formali:}\\
\begin{center}
  \textit{Si parla di isomorfismo quando due strutture complesse si possono
    applicare l'una sull'altra, cioè far corrispondere l'una all'altra, in modo
    tale che per ogni parte di una delle strutture ci sia una parte
    corrispondente nell'altra struttura; in questo contesto diciamo che due
    parti sono corrispondenti se hanno un ruolo simile nelle rispettive
    strutture.}
\end{center}
Diamo ora una definizione formale di isomorfismo tra sistemi di transizione
etichettati, che possono quindi essere grafi dei casi o grafi dei casi
sequenziali.
\begin{definizione}
  Siano dati due sistemi di transizione etichettati:\\
  $A_1 = (S_1,E_1,T_1,s_{01})$ e $A_2 = (S_2 , E_2 , T_2 , s_{02})$.\\
  e siano date due \textbf{mappe biunivoche}:
  \begin{enumerate}
    \item $\alpha:S_1\to S_2$, ovvero che passa dagli stati del primo sistema a
    quelli del secondo
    \item $\beta:E_1\to E_2$, ovvero che passa dagli eventi del primo sistema a
    quelli del secondo
  \end{enumerate}
  allora:
  \[\langle \alpha,\beta\rangle:A_1= (S_1 , E_1 , T_1 ,s_{01})\to A_2 = (S_2 ,
    E_2 , T_2 , s_{02})\]
  è un \textbf{isomorfismo} sse:
  \begin{itemize}
    \item $\alpha(s_{01})=s_{02}$, ovvero l'immagine dello stato iniziale del
    primo sistema coincide con lo stato iniziale del secondo
    \item $\forall s,s'\in S_1,\forall e\in E_1:\,(s,e,s')\in T_1
    \Leftrightarrow (\alpha(s),\beta(e),\alpha(s'))\in T_2$ ovvero per ogni
    coppia di stati del primo sistema, tra cui esiste un arco etichettato $e$,
    vale che esiste un arco, etichettato con l'immagine di $e$, nel secondo
    sistema che va dall'immagine del primo stato considerato del primo sistema
    all'immagine del secondo stato considerato del secondo sistema, e viceversa
  \end{itemize}
\end{definizione}
\begin{definizione}
  Si definiscono due \textbf{sistemi equivalenti} sse hanno grafi dei casi
  sequenziali, e quindi di conseguenza anche grafi dei casi, \emph{isomorfi}.\\
  Due sistemi equivalenti accettano ed eseguono le stesse sequenze di eventi
\end{definizione}
\end{shaded}
\section{Riduzioni polinomiali}
Si hanno alcuni problemi che sono in grado di risolvere qualunque problema di
decisione in \textbf{NP}. Serviranno prima le definizioni di \textbf{NP-hard} e
\textbf{NP-complete}.\\
Vediamo innanzitutto il problema \textit{independent-set} che ci aiuterà
analisi.
\begin{definizione}
  L'\textit{independent-set } di un grafo non orientato è un sottoinsieme
  $I\subseteq V$ tale che $\forall u,v\in I$ $(u,v)\not\in E$. Il problema
  \textit{ind\_set}, nella versione di ottimo, è quello di trovare
  l'\textit{independent-set} di cardinalità massima di un grafo non
  orientato. Nella versione di decisione $ind\_set_d$ si ha anche il parametro
  $k$ intero e si cerca se esiste un \textit{independent-set} di cardinalità
  uguale a $k$. L'\textit{independent-set} di cardinalità massima può essere
  usato come ``certificato''.
\end{definizione}
Questo problema è legato alla \textit{copertura dei vertici}, infatti sappiamo
che se dall'insieme dei vertici togliamo un sottoinsieme di minima copertura
troviamo un \textit{independent-set} di cardinalità massima perché sto facendo
il complemento di un insieme di copertura di cardinalità minima, infatti tra i
vertici non nell'insieme di copertura di cardinalità minima, ovvero nel
complemento, non posso avere un arco per definizione e quindi se il primo è di
cardinalità minima allora il secondo, che è l'\textit{independent-set}, è di
cardinalità massima. Infatti i vertici nella copertura sono vertici che toccano
tutti gli archi.\\
Dal punto di vista delle applicazioni pratiche questi problemi si prestano allo
studio, per esempio, delle telecomunicazioni.
\begin{proof}
  Dimostriamo che $ind\_set_d$ è \textbf{NP}, infatti esiste un algoritmo $A$
  che in costo polinomiale prende in ingresso il grafo, $k$, e un
  ``certificato'' $y$ e i vertici in $y$, che sono vertici di un
  \textit{independent\_set} per il grafo $G$ di cardinalità $k$. L'algoritmo
  verifica che $y$ è un \textit{independent-set} e il costo della verifica è
  quadratico su $|y|$, ovvero $|y|^2$ che nel caso peggiore è $|V|^2$. So anche
  che, per l'input $x$, $O(|x|)=O(|E|+|V|)=O(|V^2|+|V|)$ nel caso peggiore,
  quindi il tempo di verifica è \textbf{polinomiale}.
\end{proof}
\begin{definizione}
  Vediamo ora il problema di \textbf{soddisfacibilità} $SAT$. Questo problema
  prende in input una formula booleana $\phi$ in \textbf{forma normale congiunta
    (CNF)}, ovvero che ha una congiunzione ($\land$) come legame tra le
  \textbf{clausole}. Una clausola è un $\lor$ di \textbf{letterali}, ovvero di
  variabili booleane $x_i$ o $\neg x_i$. In output ho se la forma sia
  soddisfacibile o meno. 
  \begin{esempio}
    Prendo 3 variabili, $x_1,x_2,x_3$. Creo i letterali  $x_1,x_2,x_3$ e anche
    $\neg x_1,\neg x_2,\neg x_3$. Creo quindi le clausole $c_1=x_1\lor x_2$,
    $c_2=x_1\lor \neg x_2$ e $c_3=x_1\lor \neg x_2$. Definisco quindi la CNF
    $\phi$:
    
    \[\phi=(x_1\lor x_2)\land (x_1\lor \neg x_2)\land
      (x_1\lor \neg x_2)=c_1\land c_2\land c_3\]
    Quindi in ogni clausola almeno un letterale deve essere vero, cosicché tutte
    le clausole siano vere rendendo vera la CNF.\\
    Avendo due letterali a clausola si è definito un $2SAT$.
  \end{esempio}
  Il numero di letterali $k$ che compongono la clausola definsice un problema
  $kSAT$. Si ha che $2SAT\in P$ ma con $k>2$ si ha che $kSAT\in NP$ (in
  realtà è in \textbf{NP-hard})
\end{definizione}
\begin{definizione}
  Definiamo un problema \textbf{NP-hard} come un problema difficile almeno
  quanto un problema \textbf{NP}. Ogni che problema $A$ in \textbf{NP} può
  essere risolto con un chiamata di procedura a $B$, che è un problema
  \textbf{NP-hard} a cui tutti gli altri ``chiedono aiuto'' per trovare una
  soluzione. \\
  Ad esempio \textit{vertex-cover} è un problema \textbf{NP-hard} e quindi posso
  risolvere ogni problema $A$ in \textbf{NP} con il problema
  \textit{vertex-cover} $B$.\\
  Trasformo quindi l'input $w$ di $A$ in un input
  $f(w)$ per $B$ in tempo polinomiale. La risposta di $B$ con input $f(w)$ è la
  stessa che $A$ da su input $w$.\\
  Non tutti i problemi \textbf{NP-hard} sono dentro la classe \textbf{NP}
\end{definizione}
\begin{definizione}
  Definiamo quindi il concetto di \textbf{riduzione}, rappresentato in figura
  \ref{fig:rid}.\\
  La riduzione è la trasformazione dell'input di $w$ in $A$ in un input $f(w)$
  per $B$, in tempo polinomiale. La risposta di $B$ con input $f(w)$ è la stessa
  che $A$ da su input $w$.\\
  Si ha che $A$ si riduce polinomialmente a $B$, e si scrive:
  \[A\leq_p B\]
  se $\exists\,f$ tale che:
  \[w \in L_A\mbox{ sse } f(w)\in L_B\]
  con $f$ calcolabile in tempo polinomiale, infatti il calcolo di $f(w)$ è
  $=(|w|^p)$, con $p\in\mathbb{N}$ per semplicità. Non devo introdurre una
  complessità superiore nel contesto di confronto tra problemi (??).\\ 
  \begin{figure}
    \centering
    
    \psscalebox{0.9 0.9} % Change this value to rescale the drawing.
    {
      \begin{pspicture}(0,-2.8)(15.0,2.8)
        \definecolor{colour1}{rgb}{0.34901962,0.6627451,0.3019608}
        \definecolor{colour0}{rgb}{0.8784314,0.20392157,0.20392157}
        \definecolor{colour2}{rgb}{0.3254902,0.38039216,0.99607843}
        \psframe[linecolor=colour1, linewidth=0.04, dimen=outer]
        (13.52,2.8)(1.52,-2.8)
        \psframe[linecolor=colour0, linewidth=0.04, dimen=outer]
        (6.32,1.2)(2.72,-1.2)
        \psframe[linecolor=colour0, linewidth=0.04, dimen=outer]
        (6.32,1.2)(2.72,-1.2)
        \psframe[linecolor=colour2, linewidth=0.04, dimen=outer]
        (11.52,1.2)(7.92,-1.2)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(6.32,0.0)(7.92,0.0)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(0.72,0.0)(2.72,0.0)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(11.52,0.4)(14.32,0.4)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(11.52,-0.4)(14.32,-0.4)
        \rput[bl](3.7,1.3){Riduzione}
        \rput[bl](0.0,-0.14){$w$}
        \rput[bl](4.1,-0.08){$f(w)$}
        \rput[bl](9.42,-0.08){$B$}
        \rput[bl](12.26,0.6){yes}
        \rput[bl](14.42,0.25){yes}
        \rput[bl](12.34,-0.74){no}
        \rput[bl](14.42,-0.5){no}
        \rput[bl](9.24,1.3){$SAT$}
        \rput[bl](6.72,0.4){$f(w)$}
      \end{pspicture}
    }
    \caption{Rappresentazione grafica della riduzione}
    \label{fig:rid}
  \end{figure}
  Ogni problema $A$ che in \textbf{NP} può essere risolto con una chiamata di
  procedura a $B$, quindi posso risolvere ogni problema $A\in NP$ con
  \textit{vertex-cover}, essendo esso un problema \textbf{NP-hard}.
  
\end{definizione}
\begin{definizione}
  $B$ è \textbf{NP-hard} sse $\forall A\in NP$ $A$ si riduce a $B$ in tempo
  polinomiale:
  \[A\leq_p B,\,\,\forall A\]
  Un problema $NP-hard$ può non essere in $NP$, in quanto potrebbe non avere un
  ``certificato'' per consentire la verifica in tempo polinomiale.
\end{definizione}
\begin{definizione}
  Un problema \textbf{NP-hard} e anche \textbf{NP} si dice che il problema è
  \textbf{NP-complete} 
\end{definizione}
$kSAT$ è il primo problema che si è dimostrato essere anche
\textbf{NP-complete}.
\begin{esempio}
  Vediamo un esempio di riduzione, rappresentata in figura \ref{fig:ride}:
  \[3SAT\leq_p ind\_set_d\]
  arrivando e alla conclusione che $ind\_set_d$ è \textbf{NP-completo}.\\
  \begin{figure}
    \centering
    
    \psscalebox{0.9 0.9} % Change this value to rescale the drawing.
    {
      \begin{pspicture}(0,-2.8)(15.0,2.8)
        \definecolor{colour1}{rgb}{0.34901962,0.6627451,0.3019608}
        \definecolor{colour0}{rgb}{0.8784314,0.20392157,0.20392157}
        \definecolor{colour2}{rgb}{0.3254902,0.38039216,0.99607843}
        \psframe[linecolor=colour1, linewidth=0.04, dimen=outer]
        (13.52,2.8)(1.52,-2.8)
        \psframe[linecolor=colour0, linewidth=0.04, dimen=outer]
        (6.32,1.2)(2.72,-1.2)
        \psframe[linecolor=colour0, linewidth=0.04, dimen=outer]
        (6.32,1.2)(2.72,-1.2)
        \psframe[linecolor=colour2, linewidth=0.04, dimen=outer]
        (11.52,1.2)(7.92,-1.2)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(6.32,0.0)(7.92,0.0)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(0.72,0.0)(2.72,0.0)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(11.52,0.4)(14.32,0.4)
        \psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,
        arrowlength=1.4,arrowinset=0.0]{->}(11.52,-0.4)(14.32,-0.4)
        \rput[bl](3.7,1.3){Riduzione}
        \rput[bl](0.35,-0.15){$\phi$}
        \rput[bl](4.1,-0.08){$f(\phi)$}
        \rput[bl](9.42,-0.08){$B$}
        \rput[bl](14.42,0.25){1}
        \rput[bl](14.42,-0.5){0}
        \rput[bl](9.0,1.3){$ind\_set_d$}
        \rput[bl](6.85,0.2){$G_\phi$}
      \end{pspicture}
    }
    \caption{Rappresentazione grafica del'esempio \ref{es:1}}
    \label{fig:ride}
  \end{figure}
  e vediamo che $\phi$ è soddisfacibile sse $G_\phi$ ha un
  \textit{independent-set} di dimensione $k=|\phi|$, con $|\phi|$ pari al numero
  di clausole della formula.\\
  Costruisco quindi un grafo che ha un vertice per ogni letterale della
  clausola. Collego i tre letterale della clausola ottenendo un
  ``triangolo'', detto \textit{gadget}, che rappresenta una clausola. Infine
  collego ogni letterale al suo negato.
  \newpage
  Quindi per la formula:
  \[\phi=(\neg x_1\lor x_2\lor x_3)\land(x_1\lor \neg x_2\lor x_3)\land(\neg
    x_1\lor x_2\lor x_4)\]
  avrò il grafo $G_\phi$ che codifica la formula $\phi$:
  \begin{figure}[H]
    \centering
    \psscalebox{1.0 1.0} % Change this value to rescale the drawing.
    {
      \begin{pspicture}(0,-1.5798438)(12.92,1.5798438)
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](1.6,0.9050586){0.4}
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](0.4,-0.6949414){0.4}
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](2.8,-0.6949414){0.4}
        \psline[linecolor=black, linewidth=0.02](1.6,0.5050586)(2.4,-0.6949414)
        \psline[linecolor=black, linewidth=0.02](1.6,0.5050586)(0.8,-0.6949414)
        \psline[linecolor=black, linewidth=0.02](0.8,-0.6949414)(2.4,-0.6949414)
        \rput[bl](1.2,1.3050586){$\neg x_1$}
        \rput[bl](2.6,-1.4949414){$x_3$}
        \rput[bl](0.2,-1.4949414){$x_2$}
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](6.4,0.9050586){0.4}
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](5.2,-0.6949414){0.4}
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](7.6,-0.6949414){0.4}
        \psline[linecolor=black, linewidth=0.02](6.4,0.5050586)(7.2,-0.6949414)
        \psline[linecolor=black, linewidth=0.02](6.4,0.5050586)(5.6,-0.6949414)
        \psline[linecolor=black, linewidth=0.02](5.6,-0.6949414)(7.2,-0.6949414)
        \rput[bl](6,1.3050586){$\neg x_2$}
        \rput[bl](7.4,-1.4949414){$x_3$}
        \rput[bl](5.0,-1.4949414){$x_1$}
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](11.2,0.9050586){0.4}
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](10.0,-0.6949414){0.4}
        \pscircle[linecolor=black, linewidth=0.04, fillstyle=solid,
        fillcolor=black, dimen=outer](12.4,-0.6949414){0.4}
        \psline[linecolor=black,linewidth=0.02](11.2,0.5050586)(12.0,-0.6949414)
        \psline[linecolor=black,linewidth=0.02](11.2,0.5050586)(10.4,-0.6949414)
        \psline[linecolor=black,linewidth=0.02]
        (10.4,-0.6949414)(12.0,-0.6949414)
        \rput[bl](10.8,1.3050586){$\neg x_1$}
        \rput[bl](12.2,-1.4949414){$x_4$}
        \rput[bl](9.8,-1.4949414){$x_2$}
        \psline[linecolor=black, linewidth=0.02](0.8,-0.6949414)(6.0,0.9050586)
        \psline[linecolor=black, linewidth=0.02](2.0,0.9050586)(4.8,-0.6949414)
        \psline[linecolor=black, linewidth=0.02](5.6,-0.6949414)(10.8,0.9050586)
        \psline[linecolor=black, linewidth=0.02](6.8,0.9050586)(9.6,-0.6949414)
      \end{pspicture}
    }
    \label{es:1}
  \end{figure}
  Quindi un \textbf{gadget} è una rappresentazione dell'input del problema $A$
  di partenza e ogni \textbf{gadget} rappresenta una clausola.\\
  Ricordiamo che $\phi$ è soddisfacibile sse esiste un assegnamento delle
  variabili della formula tale per cui almeno un letterale di ogni clausola è
  vero. Nel grafo relativo alla formula lego quindi un letterale ad ogni suo
  complemento al fine di poter identificare i valori di verità e avendo il
  calcolo di \textit{independent-set} (per la \textbf{riduzione}) prendo uno
  solo degli estremi di un arco, e quindi uno solo tra $x_i$ e $\neg x_i$,
  codificando l'assegnamento di verità. Il concetto di \textbf{riduzione} è
  quindi ritrovabile nella capacità di rappresentare un problema in un'altra
  forma, studiabile con un altro algoritmo.
  \begin{proof}
    Indichiamo con $A$ $3SAT$ e con $B$ \textit{independent-set}.\\
    Effettuiamo quindi la prova finale, la dimostrazione vera e propria. A
    questo livello di comprensione abbiamo dimostrato
    l'esistenza della funzione $f$, che trasforma $\phi$ (ovvero l'input di $A$)
    in $\langle G, k\rangle$, ovvero l'input di $B$, e che essa è in tempo
    polinomiale. Abbiamo quindi che $w\in L_A$ sse $f(w)\in L_B$. Se $\phi$ è
    vera allora esiste un \textit{independent-set} di dimensione $k$ per
    $\langle G, k\rangle$.\\ 
    Nell'\,''altro verso'' abbiamo che se esiste un \textit{independent-set} di
    dimensione $k$ per $\langle G, k\rangle$ allora $\phi$ è vera. Dimostrare
    questi due ``versi'' equivale a dimostrare la riduzione.\\
    Il \textbf{primo verso} si dimostra dicendo che dato un assegnamento di
    verità si seleziona un letterale vero da ogni triangolo. Questi letterali
    veri scelti formano l'\textit{independent-set} $S$, che ha dimensione
    $k$. Questo può accadere sse $\phi$ è vera, infatti per ogni clausola $c_i$
    $\exists \,\,l_{ij}$, letterale, che rende vera $c_i$, a questo punto tale
    letterale è un vertice del triangolo, ovvero del gadget $g_{c_i}$, (mi basta
    infatti un letterale vero per triangolo) e, poiché
    tutte le clausole sono vere, ho la scelta di $k$ vertici se $k$ è il numero
    delle clausole. Esiste quindi un \textit{independent-set} di dimensione
    $k$.\\
    Per questo si potrebbe fare una \textbf{dimostrazione per costruzione},
    ovvero se rendo vera $c_y$ con $l_y$ significa che la variabile $x_i$ può
    essere usata o come 1 o come 0 nell'assegnamento di verità in un altro
    letterale $l_z$, che rende vera la clausola $c_z$. Ma $x_i$, se già usata,
    non posso più usarla con valore opposto a quello scelto per $c_y$ e quindi
    non esiste un arco di collegamento tra $l_y$ e $l_z$.\\
    Si può dimostrare anche per \textbf{assurdo}. Se esiste un arco tra i due
    letterali $l_z$ e $l_y$ che rendono vere le clausole $c_z$ e $c_y$,
    allora ottengo una contraddizione sugli assegnamenti di verità, asserendo
    che i due letterali sono uno la negazione dell'altro ma entrambi sono
    veri, per poter rendere vere le clausole. 
    \\
    \\
    Il \textbf{secondo verso} si dimostra dicendo che, dato un
    \textit{independent-set} 
    $S$ di dimensione $k$, $S$ deve contenere un vertice per triangolo. Ponendo
    quindi i letterali contenuti in $S$ come veri si ottiene un assegnamento di
    verità che è \textbf{consistente} e tutte le clausole sono soddisfatte.
    Quindi se esiste un \textit{independent-set} di dimensione $k$ allora trovo
    un assegnamento alle variabili $x_i,\,\,\forall \,\,1\ldots n$ che rende
    vera $\phi$, ovvero assegno 0 o 1 a ciascuna variabile (ovviamente o 1 o 0,
    non entrambi). Se esiste l'\textit{independent-set} di dimensione $k$ pari
    al numero delle clausole, allora per ogni gadget $g_{c_i}$,
    che rappresenta una clausola, esiste un vertice nel gadget che si trova
    anche nell'\textit{independent-set}, quindi esiste un letterale, per ogni
    clausola $c_i$ tale che non è collegato ad un altro letterale
    dell'\textit{independent-set}, ovvero non è collegato ad un altro letterale
    di un'altra clausola (ovvero ho un nodo per gadget che non ha un arco verso
    un nodo di un altro gadget). Quindi per ogni letterale vedo la variabile che
    rende vero il letterale e con il valore dato alla variabile costruisco
    l'assegnamento o 0 o 1 a quella variabile (se $l_i=\neg x_j$ allora
    $x_j=0$ e se $l_i= x_j$ allora $x_j=1$). Trovo quindi l'assegnamento delle
    variabili che è di verità per $\phi$, dimostrando quindi che $\phi$ è vera.
  \end{proof}
\end{esempio}
Siccome $3SAT$ è \textbf{NP-complete} allora anche \textit{independent-set} è
\textbf{NP-complete}, infatti:
\[\forall\,\, A_{\in NP}\leq_p 3SAT\leq_p \mbox{ \textit{independent-set}}\]
in quanto la riduzione $\leq_p$ è \textbf{transitiva}, e quindi:
\[\forall\,\, A_{\in NP}\leq_p \mbox{ \textit{independent-set}}\in NP\]
\begin{teorema}
  Se un problema $\Pi$ \textbf{NP-complete} è in \textbf{P} allora si può dire
  $P=NP$, implicando che ogni problema $A\in NP$ è risolvibile da $\Pi\in P$.
  Questa cosa non è stata ancora dimostrata (e probabilmente si riuscirà
  a dimostrare l'opposto).
\end{teorema}
\begin{proof}
  Per ogni problema $A$ in \textbf{NP} so che $A\leq_p \Pi$, ovvero $\Pi$ è una
  procedura che risolve $A$, con trasformazione dell'input $x$ di $A$ nell'input
  $f(x)$ di $\Pi$, con $f(x)$ calcolabile in tempo polinomiale.\\
  Assumendo quindi che $\Pi\in P$ allora anche ogni $A\in P$, e quindi $NP=P$.
\end{proof}
\begin{teorema}
  La riduzione polinomiale è transitiva. Ovvero se $A\leq_p B$ e $B\leq_p C$
  allora:
  \[A\leq_p C\]
\end{teorema}
\begin{proof}
  Infatti $A\leq_p B$ implica l'esistenza di $f$ tale che $x\in L_A$ sse
  $f(x)=L_B$. Ugualmente $B\leq_p C$ implica l'esistenza di $g$ tale che $x\in
  L_B$ sse $g(x)=L_C$.\\
  Devo dimostrare che $\exists\,\,f'$ tale che $x\in L_A$ sse $f'(x)\in
  L_C$. Per ottenere $f'$ compongo $f$ e $g$. Assumendo che $x$ appartiene
  all'input di ha compongo le funzioni, quindi ho che $f'=g\circ f$,
  e ho che $x\in L_A$ e che $f(x)\in L_B$ (quindi $f(x)$ è un input per $B$) ma
  quindi $g(f(x))\in L_C$ (quindi $g(f(x))$ è un input per $C$). Ho quindi
  dimostrato la transitività.\\
  Se $f$ e $g$ sono costruibili in tempo polinomiale, la prima sulla dimensione
  di $x$ e la seconda su quella di $f(x)$, allora anche $f'=g\circ f$
  è costruibile in tempo polinomiale, proporzionalmente alla cardinalità di $x$.
\end{proof}
Quindi per dimostrare che un problema $\Pi'$ è \textbf{NP-hard} devo ridurre
$\Pi'$ ad un problema qualsiasi \textbf{NP-hard} (in base alla somiglianza del
problema), sapendo che $\forall A\in NP$ $A$ si riduce ad un problema
\textbf{NP-hard}, come $SAT$.\\
In modo equivalente per dire che è un problema è \textbf{NP-complete} faccio
quanto fatto per \textbf{NP-hard} ma devo aggiungere che esso sia in $NP$,
dovendo quindi aggiungere il ``certificato''.
% immagine slide 6(teorema cook)
\begin{teorema}
  Si ha che $SAT\leq_p 3SAT$
\end{teorema}
\begin{proof}
  \textbf{Dimostrazione solo parziale, solo l'inizio è stato fatto in aula.\\}
  Prendo una $\phi$ con $k\geq 4$ letterali. Ogni clausola deve diventare una
  clausola con al più 3 letterali (introducendo nuovi letterali ogni volta che
  viene negato uno). 
\end{proof}
\subsection{Problema set-cover}
Questo problema si applica bene allo studio, nel campo delle telecomunicazioni,
dei ripetitori e dello studio della copertura per reti mobili.
\begin{definizione}
  Dato un universo $U$ di $n$ elementi sia $S=\{S_1,\ldots,S_M\}$ una collezione
  di sottoinsiemi di $U$. Sia anche data una funzione di costo
  $c:S\to\mathbb{Q}^+$. Il problema \textbf{set-cover} consiste nel trovare una
  collezione $C$ di sottoinsiemi di $S$ di costo minimo che copra tutti gli
  elementi di $U$.
\end{definizione}
\begin{esempio}
  Se ho $U=\{1,2,3,4,5\}$, $S_1=\{1,2,3\}$, $S_2=\{2,3\}$, $S_3=\{4,5\}$ e
  $S_4=\{1,2,4\}$, con $S=\bigcup S_i$ . Per praticità assumo costo
  uniforme, ovvero che  $c_1=c_2=c_3=c_4=1$.\\
  Quindi la soluzione $C$ è $\{S_1,S_3\}$, in quanto questi due insiemi coprono
  tutti gli elementi di $U$, con costo pari a $5$ (che è il minimo che posso
  avere).
\end{esempio}
\begin{definizione}
  Qualora il costo sia uniforme allora il \textbf{set-cover} diventa la ricerca
  di una sotto-collezione che copra tutti gli elementi di $U$ con minima
  dimensione.
\end{definizione}

\begin{teorema}
  \textit{set-cover}, nella versione decisionale e nella
  versione con peso uniforme, è \textbf{NP-complete} (quindi se esiste una
  collezione $C$ di sottoinsiemi di $S$ 
  la cui unione sia $U$ con cardinalità minore uguale di $k$).
\end{teorema}
\begin{proof}
  Posso facilmente dimostrare che $|C|\leq k$ in tempo polinomiale e che
  l'unione degli insieme di $C$ include tutti gli elementi di $U$, in
  tempo polinomiale su $|U|$. Posso aggiungere anche un ``certificato'', nella
  forma di una collezione che copre tutto $U$ (e quindi la verifica è in tempo
  polinomiale sulla dimensione del ``certificato'' più quella di $U$). Quindi
  \textbf{set-cover} è in \textbf{NP}.\\
  Per dimostrare che \textbf{set-cover} è \textbf{NP-hard} dimostro che:
  \[\mbox{vertex-cover} \leq_p \mbox{set-cover}\]
  ovvero uso un problema che so già essere \textbf{NP-hard}, appunto
  \textit{vertex-cover} (avendo già dimostrato che $\forall A\in NP, \,\,A\leq_p
  \mbox{vertex-cover}$, dimostrando che \textit{set-cover} dimostra tutti i
  problemi in \textbf{NP}).\\
  Faccio vedere che posso usare \textit{set-cover} per dimostrare
  \textit{vertex-cover}.\\
  Devo quindi trasformare un'istanza di \textit{vertex-cover} $C=\langle
  G=(V,E), j\rangle$ in un'istanza $C'$ di \textit{set-cover}, in tempo
  polinomiale, tale che $C$ è soddisfacibile sse $C'$ è soddisfacibile.\\
  Procedo quindi con la trasformazione. Pongo innanzitutto $U=E$. Per quanto
  riguarda la collezione $S$ procedo nel seguente modo. Etichetto i vertici in
  $V$ da $1$ a $n$. A questo punto $S_i$ diventa l'insieme degli archi incidenti
  al vertice $i$-simo. A questo punto basta porre $k=j$ per concludere la
  costruzione polinomiale dell'istanza di \textit{set-cover}.\\
  In poche parole ciascun arco è un elemento di $U$ e ciascun vertice è un
  insieme di $S$.\\
  Vediamo anche la dimostrazione formale che \textit{vertex-cover} risponde
  ``yes'', per $j$, sse istanza di \textit{set-cover} risponde ``yes'' per
  $k=j$.\\
  Innanzitutto se \textit{vertex-cover} risponde ``yes'' per $j$ allora trovo
  una collezione di \textit{set-cover} buona di cardinalità $j$. Suppongo
  infatti $G$ ha una copertura C di al più $j$ vertici e quindi $C$ corrisponde
  ad una collezione di $C'$ di sottoinsiemi $U$. Poiché assumo $k=j$ allora
  $|C'|\leq k$. Inoltre $C'$ copre tutti gli elementi di $U$ coprendo tutti gli
  archi di $G$, in quanto ogni elemento di $U$ è un arco in $G$. Poiché $C$ è
  una copertura, almeno un estremo dell’arco è in $C$  e quindi l’arco è in un
  insieme di $C'$.\\
  Dimostriamo anche l'altro verso della dimostrazione, ovvero devo garantire
  l'esistenza della copertura. Suppongo di avere un set cover $C'$ di dimensione
  $k$. Dato che ad ogni insieme di $C'$ ho associato un vertice in $G$ allora
  $|C|=|C'|\leq k=j$. Inoltre, $C$ è una copertura di $G$ poiché $C'$ è un
  set-cover, poiché, preso un arco $e$ ho che $e\in U$ e quindi $C'$ deve
  contenere almeno un insieme che contiene $e$ e tale insieme è quello che
  corrisponde ai nodi che sono estremi di $e$. Quindi $C$ deve contenere almeno
  un estremo di $e$. Quindi posso concludere dicendo che $C$ è copertura di $G$.
\end{proof}
Si continua la ricerca di una dimostrazioni di \textbf{NP-completezza}, ambito
di studio nato dopo la scoperta di Cook di $SAT\in$ \textbf{NP-complete}.\\
Partiamo ricordando che:
\begin{teorema}
  Se $B$ è un problema tale che $A\leq_p B$, con $A$
  \textbf{NP-hard} (o ovviamente anche \textbf{NP-complete}), allora $B$ è
  \textbf{NP-hard} e se inoltre $B\in NP$ allora $B$ è \textbf{NP-complete}.  
\end{teorema}
\begin{proof}
  Con la transitività della riduzione $\leq_p$ si ha che $\forall\pi\in
  NP,\,\,\, \pi\leq_p A$ e quindi $A$ è \textbf{NP-hard}. Inoltre avendo
  $A\leq_p B$ ho che  $\pi\leq_p B$ e quindi $B$ è \textbf{NP-hard}, inoltre,
  avendo $B\in NP$, ho che è \textbf{NP-complete}
\end{proof}


\subsection{Problemi di ottimizzazione}
\subsubsection{Clique-problem}
\begin{definizione}
  Definisco \textbf{clique (\textup{cricca})} di un grafo non orientato
  $G=(V,E)$ come un sottoinsieme $V'\subseteq V$ di vertici tale che:
  \[\forall \,v_1,v_2\in V' (v_1,v_2)\in E\]
  quindi un sottoinsieme di vertici con solo vertici collegati da un arco.
\end{definizione}
Definisco quindi il problema.
\begin{definizione}
  Il \textbf{clique-problem} è un problema di ottimizzazione (nel
  dettaglio di massimo) in cui si cerca la \textbf{clique} di dimensione massima
  di un grafo (ovvero $|V'|$ è massimo). Nella versione decisionale chiedo se
  esiste una \textbf{clique} di dimensione $k$.\\
  Il problema è \textbf{NP-complete}
\end{definizione}
\begin{proof}
  Dimostriamo che sia \textbf{NP-complete} (nella versione decisionale). Cerco
  quindi un algoritmo 
  polinomiale, con``certificato''. Uso quindi un insieme $V'\subseteq V$ come
  vertici della \textbf{clique} come ``certificato'' per il grafo in input. Per
  verificare che $V'$ è una \textbf{clique} controllo che:
  \[\forall\,v_1,v_2\in    V' (v_1,v_2)\in E\]
  e questa verifica è polinomiale, infatti è $O(|V'|^2)$ e
  quindi è quadratico nella dimensione dell'input.\\
  Bisogna ora trovare un problema $A\in \mbox{NP-complete}$ tale che $A$ si
  riduce in tempo polinomiale al \textbf{clique-problem} (quindi
  \textbf{clique-problem} risolve $A$).  \\
  Notiamo che una \textbf{clique} è l'opposto di \textbf{independent-set}, che
  avevamo dimostrato tramite $3SAT$ (che può risolvere
  \textbf{independent-set}).\\
  Quindi per \textbf{clique-problem} provo ancora ad usare $3SAT$. Devo fare in
  modo che $\phi\in 3SAT$ sse il grafo $G_\phi$ ha una \textbf{clique} di
  dimensione uguale al numero di clausole di $\phi$. Quindi avendo $k$ clausole
  in $\phi$ avrò che ciascuna clausola sarà un gadget e da ogni gadget estrarrà
  un vertice che comporrà la \textbf{clique} di dimensione $k$.\\
  Il grafo $G_\phi$ è costruito come nel caso di \textit{independent-set} coi
  letterali come vertici.\\
  Bisogna studiare il collegamento tra tali vertici (che sarà diverso al caso di
  \textit{independent-set}, non avendo quindi i triangoli).\\
  Ipotizzo:
  \[\phi=c_1\land c_2\land c_3\]
  con:
  \[c_1=x_1\lor \neg x_2\lor \neg x_3\]
  \[c_2=\neg x_1\lor x_2\lor x_3\]
  \[c_3=x_1\lor x_2\lor x_3\]
  Per ogni clausola faccio i vertici per ogni letterale (distinti anche per lo
  stesso letterale, come nel caso di \textit{independent-set}).\\
  Per ora abbiamo vertici isolati e i tre vertici isolati di ogni clausola sono
  il gadget. A questo punto collego ogni letterale di ogni clausola con ogni
  altro letterale di ogni altra clausola (non della stessa) che non sia
  l'opposto, collegando quindi solo vertici \textbf{consistenti} (in quanto non
  potrei avere una \textbf{clique} connettendo due vertici non consistenti). Sto
  facendo esattamente l'opposto di \textit{independent-set}. 
  \begin{figure}
    \centering
    \psscalebox{0.9 0.9} % Change this value to rescale the drawing.
    {
      \begin{pspicture}(0,-4.3798437)(8.58,4.3798437)
        \definecolor{colour0}{rgb}{0.003921569,0.003921569,0.003921569}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](2.8,3.3050585){0.4}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](6.8,3.3050585){0.4}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](4.8,3.3050585){0.4}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](1.2,2.1050587){0.4}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](1.2,-1.8949414){0.4}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](1.2,0.105058596){0.4}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](2.8,-3.0949414){0.4}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](6.8,-3.0949414){0.4}
        \pscircle[linecolor=black, linewidth=0.04,
        fillstyle=solid,fillcolor=colour0, dimen=outer](4.8,-3.0949414){0.4}
        \rput[bl](2.65,3.9050587){$x_1$}
        \rput[bl](4.4,3.9050587){$\neg x_2$}
        \rput[bl](6.4,3.9050587){$\neg x_3$}
        \rput[bl](0.0,2.0050587){$\neg x_1$}
        \rput[bl](0.2,0.0405058596){$x_2$}
        \rput[bl](0.2,-2.0949414){$x_3$}
        \rput[bl](2.63,-3.8949413){$x_1$}
        \rput[bl](4.63,-3.8949413){$x_2$}
        \rput[bl](6.63,-3.8949413){$x_3$}
        \psline[linecolor=black, linewidth=0.04](4.8,2.9050586)(2.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](4.8,2.9050586)(6.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](4.8,2.9050586)(1.6,2.1050587)
        \psline[linecolor=black, linewidth=0.04](4.8,2.9050586)(1.6,-1.8949414)
        \psline[linecolor=black, linewidth=0.04](6.8,2.9050586)(4.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](6.8,2.9050586)(2.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](6.8,2.9050586)(1.6,0.105058596)
        \psline[linecolor=black, linewidth=0.04](6.8,2.9050586)(1.6,2.1050587)
        \psline[linecolor=black, linewidth=0.04](1.6,2.1050587)(4.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](1.6,2.1050587)(6.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04]
        (1.6,0.105058596)(2.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04]
        (1.6,0.105058596)(4.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04]
        (1.6,0.105058596)(6.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](1.6,-1.8949414)(2.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](1.6,-1.8949414)(4.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](1.6,-1.8949414)(6.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](2.8,2.9050586)(2.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](2.8,2.9050586)(4.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](2.8,2.9050586)(6.8,-2.6949415)
        \psline[linecolor=black, linewidth=0.04](2.8,2.9050586)(1.6,0.105058596)
        \psline[linecolor=black, linewidth=0.04](2.8,2.9050586)(1.6,-1.8949414)
      \end{pspicture}
    }
    \caption{Grafo $G_\phi$ per \textbf{clique-problem}, con le due righe di
      nodi e la colonna che rappresentano i tre gadget}
    \label{fig:cli}
  \end{figure}
  Costruito il grafo $G_\phi$ cerco almeno una \textbf{clique} di dimensione 3
  per $3SAT$. Non potrò mai avere più di un letterale per clausola nella
  \textbf{clique} non essendo tra loro collegati.\\
  Passare da $\phi$ a $G_\phi$ ha costo polinomiale in tempo, ottenendo quindi
  istanza, in tempo polinomiale. \\
  Inoltre bisogna dimostrare che se $\phi$ ha un assegnamento che la rende vera
  allora esiste una \textbf{clique} per $G_\phi$ di dimensione $k$. Se $\phi$ è
  vera allora per ogni clausola $c_r$ esiste almeno un letterale che è vero e
  assumiamo che tale letterale sia $l_{i}^r=1$. Questo letterale è associato al
  vertice $v_i^r$. Quindi data la clausola $c_i$, vera per il letterale $l_j^i$
  allora costruisco l'insieme dei vertici $V'\subseteq V$ tale che sia formato
  da quei singoli vertici corrispondenti ai singoli letterali veri. Questo $V'$
  è una \textbf{clique} infatti avrò solo letterali ``collegabili'' nel grafo
  (non essendo vero che un letterale sia vero e contemporaneamente falso, cosa
  che comporterebbe l'assenza dell'arco). Ho solo archi tra letterali
  consistenti tra loro e quindi, per costruzione, esiste l'arco tra i vertici
  corrispondenti ($\forall\, u,v\in V'$).\\
  Inoltre bisogna dimostrare che, se esiste una \textbf{clique} di dimensione
  $k$, allora $\phi$ è vera e quindi le $K$ clausole sono tutte vere. \\
  Per ogni vertice di $v_{it}\in V'$ se appartiene al gadget della clausola
  $c_t$ rendo vero il letterale associato (o falso se esso è negato). A questo
  punto rendo vera ogni clausola rendendo vero almeno un suo letterale e quindi
  so di ottenere un assegnamento di verità per $\phi$ in quanto i letterali, per
  come sono stati definiti, sono consistenti.
  \\ \textbf{sistemare formalismi}
\end{proof}
\begin{proof}
  Dimostriamo che la riduzione del problema \textit{clique}, che sappiamo essere
  \textbf{NP-complete}, è in tempo polinomiale.\\ 
  Vediamo che la trasformazione della formula $\phi$ di $3SAT$ nel grafo
  $G_\phi$ è una riduzione in tempo polinomiale.\\
  Dato un assegnamento che rende vera $\phi$, che ha $k$ clausole, dobbiamo
  costruire una \textit{clique} per $G_\phi$ che abbia $k$ vertici. Ciascuna
  clausola $c_{r}$ ha almeno un letterale $l_{i}^r$ che ha assegnato il valore 1,
  $\top$. Ciascun letterale $l_{i}^r$ corrisponde ad un vertice
  $v_{i}^r$. Costruiamo $V'$ insieme di vertici per ogni letterale $l_i^r$ vero
  per ogni clausola $c_r$. Dimostriamo che $V'$ è una \textit{clique}. Si ha che
  per ogni coppia $v_i^r,v_j^s\in V',\,\,r\neq s$ (due vertici rappresentanti
  due letterali di due clausole diverse) abbiamo che $(v_i^r,v_j^s)\in
  E$ essendo entrambi i letterali veri e,per costruzione, sono consistenti, cioè
  uno non è l'opposto dell'altro.\\
  Vediamo l'altro verso.\\
  Supponiamo che $G_\phi$ abbia una \textit{clique} $V'$ di dimensione $k$ e
  bisogna vedere se $\phi$ è vera. Sappiamo che $V'$, per costruzione ha
  esattamente un solo vertice per ogni clausola, $v_i^r$ per la clausola
  $c_r$. Prendo quindi il letterale corrispondente $l_i^r$ e assegno a tale
  letterale il valore 1 (se negato 0). Quindi so che il letterale complemento di
  $l_i^r$ non verrà usato per rendere vera una clausola perché il vertice
  $v_i^r$ non è collegato con un arco ad un vertice che rappresenta il
  c0complemento di $l_i^r$. Quindi riesco a costruire un assegnamento di verità
  \textit{consistente} (ovvero ogni variabile è presa come $x_i=1$ oppure
  $x_i=0$ ma non entrambi i valori sono usati) per $\phi$.
\end{proof}
Si può dimostrare che:
\[clique\leq_p vertex\_cover\]
\[independent\_set\leq_p vertex\_cover\]
\[vertex\_cover\leq_p independent\_set\]
\[vertex\_cover\leq_p clique\]
Infatti vale il seguente teorema:
\begin{teorema}
  Se ho che:
  \[A\leq_p B\]
  e $A,B\in \mbox{NP-complete}$, allora:
  \[B\leq_p A\]
\end{teorema}
\begin{proof}
    Infatti, siccome $B$ è in \textbf{NP-complete} allora è anche in \textbf{NP},
  allora: 
  \[B\leq_p A\]
  Ma posso fare lo stesso discorso con $A$, essendo in \textbf{NP-complete} e
  quindi sono interscambiabili e quindi:
  \[A\leq_p B\]
\end{proof}
I problemi \textbf{NP-complete} richiedono quindi una soluzione efficiente.\\
Bisogna quindi pensare ad algoritmi di approssimazione, algoritmi polinomiali
con ``garanzia'', ovvero \textbf{euristiche} (ovvero un qualcosa che funziona in
pratica) per il problema. Non si ha quindi una soluzione esatta al problema,
l'euristica non fornisce una soluzione esatta.\\
L'obiettivo è quindi quello di risolvere i problemi \textbf{NP-complete}.
\begin{definizione}
  Definiamo un \textbf{problema di ottimizzazione}, dato un problema $\Pi$ e
  un'istanza $x$, come la ricerca di un ottimo di $x$, detto $opt(x)$. Il costo
  di una soluzione ammissibile su $x$ calcolato da un algoritmo $A$ per il
  problema $\Pi$ è indicato con $A(x)$ (che quindi è il costo della soluzione
  calcolato da $A$).\\
  
  Si cerca quindi la soluzione, tra tutte quelle di $\Pi$, che massimizza o
  minimizza una funzione costo (appunto $opt(x)$). Non sappiamo chi calcoli
  questo costo.
\end{definizione}
\begin{definizione}
  Definiamo un \textbf{algoritmo $\varepsilon$-approssimato} per un problema
  $\Pi$ è un 
  algoritmo $A$ polinomiale che restituisce una soluzione ammissibile che dista
  da quella ottima di un fattore $\varepsilon$.\\
  Se $\Pi$ è un \textbf{problema di minimo} allora:
  \[A(x)\leq \varepsilon\cdot opt(x), \mbox{ \textnormal{con} }\varepsilon>1\]
  Si raggiunge quindi un valore più grande del minimo, di una quantità fissata
  $\varepsilon$. Devo quindi garantire che non sia troppo grande, tramite
  $varepsilon$.\\
  Si ha quindi che:
  \[\frac{A(x)}{opt(x)}\leq \varepsilon\]
  L'algoritmo lavora in tempo polinomiale e risolve in modo approssimato
  problemi $\Pi$ \textbf{NP-completi}. Se avessi $\varepsilon=1$ avrei la
  soluzione ottima, ma non sarebbe possibile in quanto avrei un algoritmo
  polinomiale per un algoritmo \textbf{NP-completo}.\\
  Se $\Pi$ è un \textbf{problema di massimo} allora:
  \[A(x)\geq \varepsilon\cdot opt(x), \mbox{ \textnormal{con} }0<\varepsilon<1\]
  In quanto raggiungo una soluzione più piccola del massimo ma devo garantire
  che non sia troppo piccola, tramite $varepsilon$. $\varepsilon\cdot opt(x)$ è
  infatti una ``frazione'' dell'ottimo.\\
  Si ha quindi che:
  \[\frac{A(x)}{opt(x)}\geq \varepsilon\Longrightarrow\frac{opt(x)}{A(x)}\leq
    \frac{1}{\varepsilon}\]
  A seconda del problema devo trovare un $\varepsilon$, che viene fissata
  costante per ogni input (e quindi è indipendente dall'input stesso e dalla sua
  dimensione) che serve da ``garanzia''. So quindi che $A(x)$, ovvero il costo
  della soluzione approssimata ammissibile calcolata da $A$, in tempo
  polinomiale, si rapporta a $opt(x)$, calcolata dall'algoritmo esatto in tempo
  esponenziale (se fosse polinomiale si avrebbe $P=NP$), tramite una distanza
  $\varepsilon$ (da un alto è un $\limsup$ e dall'altro un $\liminf$).\\
  Tipicamente si ha come valore tipico per $varepsilon$ 2, avendo quindi
  una \textbf{2-approssimazione}, in quanto facilmente dimostrabile.
\end{definizione}
\begin{esempio}
  Prendiamo \textit{vertex-cover} nella versione di ottimo. Dato $G=(V,E)$ ha
  come soluzione ammissibile una copertura di vertici per $G$. Il costo di una
  soluzione è la cardinalità di $V'$, se $V'$ è una soluzione ammissibile.\\
  Il costo ottimo di \textit{vertex-cover} invece è la minima cardinalità di
  $V'$.
\end{esempio}
\begin{definizione}
  Si ha il cosiddetto \textbf{approximation ratio} $r$ dicendo che:
  \[\max\left\{\frac{A(x)}{opt(x)},\frac{opt(x)}{A(x)}\right\}\leq r,\mbox{
      \textnormal{con} }r>1\] 
  Il primo caso copre il caso di minimo mentre il secondo caso copre il massimo.
\end{definizione}
\textbf{Non tutti i problemi ammettono una \textit{r}-approssimazione, per
  \textit{r} costante}.\\
Ci sono problemi infatti dove il calcolo di:
\[\max\left\{\frac{A(x)}{opt(x)},\frac{opt(x)}{A(x)}\right\}\leq \rho(n)\]
per una certa funzione $\rho$,con $|x|=n$,
ovvero il massimo dipende dalla
dimensione dell'input, non avendo quindi più un $r$ costante. La dimensione del
problema influisce sulla capacità di approssimazione e degradano all'aumentare
della dimensione. Si ha, per esempio, $\rho(n)=\log n$.\\
Per capire $\varepsilon$ fornisco un'euristica $A$ per il problema e provo a
dimostrare, senza conoscere l'ottimo (quindi per ogni possibile input), che in
ogni caso:
\[\frac{A(x)}{opt(x)}\leq \varepsilon\]
dimostrando che si ha una $\varepsilon$-approssimazione (non sempre è
dimostrabile o perlomeno per alcuni problemi si è ancora riusciti).
\begin{teorema}
  Esiste $A$ polinomiale per \textit{vertex-cover} che è 2-approssimante,
  quindi:
  \[\frac{A(x)}{opt(x)}\leq 2\]
\end{teorema}
\begin{proof}
  Prendo il grafo $G=(V,E)$.\\
  Cerco un $A$ che calcoli in tempo polinomiale una soluzione ammissibile,
  ovvero una copertura di vertici del grafo. Si sceglie che $A$ non deve essere
  un \textit{algoritmo greedy}.\\
  Prendo quindi un arco del grafo $e=(u,v)\in E$. Inizio a costruire una
  copertura $C$, all'inizio vuota ($C=\emptyset$), che ora diventa, aggiungendo
  i due vertici:
  \[C=C\cup\{u,v\}\]
  Inoltre rimuovo dall'insieme degli archi $E$ tutti gli archi che hanno un
  estremo nell'attuale copertura $C$, e quindi nel nostro caso in $u$ o $v$.\\
  Procedo quindi iterativamente costruendo $C$ fermandomi quando $E$ risulti
  vuoto, ovvero fino a che $E=\emptyset$.\\
  È quindi una 2-approssimazione in quanto al massimo posso prendere il doppio
  dei vertici per fare la copertura, infatti, per costruzione, seleziono ogni
  volta archi che non hanno estremi in comune. Per ognuno di questi archi scelti
  devo prendere i due estremi, quindi ho $2\cdot k$ vertici in $C$ per $k$ archi
  e quindi:
  \[A(x)=2\cdot k\]
  Ma di $opt(x)$ so che nel casi migliore prende esattamente un estremo per ogni
  scelto dall'algoritmo, e quindi per archi che non hanno estremi in
  comune. Quindi nel caso migliore:
  \[opt(x)\geq k\]
  Dato che stiamo cercando di minimizza, quindi $k$ è il \textit{lower bound} e
  quindi:
  \[\frac{A(x)}{opt(x)}\leq\frac{2k}{k}\leq 2\]
  Come volevasi dimostrare.
\end{proof}
\begin{algorithm}
  \begin{algorithmic}
    \Function{VCApprox}{$G=(V,E)$}
    \State $C\gets\emptyset$
    \State $E'\gets E$
    \While {$E'\neq \emptyset$}
    \State $\mbox{let }(u,v)\in E'$
    \State $C\gets C\cup \{u,v\}$
    \For {$\mbox{every }(u,z)\in E'\land\mbox{ every } (v,z')\in E'$}
    \State $\mbox{delete } (u,z),(v,z') \mbox{ from } E'$
    \EndFor
    \EndWhile
    \State \textbf{return} $C$
    \EndFunction
  \end{algorithmic}
  \caption{Algoritmo di vertex-cover approssimato}
\end{algorithm}
\newpage
\begin{shaded}
  L'algoritmo per il test di primalità funziona in $\log (n^{12})$, nella
  versione più efficiente sviluppata da un gruppo di indiani, che arriva ad
  ottenere un tempo polinomiale.
\end{shaded}
\chapter{Macchina di Turing}
\section{Problemi intrattabili}
Trovare una soluzione polinomiale ad un algoritmo \textbf{NP-hard} come TSP
comporterebbe la rottura di tutte le chiavi crittografiche in quanto trovato un
algoritmo polinomiale per uno lo trovi per tutti.\\
Ci serve a questo punto una definizione più rigorosa di \textbf{algoritmo}, per
poterne calcolare meglio i tempi. Ricordiamo che per assunzione un algoritmo è
\textbf{efficiente} se è in tempo polinomiale rispetto alla dimensione
dell'input $x$, sapendo che $|x|=n$ (solitamente al più si arriva a $O(n^5)$ o
poco più poi si salta ad algoritmi esponenziali). Un algoritmo esponenziale è un
algoritmo \textbf{non efficiente}, il tempo cresce troppo velocemente
all'aumentare dell'input (anche se magari in alcuni casi non è in tempo
esponenziale). Si ricorda che si studia sempre il tempo nel 
\textbf{caso peggiore}, prenderemo quindi sempre l'O-grande sulla dimensione
dell'input $O(f(x))$.\\
Vediamo degli esempi:
\begin{itemize}
  \item un algoritmo che cerca l'arco minimo lavora in tempo polinomiale nel
  caso peggiore ed è quindi un \textbf{problema trattabile}
  \item problemi, come il test di primalità o TSP, che non hanno un algoritmo
  polinomiale sono \textbf{intrattabili}, infatti nessuno ha mai dimostrato che
  esiste un algoritmo efficiente
\end{itemize}
Tra i problemi intrattabili abbiamo però problemi che sono
\textbf{dimostrabilmente intrattabili}. Banalmente un problema che mi chiede di
stampare tutte le possibili sequenze per una certa proprietà è in questa
categoria, dovendo stampare tutte le sequenze possibili si ha $O(2^n)$ e si può
dimostrare che con meno operazioni non si stamperebbero alcune soluzioni
corrette.\\
Ci concentreremo su problemi intrattabili ma non \textit{dimostrabilmente
  intrattabili}.\\
Per anni problemi come il \textit{test di primalità} potevano garantire che
prima o poi si sarebbe trovato un algoritmo polinomiale (forse). Quindi abbiamo:
\begin{itemize}
  \item problemi dimostrabilmente intrattabili
  \item problemi non dimostrabilmente intrattabili, sono, diciamo, ``i più
  difficili'' tra i problemi intrattabili
  \item tutti gli altri problemi
\end{itemize}
\textbf{Un problema indecidibile non è un problema intrattabile}, in quanto non
si hanno proprio algoritmi che risolvono un certo problema e questo è
dimostrabile (c'è almeno un input che manda in crisi un algoritmo, che va in
loop infinito o sbaglia risposta) mentre un problema intrattabile comunque in
qualche modo lo posso risolvere ma in tempi troppo elevati.
\section{Definizione della TM}
Definiamo quindi in modo più rigoroso il concetto di algoritmo per poter
dimostrare che un certo algoritmo può anche non esistere. Non ci basta più la
definizione di algoritmo come sequenza di passi logici, in quanto andrebbe anche
definito un certo linguaggio (con scelte, cicli, operazioni aritmetiche,
operazioni logiche) ma ancora non basterebbe, non si è ancora sicuri di poter
trasformare un certo input in un certo output (per qualunque problema in
input). Lo step mancante è la \textbf{macchina di Turing}, con essa si può
garantire quanto appena detto, con essa si formalizza il processo di calcolo,
ovvero la serie di passaggi che porta da un input ad un output. Turing ragionò
dicendo che normalmente si risolve un problema partendo da carta e penna,
ponendosi poi in un certo stato mentale in cui si risolve o si studia una parte
dell'input (ad esempio in uno stato \textit{leggi tutti} leggo ``step by step''
tutto l'input, senza cambiare sto mentale, che verrà cambiato quando finisco
quello precedente). Divide quindi l'input in \textit{caselle} su cui si fanno
operazioni semplici, spostandosi a destra o a sinistra di una casella, o
leggendo/scrivendo la casella corrente. Turing ipotizza di avere carta
illimitata. Quindi la MT avrà un nastro infinito che permette di memorizzare
informazioni e si ha una testina di lettura e scrittura. Si ha un meccanismo che
si pone in uno \textit{stato}, sulla base del contenuto letto dalla testina e
dallo stato posso scegliere di spostarmi di una testina a destra o di una a
sinistra, eventualmente dopo avere scritto e modificando, sempre eventualmente,
lo stato. Tutto questo basta per il \textbf{calcolo}, infatti:
\begin{teorema}[Tesi di Turing-Church]
  Non esiste nessun formalismo di calcolo che sia più
  potente della Macchina di Turing:
  \begin{center}
   ``Se un problema è umanamente calcolabile, allora esisterà una macchina di
    Turing in grado di risolverlo (cioè di calcolarlo)''
  \end{center}
  È una tesi che non ha dimostrazione formale (non avendo chiara la definizione
  di \textbf{calcolo}) ma è stata dimostrata
  empiricamente nel corso degli anni. Portando quindi a dire che il calcolo è
  ciò che può essere eseguito con un Macchina di Turing (anche se non tutti i
  meccanismi di calcolo sono equivalenti ad una TM, ad esempio gli automi a
  stati finiti).\\
  Quindi ciò che è computabile è computabile da una TM o da un suo equivalente
  (come un linguaggio di programmazione). Banalmente anche una rete neurale lo
  è. 
\end{teorema}
\begin{figure}
  \centering
  \begin{tikzpicture}
    \tikzstyle{every path}=[very thick]

    \edef\sizetape{0.7cm}
    \tikzstyle{tmtape}=[draw,minimum size=\sizetape]
    \tikzstyle{tmhead}=[arrow box,draw,minimum size=.5cm,arrow box
    arrows={east:.25cm, west:0.25cm}]
    \begin{scope}[start chain=1 going right,node distance=-0.15mm]
    \node [on chain=1,tmtape,draw=none] {$\ldots$};
    \node [on chain=1,tmtape] {};
    \node [on chain=1,tmtape] {$\triangleright$};
    \node [on chain=1,tmtape] (input) {b};
    \node [on chain=1,tmtape] {b};
    \node [on chain=1,tmtape] {a};
    \node [on chain=1,tmtape] {a};
    \node [on chain=1,tmtape] {a};
    \node [on chain=1,tmtape] {a};
    \node [on chain=1,tmtape] {$\sqcup$};
    \node [on chain=1,tmtape,draw=none] {$\ldots$};
    \node [on chain=1] {\textbf{Input/Output Tape}};
\end{scope}
  \end{tikzpicture}
  \caption{Esempio di nastro di una TM}
  \label{fig:tur}
\end{figure}
Formalizziamo quindi la macchina di Turing.
\begin{definizione}
  Si definisce formalmente una TM come la quintupla:
  \[TM=(K,\Sigma,k_0, \delta, F)\]
  \begin{itemize}
    \item insieme $K$ di stati
    \item un alfabeto $\Sigma$
    \item uno stato di partenza $k_0$
    \item una funzione di transizione $\delta$
    \item un insieme $F$ di stati finali
  \end{itemize}
  Si hanno inoltre i seguenti stati finali:
  \begin{itemize}
    \item $H$, per l'\textit{halt}
    \item $Y$, per lo \textit{yes}
    \item $N$, per il \textit{no}
  \end{itemize}
  Il simbolo $\sqcup$ specifica che non ho un simbolo e il simbolo
  $\triangleright$ mi specifica che da lì parte l'input.
\end{definizione}
\begin{definizione}
  La funzione di transizione esprime cosa fa passo-passo la TM:
  \[\delta:K\times\Sigma\to K\times \Sigma\times\{\leftarrow,\rightarrow,-\}\]
  Ovvero prende in input uno stato e un simbolo e può avere in input un cambio
  di stato oppure il cambio del simbolo in quel punto o lo spostamento della
  testina di una posizione (che può comunque restare ferma).\\
  Si possono avere diverse varianti di implementazioni, obbligando al testina a
  spostarsi (andando avanti e indietro per indicare che deve stare ferma
  etc$\ldots$).\\
  Vedremo poi che avere questa funzione di transazione comporta l'avere una
  \textbf{TM deterministica} e che si potrà sviluppare una \textbf{TM non
    deterministica}.
\end{definizione}
Ogni operazione sulla TM ha lo stesso tempo e quindi posso usare il numero di
passi per calcolare il tempo di risoluzione.\\
Per esprimere la computazione di una TM usiamo una \textbf{configurazione},
ovvero sulla base della definizione della TM e dello stato attuale devo
definire tutti i passi.
\begin{definizione}
  Un \textbf{configurazione} di una TM è definita da:
  \begin{itemize}
    \item lo stato in cui si trova
    \item la stringa sul nastro (definita da tutti i simboli a destra della
    testina e tutti quelli a sinistra, ai quali viene aggiunto quello sotto la
    testina)
    \item la posizione delle testina 
  \end{itemize}
  In base alla configurazione la TM saprà come procedere.\\
  La configurazione descrive in ogni istante lo stato della macchina e quindi si
  ha la seguente \textbf{configurazione iniziale}, per la stringa $X$ (che ha un
  carattere per posizione del nastro, al quale comunque viene aggiunto lo
  \textit{start} $triangleright$):
  \[(k_0,\triangleright X, 1)\]
  e ad un certo punto sia arriverà ad uno stato di arresto, per esempio dopo
  aver cambiato $X$ in $Y$ ed essere tornata nello stato 2:
  \[(H,\triangleright Y, 2)\]
  e quindi output sarà la stringa $Y$ (senza il $\triangleright$).\\
  Potrei avere anche $Y$ o $N$ al posto di $H$ in problemi decisionali, per
  capire magari se una certa stringa ha le caratteristiche desiderate o meno.
\end{definizione}
Vediamo alcuni esempi di costruzione di TM:
\begin{esempio}
  Si scriva la TM che calcoli il successore di un numero binario, che sarà
  l'input (e si da per scontato che sia correttamente formattato avendo solo 0 o
  1 come simboli). Si trascuri il riporto (nel senso che non aggiungo ulteriori
  bit).\\
  \begin{shaded}
    Vediamo nella pratica un esempio di somma binaria:\\
    prendo $01101010$ e sommo 1:
    \[10010101 \, +\]
    \[00000001=\]
    \[\rule{70pt}{.4pt}\]
    \[10010110\,\,\,\,\,\,\,\]
  \end{shaded}
  Definisco quindi la TM:
  \begin{itemize}
    \item $S=\{s_0,s_1\}$
    \item $\Sigma =\{\triangleright, \sqcup, 0,1\}$
    \item per la funzione di transizione si ha:
    \[\delta\to(s_0,[\triangleright, 0,1])\to(s_0,[\triangleright, 0,1],
      \rightarrow)\]
    \[\delta\to(s_0,\sqcup)\to(s_1,\sqcup,\leftarrow)\]
    \[\delta\to(s_1,0)\to(H,1,-)\]
    \[\delta\to(s_1,1)\to(s_1,0,\leftarrow)\]
    \[\delta\to(s_1,\triangleright)\to(H,\triangleright,-)\]

    ovvero scorro fino alla fine e inverto l'ultimo numero 
    (se è 0 diventa 1 e fine ma se è 1 lo rendo 0 e poi mi sposto a sinistra e
    se è un 1 diventa 0 e così via, fino alla
    fine dove metto 1, come prevede la somma binaria)
    \item $s_0$ è lo stato iniziale
  \end{itemize}
\end{esempio}
Volendo ad un certo punto si arriva allo stato finale $H$. In quel momento ho
una nuova stringa $Y$ (il risultato delle modifiche su $X$):
\[(H,\triangleright Y, -)\]
e la testina non deve più spostarsi. Questa può essere interpretata come uno
\textbf{stato di arresto}.\\
Un altro caso è avere una computazione infinita nel caso in cui, letto un
simbolo e lo stato, la testina ricopia il simbolo ma non si sposta ne a destra
ne a sinistra. Oppure una testina potrebbe ``rimbalzare'' infinitamente tra due
posizioni. Quest'ultima cosa può anche accadere tra molte posizioni, entrando
comunque in \textbf{loop infinito}, ritornando sempre, prima o poi, in una
configurazione (e si noti ``configurazione'', non coppia stato-simbolo) già
avuta. In questo caso la computazione è \textbf{non terminante} e la
computazione non produce alcun output. \textit{Quindi un insieme finito di
  elementi (stati, simboli e possibili transazioni) può comunque descrivere un
  comportamento infinito (infiniti passi)}.\\ 
Analizziamo meglio gli stati terminanti $Y$ e $N$ il primo indicante che la
stringa in input è accettata, avendo certe caratteristiche richieste, il secondo
che la stringa viene rifiutata. Non ho quindi più bisogno della stringa in
output, quindi posso lasciar scritto quello che voglio quando finisco, mi basta
sapere lo stato finale $Y$ e $N$.
\begin{esempio}
  Sistemiamo l'esempio precedente aggiungendo il riporto, dovendo aggiungere un
  bit. \\
 \textit{Si indica solo la funzione di transizione}.\\
  Scorro fino alla fine (quindi vado a destra indipendentemente dal simbolo fino
  ad un blank):
  \[\delta\to(s_0,[\triangleright, 0,1])\to(s_0,[\triangleright, 0,1],
    \rightarrow)\]
  Sono a destra dell'ultimo carattere di $X$ (avendo un blank):
  \[\delta\to(s_0,\sqcup)\to(s_1,\sqcup,\leftarrow)\]
  eseguo il conto tornando indietro:
  \[\delta\to(s_1,0)\to(H,1,-)\]
  \[\delta\to(s_1,1)\to(s_1,0,\leftarrow)\]
  sono tornato all'inizio:
  \[\delta\to(s_1,\triangleright)\to(s_2,1,\leftarrow)\]
  devo ``shiftare'' tutti i caratteri (per farlo semplicemente metto
  $\triangleright$ nel primo blank a sinistra del precedente $\triangleright$
  arrivando nello stato $s_2$, come indicato nello step precedente):
  \[\delta\to(s_2,\sqcup)\to(H,\triangleright,-)\]
  (avrei comunque potuto shiftare tutte le unità a destra di una posizione, o
  semplicemente, sapendo che ora sul nastro ho soli 0 dovrei tornare a destra di
  un passo, cambiare il primo 0 in 1 e aggiungere uno 0 in fondo alla
  stringa).
\end{esempio}
\begin{esempio}
  Vediamo un esempio in cui una TM riconosce una stringa binaria che è
  palindroma o meno.\\
  \textit{Si indica solo la funzione di transizione}.\\
  Se sono sullo start vado a destra di uno:
  \[\delta(s_0,\triangleright)\to (s_0,\triangleright, \rightarrow)\]
  se vedo 0 vado nello stato $zero$, scrivo $\sqcup$ e vado a destra:
  \[\delta(s_0,0)\to (zero,\sqcup, \rightarrow)\]
  analogo leggendo 1, andando nello stato $1$ scrivendo blank:
  \[\delta(s_0,1)\to (one,\sqcup, \rightarrow)\]
  vado in fondo alla stringa (qualsiasi incrocio tra gli stati $one$ e $zero $ e
  simboli 1 e 0 legga vado a destro riscrivendo lo stesso simbolo):
  \[\delta\to([zero, one],[0,1])\to(\delta\to([zero, one],[0,1],\rightarrow)\]
  sono in fondo alla stringa, se sono in stato $zero$ scrivo blank e torno
  indietro, in uno stato $zero'$:
  \[\delta(zero, \sqcup)\to(zero', \sqcup, \leftarrow)\]
  idem per stato $one$
  \[\delta(one, \sqcup)\to(one', \sqcup, \leftarrow)\]
  se sono in $zero'$ e leggo $0$ vado in stato $s_1$ e vado a sinistra: 
  \[\delta(zero', 0)\to(s_1,\sqcup, \leftarrow)\]
  se sono in $zero'$ e leggo $1$ la stringa non è palindroma, esco con stato
  $N$:
  \[\delta(zero', 1)\to(N,\sqcup, \leftarrow)\]
  idem per stato $one'$, andando in stato $s_2$:
  \[\delta(one', 1)\to(s_2,\sqcup, \leftarrow)\]
  \[\delta(one', 0)\to(N,\sqcup, \leftarrow)\]
  ora proseguo a sinistra fino ad un blank riscrivendo quanto letto:
  \[\delta(s_1,[0,1])\to(s_1,[0,1], \leftarrow)\]
  sono nel blank e torno allo stato iniziale, potendo ricominciare la
  computazione:
  \[\delta(s_1\sqcup)\to(s_0,\sqcup, \rightarrow)\]
  ma se la stringa (di cardinalità pari) è palindroma cancello tutto, devo
  quindi specificare che se in $s_0$ ho blank la stringa è valida:
  \[\delta(s_0,\sqcup)\to(Y, \sqcup,-)\]
  se invece la stringa è di cardinalità dispari, allo stato attuale, entra in
  loop, dobbiamo quindi aggiungere un uscita d:a $zero'$ o $one'$ in questo
  caso:
  \[\delta([zero',one'], \sqcup)\to(Y, \sqcup, -)\]
\end{esempio}
\begin{esempio}
  Scrivo una TM per decidere se la stringa in input è del tipo $a^nb^nc^n$.\\
  \textit{Si indica solo la funzione di transizione}.\\
  Se leggo $a$ vado nello stato $A$ e vado a destra
  \[\delta(s_0,a)\to (A,\sqcup, \rightarrow)\]
  Se leggo in $s_0$ $b$ o $c$ significa che non ho $a$ quindi esco:
  \[\delta(s_0,[b,c])\to (N,\sqcup, -)\]
  Se in $A$ trovo un'altra $a$ la lascio e vado a destra:
  \[\delta(A,a)\to (A,a', \rightarrow)\]
  Se in $A$ leggo $b$ vado in $B$, e vado a destra:
  \[\delta(A,b)\to (B,b', \rightarrow)\]
  se invece leggo $c$ non ho $b$ e esco:
  \[\delta(A,c)\to (N,\sqcup, -)\]
  se in $B$ e trovo $a$ non va bene:
  \[\delta(B,a)\to (N,\sqcup, -)\]
  Se in $B$ trovo un'altra $b$ la lascio e vado a destra:
  \[\delta(B,b)\to (B,b, \rightarrow)\]
  Se in $B$ leggo $c$ vado in $C$ e vado a sinistra a controllare:
  \[\delta(B,c)\to (C,c', \leftarrow)\]
  controllo tramite i sentinella indicati con $'$.\\
  Siamo in $C$ quindi:
  \[\delta(C,c')\to (C,c', \leftarrow)\]
  Se però vedo $b$ o $b'$ resto in $C$ e riscrivo:
  \[\delta(C,[b,b'])\to (C,[b,b'], \leftarrow)\]
  Per $A$ cambia, se vedo $a$ scorro:
  \[\delta(C,a)\to (C,a, \leftarrow)\]
  ma se vedo $a'$ torno in $s_0$:
  \[\delta(C,a')\to (s_0,a', \rightarrow)\]
  ma ora $A$ può incontrare $b'$, che va bene, o $c'$ che non va bene:
  \[\delta(A,b')\to (A,b', \rightarrow)\]
  \[\delta(A,c')\to (N,\sqcuo, -)\]
  Per $C$:
  \[\delta(C,[b,c])\to (C,[b,c'], \rightarrow))\]
  Se sono in $s_0$ e trovo $a'$ ho finito le $a$:
  \[\delta(s_0,a')\to (s_0,a' , \rightarrow))\]
  idem:
  \[\delta(s_0,[b',c')\to (s_0,[b',c'] , \rightarrow))\]
  Vado in stato $check$ se:
  \[\delta(s_0,\sqcup)\to(check,\sqcup, \leftarrow)\]
  e proseguo fino a trovare solo $b'$ o $c'$, se trovo invece $b$ o $c$ esco.
  Diventa quindi molto lungo.
\end{esempio}
L'esempio sopra mostra quanto possa diventare lunga una semplice computazione
sulla TM, ci servirebbe quindi una sorta di \textit{TM alternativa}.
\begin{teorema}[teorema di B\"{o}hm-Jacopini]
  Qualunque algoritmo può essere implementato utilizzando tre sole strutture, la
  sequenza, la selezione e il ciclo, da applicare ricorsivamente alla
  composizione di istruzioni elementari
\end{teorema}
\begin{definizione}
  Si ha la \textbf{TM a $k$ nastri}, ovvero la \textbf{TM
    multi-nastro} dove ho $k$ nastri di lettura e scrittura, magari avendo
  l'input solo su un nastro o su multipli. La macchina quindi legge uno stato e
  $k$ simboli $\{\sigma_1\ldots,\sigma_k\}$. Prima dello spostamento quindi
  scrive tutti i simboli. Lo spostamento sarà uno per ogni nastro.
\end{definizione}
\begin{esempio}
  Risolviamo il precedente problema (decidere se la stringa in input è del tipo
  $a^nb^nc^n$) con $k$ nastri.\\
  Potrei usare 4 nastri, sul primo c'è l'input. Finché trovo $a$ scrivo sul
  primo, quando trovo $b$ passo a secondo e faccio lo stesso se leggo $c$. In
  ogni caso mi interrompo se dopo delle $b$ trovo $a$ o dopo $c$ trovo
  $a,b$. Infine torno indietro sui tre nastri e vedo se sono allineati.
\end{esempio}
Una \textbf{TM a $k$ nastri} non è più potente di una \textbf{TM a singolo
  nastro} (vale il rapporto linguaggio di programmazione e linguaggio
macchina). Esiste sempre una traduzione verso la TM a singolo nastro.\\
Per esempio invece un automa a stati finiti è meno potente di un TM, che può
spostarsi e scrivere dove vuole, a differenza degli automi.\\
Vediamo cosa quindi può fare una TM:
\begin{itemize}
  \item una TM può \textbf{computare} funzioni su stringhe
  \item una TM può \textbf{decidere} (rispondendo $Y$ o $N$) un linguaggio
  (ovvero un insieme finito o infinito di stringhe, come il caso sopra
  $a^nb^nc^n$), ovvero data una stringa in input è sempre in grado di dire se
  appartiene o meno al linguaggio. Un linguaggio è \textbf{decidibile}, o, più
  formalmente, \textbf{ricorsivo}, se esiste
  almeno una TM che decide il linguaggio, fermandosi sempre in $Y$ o
  $N$. Solitamente è un linguaggio finito, ma potrebbe anche essere infinito (ma
  tutte le stringhe sarebbero comunque riconoscibili)
  \item una TM può \textbf{accettare} un linguaggio, ovvero se fornisco una
  stringa in input che appartiene al linguaggio la riconosce come tale e prima o
  poi arriva allo stato $Y$, altrimenti la macchina potrebbe:
  \begin{itemize}
    \item fermarsi in uno stato $N$
    \item entrare in un loop infiniti, non avendo mai la risposta ma magari in
    realtà non è un loop infinito ma solo servono anni per avere la risposta
  \end{itemize}
  Quindi la TM non è in grado di distinguere completamente le stringhe che non
  fanno parte di un linguaggio. Un linguaggio è \textbf{linguaggio
    ricorsivamente enumerabile} è un linguaggio per cui data una stringa in
  input buona la TM si ferma, altrimenti la TM potrebbe o fermarsi o andare
  avanti all'infinito nella computazione. Posso enumerare tutte le stringhe che
  fanno parte del linguaggio. Potrei quindi avere un linguaggio infinito ma con
  stringhe che mandano in loop la TM. Ci sono linguaggi che si può dimostrare
  essere ricorsivamente enumerabili (e alcuni che non sono nemmeno
  ricorsivamente enumerabili)
\end{itemize}
Ricordiamo comunque che esistono problemi irrisolvibili, dimostrati tali, come la
soluzione di \textit{equazioni Diofantee}.




\chapter{Pattern matching}
Il \textbf{pattern matching} si occupa di ricercare un pattern $P$ in un testo
$T$. Si hanno:
\begin{itemize}
  \item ricerca esatta
  \item ricerca approssimata
\end{itemize}
\begin{definizione}
  Definiamo stringa come una sequenza di simboli appartenenti ad un dato
  alfabeto $\Sigma$, scritta come:
  \[X=x_1,\ldots,x_n,\,\,\,\forall x_i\in \Sigma\]
  ovviamente non è necessario che la stringa contenga tutti i simboli
  dell'alfabeto.
  Diamo alcune definizioni utili:
  \begin{itemize}
    \item $|X|$ indica la lunghezza della stringa
    \item $\varepsilon$ indica la stringa vuota
    \item $X[i]$ indica il carattere all'indice $i$ (partendo da 1 e non da 0)
    \item $X[i,j]$n indica la sottostringa che parte dall'indice $i$ e arriva
    all'indice $j$ (estremi inclusi). Inoltre se si hanno $i\neq j$ e $j\neq
    |X|$ si ha una \textbf{sottostringa propria}.\\
    (Esempio: per $X=bbaccbbaac$ ho la sottostringa $X[4,8]=ccbba$)
    \item $X[1,j]$ indico un \textbf{prefisso} (estremo finale incluso). Si ha
    inoltre il \textbf{prefisso proprio} se $j\neq |X|$ e si ha \textbf{prefisso
      nullo} se $j=0$ e si indica con $\varepsilon$.\\
    (Esempio: per $X=bbaccbbaac$ ho il prefisso $X[1,4]=bbac$)
    \item $X[i,|X|]$ indica un \textbf{suffisso} della stringa (estremi
    inclusi), di lunghezza $k=|X|-i+1$, avendo quindi il suffisso
    $X[|X|-k+1,|X|]$. Si ha inoltre che se ho $X[|X|,|X|+1]$ 
    allora ho il \textbf{suffisso nullo}, ovvero $\varepsilon$.\\
    (Esempio: per $X=bbaccbbaac$ ho il suffisso $X[8,10]=aac$ e il suffisso
    $X[11,10]=\varepsilon$)
    
\end{itemize} 
\end{definizione}
\begin{definizione}
  Definiamo il \textbf{pattern matching esatto} come la ricerca di tutte le
  occorrenze esatte di un pattern $P$ in un testo $T$.\\
  $P$ occorre esattamente in $T$, a partire dall'indice $i$ in $T$, sse:
  \[T[i,i+|P|-1]\mbox{ coincide con }P\]
  Quindi in output ho tutte le posizione $i$ nel testo tali per cui:
  \[T[i,i+m-1]\mbox{ coincide con }P\]
\end{definizione}
\begin{esempio}
  Dato il testo $T=bbaccbbaac$ e il pattern $P=ccbb$ ho un'occorrenza esatta a
  partire da $i=4$ nel testo.
\end{esempio}
L'algoritmo naive sarebbe quello di prendere una finestra $W$, di lunghezza $P$,
da far scorrere sul testo (avanzando ogni volta di un carattere). Ogni volta
tale finestra viene confrontata con $P$ confrontando tutti i caratteri e, in
caso di match, stampando in output l'indice di inizio della finestra sul
testo. Questo algoritmo è $O(|T|\cdot |P|)$ nel caso peggiore.\\ 
Questo algoritmo ignora le caratteristiche di testo e pattern, ignorando aspetti
che potrebbero accelerare la ricerca, trovabili tramite un'operazione di
preprocessamento.
\begin{definizione}
  Definiamo \textbf{distanza di edit} come il minimo numero di operazioni di
  sostituzione, cancellazione e inserimento di un unico simbolo per trasformare
  una stringa in un'altra (o viceversa). \\
  La distanza di edit tra due stringhe è simmetrica (anche se le operazioni
  saranno diverse il loro numero minimo sarà uguale).
\end{definizione}
\begin{esempio}
  Calcoliamo la distanza di edit tra $X_1=agtgcgt$ e $X_2=atgtgat$.\\
  Partiamo cancellando la $g$ in indice 2 di $X_1$:
  \[X_1=atgcgt\]
  \[X_2=atgtgat\]
  Inserisco quindi una ``a'' al penultimo indice di $X_1$:
  \[X_1=atgcgat\]
  \[X_2=atgtgat\]
  Infine sostituisco la ``c'' con una ``t'' all'indice 4 di $X_1$:
  \[X_1=atgtgat\]
  \[X_2=atgtgat\]
  Ho quindi una distanza di edit pari a 3.\\
  Per trasformare la seconda nella prima avrei dovuto inserire una ``g''
  all'indice 2 di $X_2$, cancellare la ``a'' al penultimo indice di $X_2$ e
  infine sostituire la ``t'' con una ``c'' al terzultimo indice di $X_2$. Si
  vede quindi come anche in questo caso avrei distanza di edit pari a 3.
\end{esempio}
\begin{definizione}
  Definiamo il \textbf{pattern matching approssimato} come la ricerca di
  occorrenze approssimate di un pattern $P$ in un testo $T$, con al più un certo
  errore $k$.\\
  Viene usata la \textnormal{distanza di edit}, che rappresenta l'errore che si
  commette nell'eguagliare una stringa con un altra.\\
  $P$ ha un'occorrenza approssimata in $T$, che finisce all'indice $i$, se
  esiste almeno una sottostringa $T[i-L+1,i]$ che ha distanza di edit con $P$ di
  al più $k$. $L$ è un certo valore non prevedibile in quanto sto ammettendo che
  il match sia su una stringa di lunghezza diversa da quella del
  pattern. Ovviamente in questo caso si possono avere più sottostringhe
  accettabili secondo i parametri richiesti.\\
  In output ho quindi tutte le posizioni $i$ di $T$ in cui finisce almeno una
  sottostringa che ha con $P$ distanza di edit al più uguale a $k$.
\end{definizione}
\begin{esempio}
  Prendiamo il testo $T=bbaccbbac$ e $P=ccbb$.\\
  Impongo $k=1$ e vedo che un match è ``ccbb'' terminante all'indice 6 del testo
  (avendo errore 1 massimo). Ovviamente ``ccbb'' terminante all'indice 7 va bene
  avendo errore nullo.\\
  
\end{esempio}
\begin{definizione}
  Definiamo \textbf{bordo} della stringa $X$, $B(X)$, che è il più lungo
  prefisso proprio che occorre come suffisso di $X$.
  \begin{esempio}
    Vediamo qualche esempio:
    \[X=baacagahabaac\implies B(X)=baac\]
    \[X=abababa\implies B(X)=ababa\]
    \[X=aaaaaa\implies B(X)=aaaaa\]
    \[X=aaaccbbaac\implies B(X)=\varepsilon\]
    \[X=aaaaaaaa\implies B(X)=aaaaaaa\]
  \end{esempio}
  Ovviamente può essere $\varepsilon$ (in primis se ho una stringa di lunghezza
  1).
\end{definizione}
\begin{definizione}
  Definiamo concatenazione tra una stringa $X$ e un simbolo $\sigma$ come:
  $X\sigma$
\end{definizione}
\begin{esempio}
  Dato $X=bba$ e $\sigma=d$ ho:
  \[X\sigma = bbad\]
\end{esempio}
\section{Pattern matching con automi}
Siamo nell'ambito del pattern matching esatto.\\
Ho un pattern di lunghezza $m$ e un testo di lunghezza $n$.\\
Abbiamo una fase di preprocessamento in tempo $O(m|\Sigma|)$ per calcolare la
funzione di transizione $\delta$.\\
Definiamo $\delta$ definita sul pattern $P$ come:
\[\delta:\{0,1,\ldots, m\}\times \Sigma\to\{0,\,\ldots, m\}\]
Quindi ad ogni coppia tra un simbolo e un intero tra 0 e $m$ corrisponde un
intero tra 0 e $m$. Nel dettaglio si hanno due casi:
\begin{enumerate}
  \item primo caso:
  \[\delta(j,\sigma)=j+1\iff j<m\land P[j+1]=\sigma\]

  \item secondo caso:
  \[\delta(j,\sigma)=k\iff P[j+1]\neq \sigma \lor j=m,\mbox{ con
    }k=|B(P[1,j]\sigma)|\]
  Con $k$ che è la lunghezza del bordo tra il prefisso corrente a cui viene
  concatenato il carattere $\sigma$. Si ha che $k\leq j$ per definizione (dato
  che sto calcolando il bordo su una stringa di lunghezza $j+1$). Con questa 
  transizione posso, eventualmente, andare ``indietro'' nel pattern ma comunque
  ``in avanti'' nell'automa.
\end{enumerate}
La transizione dallo stato $j$ allo stato $\delta(j,\sigma)$ avviene attraverso
il simbolo $\sigma$. Posso quindi passare allo stato $j+1$ o allo stato $k$.\\
Nell'automa avremo quindi gli archi etichettati coi simboli e i nodi
etichettati con gli indici da 0 a $m$.\\
Nella stringa gli indici partono comunque da 1 quindi l'indice 0 nelle formule
rappresenta una sorta di posizione prima dell'inizio della stringa. Qualora $k$
sia pari a 0 per la seconda formula mi sposto effettivamente in questo indice
posto prima della stringa (anche se avrò due stati diversi etichettati come 0
nell'automa).
\begin{esempio}
  Prendiamo il pattern:
  \[P=acacbabbaabac\]
  calcoliamo:
  \begin{itemize}
    \item $\delta(6,b)=7$ in quanto ho effettivamente $b$ come simbolo
    all'indice successivo a $6$, uso quindi la prima formula
    \begin{center}
      \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
        \node[state] (q_0) {$6$};
        \node[state] (q_1) [right=of q_0] {$7$};
        \path[->]
        (q_0) edge  node {b} (q_1);
      \end{tikzpicture}
    \end{center}
    \item $\delta(0,a)=1$, in quanto $a$ è il primo carattere, uso quindi la
    prima formula
     \begin{center}
      \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
        \node[state] (q_0) {$0$};
        \node[state] (q_1) [right=of q_0] {$1$};
        \path[->]
        (q_0) edge  node {a} (q_1);
      \end{tikzpicture}
    \end{center}
    \item $\delta(6,c)=2$ in quanto devo usare la seconda formula e avere:
    \[|B(P[1,6]c)|=|B(acacbac)|=|ac|=2\]
    \begin{center}
      \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
        \node[state] (q_0) {$6$};
        \node[state] (q_1) [right=of q_0] {$2$};
        \path[->]
        (q_0) edge  node {c} (q_1);
      \end{tikzpicture}
    \end{center}
    
    \item $\delta(0,c)=0$ in quanto devo usare la seconda formula e avere:
    \[|B(P[1,0]c)|=|B(c)|=|\varepsilon|=0\]
    \begin{center}
      \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
        \node[state] (q_0) {$0$};
        \node[state] (q_1) [right=of q_0] {$0$};
        \path[->]
        (q_0) edge  node {c} (q_1);
      \end{tikzpicture}
    \end{center}
    \item $\delta(13,a)=4$ in quanto devo usare la seconda formula e avere:
    \[|B(P[1,14]a)|=|B(acacbabbaabaca)|=|aca|=3\]
    \begin{center}
      \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
        \node[state] (q_0) {$13$};
        \node[state] (q_1) [right=of q_0] {$3$};
        \path[->]
        (q_0) edge  node {a} (q_1);
      \end{tikzpicture}
    \end{center}
  \end{itemize}
\end{esempio}
Facciamo due osservazioni:
\begin{enumerate}
  \item dallo stato 0 si arriva allo stato 0 per qualsiasi simbolo $\sigma\neq
  P[1]$ e quindi si arriva allo stato 1 attraverso $\sigma =P[1]$
  \item dallo stato $m$ si arriva sempre ad uno stato $k\leq m$ e da uno stato
  $m$ si può arrivare di nuovo ad uno stato $m$
\end{enumerate}
\begin{esempio}
  Se ho $P=aaaaaa$:
  \[\delta(6,a)=6\]
  in quanto:
  \[l=|B(aaaaaaa)|=|aaaaaa|=6\]
\end{esempio}
\begin{esempio}
  Sia dato un pattern $P=acacbac$, con $m=7$ e $\sigma=\{a,b,c\}$.\\
  Si hanno le seguenti transizioni $\delta(j,\sigma)$, calcolate solo sulla base
  delle formule.\\
  Parto con $\delta(j,\sigma)=j+1$:
  \begin{table}[H]
    \centering
    \begin{tabular}[H]{c||c|c|c}
      $j\backslash\sigma$ & a & b & c\\
      \hline
      \hline
      0 & 1 & & \\
      1 & & & 2\\
      2 & 3 & & \\
      3 & & & 4\\
      4 & & 5 & \\ 
      5 & 6 & & \\
      6 & & & 7\\
      7 & & &       
    \end{tabular}
  \end{table}
  Proseguo con $\delta(j,\sigma)=k$:
  \begin{table}[H]
    \centering
    \begin{tabular}[H]{c||c|c|c}
      $j\backslash\sigma$ & a & b & c\\
      \hline
      \hline
      0 & 1 & 0 & 0\\
      1 & 1 & 0 & 2\\
      2 & 3 & 0 & 0\\
      3 & 1 & 0 & 4\\
      4 & 3 & 5 & 0\\ 
      5 & 6 & 0 & 0\\
      6 & 1 & 0 & 7\\
      7 & 3 & 0 & 0      
    \end{tabular}
  \end{table}
\end{esempio}
In realtà algoritmo procede per induzione di riga in riga, riempendo la riga
basandosi con quella precedente, in $O(m|\Sigma|)$.\\
\newpage
\begin{esempio}
  Sia dato un pattern $P=acacbac$, con $m=7$ e $\sigma=\{a,b,c,d\}$.\\
  Aggiungo quindi un simbolo non appartenente al pattern, si ottiene:
   \begin{table}[H]
    \centering
    \begin{tabular}[H]{c||c|c|c|c}
      $j\backslash\sigma$ & a & b & c & d\\
      \hline
      \hline
      0 & 1 & 0 & 0 & 0\\
      1 & 1 & 0 & 2 & 0\\
      2 & 3 & 0 & 0 & 0\\
      3 & 1 & 0 & 4 & 0\\
      4 & 3 & 5 & 0 & 0\\ 
      5 & 6 & 0 & 0 & 0\\
      6 & 1 & 0 & 7 & 0\\
      7 & 3 & 0 & 0 & 0      
    \end{tabular}
  \end{table}
  Si aggiunge quindi una colonna di soli 0
\end{esempio}
\end{document}


% LocalWords:  Machine Learning NP TM yes sse Access pseudocodice store add TSP
% LocalWords:  load esguite dimostrabilmente primis Stirling NTDM vertex cover
% LocalWords:  exptime all img problem png independent ind nell CNF definsice
% LocalWords:  soddisfacibilità polinomialmente dell clique matching naive edit
% LocalWords:  sottostringa preprocessamento sottostringhe baacagahabaac step
% LocalWords:  halt approximation greedy lower bound return shiftare blank Bohm
% LocalWords:  Jacobini Jacopini Diofantee
