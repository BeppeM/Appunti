\chapter{Ipotesi non Parametriche}
Nel precedente capitolo abbiamo visto ed analizzato i test di ipotesi effettuati su dei parametri di una popolazione, su
cui si effettuano delle assunzioni sulla tipologia di distribuzioni ma ora ci occupiamo dei test di ipotesi non
parametriche, in cui non si effettua nessuna assunzione sulla distribuzione del campione/popolazione.  

In questo capitolo verranno introdotti i seguenti test:
\begin{itemize}
    \item \emph{test per la bonta' dell'adattamento}, che servono per verificare se una popolazione segue una distribuzione prestabilità
    \item \emph{test per confrontare le distribuzioni di due popolazioni}, quando queste non sono necessariamente delle normali
    \item test per verificare se sussiste \emph{indipendenza o incorrelazione} tra due diversi caratteri di una popolazione
\end{itemize}
Anche per questi test valgono le considerazioni viste in merito agli errori di prima e seconda specie ed alle regioni
critiche e di accettazione nel capitolo precedente sulle ipotesi parametriche.

\section{Test per la bontà dell'adattamento}
I test relativi alla bontà dell'adattamento pongono a capire se una popolazione $X$ è distribuita seconda una funzione
di ripartizione $F$, ossia sono dei test la cui ipotesi nulla è $H_0:F_X(t) = F(t) \forall t \in \R$ con ipotesi
alternativa $H_1:F_X(t) \neq F(t) \mbox{ per almeno un t in} \R$.\newline
I due test riguardanti la bontà dell'adattamento analizzati in questo corso sono i seguenti:
\begin{itemize}
    \item \emph{test di Kolmogorov-Smirnov} si basa sulla popolazione empirica, introdotta nel seguito del paragrafo, e
        sulla statistica
        \[ D_n = \sup _{t \in \R} |F(t) - \bar{F}_{X, n}(t)| \]
        che specifica l'estremo superiore delle distanze in valore assoluto tra la funzione di ripartizione che vogliamo
        controllare come possibile per X e quella empirica, ottenuta tramite il campione disponibile.\newline
        Si può dimostrare che quando vale $H_0$ e $F$ è una funzione continua allora tale statistica è indipendente 
        dalla forma di $F$, ovvero ha la stessa distribuzione qualunque sia $F$ e pertanto è possibile fissare delle 
        regioni di accettazione e critiche al variare di $n$ e del livello di significatività.\newline
        La distribuzione della statistica campionaria $D_n$ al variare di $n$ è stata studiata dagli inventori,
        che hanno fornito apposite tabelle per determinare i quantili.

        Si può notare che è logico aspettarsi che  $D_n$ assume valori piccoli se l’ipotesi nulla è vera, ed assume 
        valori grandi quando $H_0$ è falsa.\newline
        Infatti, per ogni ampiezza $\alpha$ del test la regione critica risulta essere
        \[ C = (d_{1 - \alpha}, 1] \]
        dove il quantile $d_{1 - \alpha}$ è quel valore per cui risulta 
        \[ P(D_n \leq d_{1 - \alpha}) = 1 - \alpha \]
        che può essere determinato sulle apposite tavole(si noti che $D_n$ non può sicuramente assumere valori maggiori di $1$)

        Si osservi che la massima distanza tra la funzione di ripartizione $F$ e la funzione di ripartizione empirica 
        viene raggiunta sempre in corrispondenza di uno dei salti della distribuzione empirica.\newline
        Questo fatto può essere utile nella determinazione di $D_n$, quando non è possibile fare uso di rappresentazioni grafiche.
        
        Osserviamo che il test di Kolmogorov-Smirnov non richiede particolari assunti sui dati o sulla distribuzione $F$
        per essere impiegato, se non quello di continuità della $F$ stessa.

        Dato un campione $(X_1, X_2, \dots, X_n)$ estratto da una popolazione, si definisce \emph{funzione di ripartizione empirica}
        della popolazione $X$, basata sul campione $(X_1, X_2, \dots, X_n)$, la funzione aleatoria
        \[ \bar{F}_{X, n}(t) = \frac{1}{n} \sum _{i = 1}^n U_{(-\infty, t]}(X_i) \forall t \in \R \]
        dove si definisce
        \[ U_{-\infty, t]}(X_i) = \begin{cases}
                                    1 \mbox{ se } X_i \in (-\infty, t] \\
                                    0 \mbox{ altrimenti}
                                  \end{cases} \]
        Si osservi che le funzioni di ripartizione empiriche sono sempre delle funzioni crescenti a gradino ed è facile
        rendersi conto poi che al crescere della numerosità $n$ del campione la funzione di ripartizione empirica 
        assomiglia sempre di più alla reale funzione di ripartizione della popolazione, fino a coincidere quando $n$
        corrisponde alla numerosità dell'intera popolazione.
    \item \emph{test del chi-quadro} può essere usato senza porre condizioni sulla funzione di ripartizione $F$. 
          Anche per poter descrivere questo test occorre introdurre alcune nozioni e notazioni e come al solito supporremo
          di poter estrarre un campione $(X_1, X_2, \dots, X_n)$ di numerosità $n$
          Un approcio classico per ottenere la bontà di adattamento di un test consiste nel partizionare i valori possibili di una variabile aleatoria di un numero finito di regioni,
          i cui il numero di valori presenti viene poi determinato e valutato con il valore atteso sotto una specifica distribuzione probabilistica e quando sono significatamente distanti           l'ipotesi nulla $H_0$ viene rifiutata.\newline
          Supponiamo che $n$ variabili aleatorie $(Y_1, Y_2, \dots, Y_n)$ ognuna avente uno dei valori $1\dots k$, vengono osservate e noi siamo interessati all'ipotesi nulla 
          $H_0:P(Y = i) = p_i  i = 1 \dots k$ vs $H_1: P(Y = i) \neq p_i$.\newline
          Per effettuare il seguente test poniamo $X_i i = 1 \dots k$ a denotare il numero di $Y_j$ che sono uguali a $i$
          e ogni $Y_j$ indipendenti tra di loro è uguale a $i$ con probabilità $P(Y = i)$.\newline
          Segue che sotto $H_0$, $X_i$ è una binomiale con parametri $n$ e $p_i$ ed in caso di $H_0$ verificato si ha $E[X_i] = np_i$ e quindi $(X_i - np_i)^2$ indica
          quanto è simile che $p_i$ è uguale a $P(Y = i)$ e quando questo è largo in relazione con $np_i$ allora $H_0$ non è corretta per cui consideriamo il seguente test statistico
          \[ T = \sum _{i = 1}^n \frac{(X_i - np_i)^2}{np_i} \]
          Per determinare la regione critica dobbiamo prima specificare un livello di significato $\alpha$ e poi determinare un valore critico $c$ tale che $P_{H_0}(T \geq c) = \alpha$.

          Il test è portato a rifiutare l'ipotesi nulla, con un livello di confidenza $\alpha$ quando si ha $T \geq c$ altrimenti accetta con $T < c$.\newline
          L'approccio classico per determinare $c$ consiste nell'usare il risultato che con $n$ largo e $H_0$ verificato, $T$ viene approssimato con una chi-quadro con $k - 1$
          gradi di libertà



\end{itemize}

\section{Test per l'uguaglianza di distribuzione}
Di seguito vengono presentati 3 test per verificare o rifiutare l’ipotesi che le distribuzioni
di due distinte popolazioni X ed Y siano identiche, non necessariamente gaussiane.

In questo paragrafo siamo interessati ad effettuare il test $H_0:F_X(t) = F_Y(t) \forall t \in \R$ con ipotesi
alternativa $H_1:F_X(t) \neq F_Y(t) \exists t \in \R$ e lo effettuiamo mediante tre diversi possibili test:
\begin{itemize}
    \item \emph{sign test}, basato sui segni negativi o positivi delle differenze tra le coppie di elementi
          presi dai campioni estratti dalle due popolazioni considerate.\newline
          Siano $(X_1, X_2, \dots, X_n)$ e $(Y_1, Y_2, \dots, Y_n)$ due campioni appaiati, estratti da $X$ e da $Y$ e
          ciò significa che la coppia $(X_i, Y_i)$ è relativa allo stesso individuo, ossia ad esempio una valutazione
          prima e dopo un avvenimento/esperimento.\newline
          Stabiliamo ora le seguenti tre quantità:
          \begin{itemize}
              \item $S^+ = \#i : X_i > Y_i$
              \item $S^- = \#i : X_i < Y_i$
              \item $S^= = \#i : X_i = Y_i$
          \end{itemize}
          Se l'ipotesi nulla $H_0$ è verificata è logico aspettarsi che le quantità $S^+$ e $S^-$ non si discostano
          molto tra di loro, infatti se $H_0$ è vera e se $n - S^= \geq 10$ la quantità $S_n$ risulta
          \[ S_n = S^+ - S^- \sim N(0, \frac{n - S^=}{2}) \]
          Da questa considerazione è possibile costruire una regione critica per $S_n$ ragionando nel solito modo 
          rifiutando $H_0$ quando $S_n$ si discosta troppo dallo zero per essere una normale e ragionando come nel
          capitolo sulla stima dei parametri, la regione di accettazione, con confidenza $\alpha$, per $S_n$ risulta essere
          \[ (-z_{1 - \frac{\alpha}{2}} \sqrt{\frac{n - S^=}{2}}, +z_{1 - \frac{\alpha}{2}} \sqrt{\frac{n - S^=}{2}}) \]
          dove $z_{1 - \frac{\alpha}{2}}$ risulta essere il quantile di ordine $1 - \frac{\alpha}{2}$ della normale standardizzata.
          
          È possibile effettuare il test anche quando la quantità $n - S^=$ è piccola ed in questo caso si può considerare
          la statistica $S^+$ e tener conto che quando sussiste l'ipotesi $H_0$ essa è distribuita come una binomiale di
          parametri $n - S^=$ e $\frac{1}{2}$.\newline
          Segnaliamo l’esistenza di un test simile a quello dei segni ma più elaborato, nel senso che tiene conto 
          non solo dei segni delle differenze ma anche delle ampiezze di queste differenze, e che porta il nome di \emph{test dei segni di Wilcoxon}.
    
    \item \emph{Wilcoxon ranked sign test} che è molto utile da usare anche per campioni non appaiati ed è molto potente.\newline
          Fornire un’espressione funzionale della statistica su cui questo test è basato è abbastanza complesso 
          pertanto ci limiteremo a descrivere il procedimento da seguire per effettuare il test senza cercare
          di fornire un’interpretazione anche solo intuitiva di esso.

          Siano quindi $(X_1, X_2, \dots, X_n)$ e $(Y_1, Y_2, \dots, Y_m)$ due campioni di lunghezza $n$ e $m$, estratti
          rispettivamente da $X$ e da $Y$.\newline
          Le fasi di procedura del test sono i seguenti:
          \begin{itemize}
              \item Si ordina in senso crescente l’insieme di tutti i dati e si associa a ciascun dato il proprio rango,
                    ovvero la posizione in cui si trova nella sistemazione in ordine crescente dei dati.
              \item Si sommano separatamente i ranghi relativi ai due campioni e siano $R_X$ e $R_Y$ le loro somme.
              \item Si calcolano le statistiche $U_X = n * m + \frac{n(n - 1)}{2} - R_X$ e $U_Y = n * m + \frac{n(n - 1)}{2} - R_Y$
                    e se i conti sono corretti la somma di $U_X$ ed $U_Y$ deve essere uguale al prodotto tra le
                    numerosità dei due campioni, vale a dire che deve essere $U_X + U_Y = n * m$.
              \item Si considera poi la statistica $U = \min (U_X, U_Y)$ e si può dimostrare che per $n$ ed $m$ 
                    sufficientemente grandi,in genere maggiori di 8, quando vale $H_0$ la U è approssimabile
                    tramite una normale con parametri $\mu_U = \frac{n * m}{2}$ e $\sigma_U^2 = \frac{n * m * (n + m + 1)}{12}$.
          \end{itemize}
          Tramite quello visto ed analizzato nei capitoli precedenti si riesce a mostrare che 
          l'intervallo diaccettazione del test, con significato $\alpha$, risulta essere 
          $(-z_{1 - \frac{\alpha}{2}}, +z_{1 - \frac{\alpha}{2}}$.\newline
          Nel caso in cui non sia verificata la condizione $n, m \geq 8$ allora è possibile ricorrere ad
          apposite tavole per determinare la regione critica per U.

    \item il terzo test è l'\emph{adattamento al test di Kolmogorov} al caso di due campioni, che non richiede che 
          i due campioni siano appaiati, ma in compenso può essere utilizzato solo quando ci siano validi motivi per
          pensare che la distribuzione delle popolazioni da confrontare sia continua.

          Siano quindi $(X_1, X_2, \dots, X_n)$ e $(Y_1, Y_2, \dots, Y_m)$ due campioni di numerosità $n$ e $m$,
          estratti rispettivamente da $X$ e $Y$ e siano poi $F_{X, n}$ e $F_{Y, m}$ le funzioni di ripartizione
          empiriche di $X$ e $Y$, ricavate tramite i due campioni e sia poi
          \[ D_{n, m} = \sup _{t \in \R} |F_{X, n}(t) - F_{Y, m}(t)| \]
          la statistica che specifica l’estremo superiore delle distanze, in valore assoluto, tra le due funzioni
          di ripartizione empiriche.\newline
          Anche in questo caso si può dimostrare che quando $H_0$ è vera e quando $F_X$ è una funzione continua,
          allora $D _{n, m}$ è indipendente dalla forma di $F_X$, ovvero ha la stessa distribuzione qualunque sia $F_X$.

          Anche per la distribuzione della statistica campionaria $D _{n, m}$ al variare di $n$ ed $m$ esistono 
          apposite tabelle per determinare i quantili ed è quindi possibile fissare al solito delle regioni di
          accettazione e critiche al variare di $n$ ed $m$ e del livello di significatività $\sigma$, in maniera uguale
          a quelli stabiliti per il test Kolmogorov-Smirnov nel caso di un solo campione.
\end{itemize}

\section{Test per l'indipendenza}
Un problema che si pone frequentemente nelle applicazioni è quello di stabilire se due caratteri di una popolazione
bidimensionale sono tra loro stocasticamente indipendenti oppure no.\newline
Per rispondere a questa domanda sono stati inventati diversi test ma uno in particolare viene sempre utilizzato ed
esso prende il nome di \emph{Test del chi-quadro per l'indipendenza} e come già nome ci porta a pensare ricorda molto la
formulazione del test chi-quadro per la bontà di adattamento.

Si consideri una popolazione bidimensionale $(X, Y)$ e si supponga di voler estrarre un campione casuale
$((X_1, Y_1), (X_2, Y_2), \dots, (X_n, Y_n))$ e si vuole controllare con livello di significatività $\alpha$
l'ipotesi $H_0:\mbox{i caratteri X e Y sono indipendenti}$, con ipotesi alternativa
$H_1:\mbox{i caratteri X e Y non sono indipendenti}$.\newline
Per effettuare il test del Chi-Quadro occorre inizialmente fare una partizione dei due supporti dei caratteri
$X$ ed $Y$ in un numero finito di intervalli ciascuno e definiamo $I_k^X \quad k \in 1,2,\dots M_X$ e 
$I_j^Y \quad j \in1,2, \dots, M_Y$ gli intervalli che definiscono una partizione del supporto di $X$ e $Y$.
Per ogni coppia $(k,j)$ consideriamo le quantità $n_k^X, n_j^Y, n_{k, j}$ indicanti il numero di elementi calcolati
sui intervalli $I_k^X, I_j^Y$ e $I_k^X \times I_j^Y$ e su cui è possibile calcolare le relative frequenze relative.

La quantità $f_{k, j}$ è detta \emph{frequenza relativa osservata} delle regioni $I_k^X \times I_j^Y$ e notiamo che per
ogni coppia $(k, j)$ il prodotto $f_k * f_j$ fornisce invece una \emph{frequenza relativa attesa} di elementi del
campione che devono ricadere nell'intervallo $I_k^X \times I_j^Y$ se fosse vera l'ipotesi nulla $H_0$, poichè in questo
caso la frequenza di ogni regione deve essere uguale al prodotto delle frequenze marginali di quella regione.\newline
Se $H_0$ è vero le differenze $f_{k, j} - f_k * f_j$ dovrebbero essere piccole in valore assoluto e consideriamo la statistica
\[ W = n \sum _{k = 1}^{M_X} \sum _{j = 1}^{M_Y} \frac{(f_{k, j} - f_k * f_j)^2}{f_k * f_j} \]
Si può mostrare che quando l’ipotesi nulla è vera e quando le $n_{k,j}$ sono sufficientemente grandi(almeno maggiori o
uguali a 5) allora $W$ è approssimatamente distribuita secondo una chi-quadro con $(M_k - 1) * (M_j - 1)$ gradi di libertà.

La regola di decisione del test segue in base alle considerazioni fatte sopra, saremo portati a rifiutare l’ipotesi nulla
quando la W assume valori troppo lontani dallo zero per essere una Chi-Quadro con opportuni gradi di libertà.\newline
Come nel caso del test Chi-Quadro per la bontà dell’adattamento anche il test Chi-Quadro per l’indipendenza 
ha il vantaggio di essere utilizzabile quando si considerano caratteri con distribuzioni non continue.\newline
Addirittura esso può essere utilizzato considerando caratteri con modalità non numeriche.

\section{Test per l'incorrelazione}
Il test di incorrelazione che presentiamo ora non è basato sul coefficiente di correlazione lineare di Pearson 
ma su un statistica campionaria che porta il nome di \emph{coefficiente di correlazione dei ranghi $R_s$ di Spearman}.\newline
Esso viene incluso nei test di tipo non-parametrico in quanto la determinazione di $R_s$ non coinvolge direttamente
i valori numerici assunti dai dati campionari ma solo i loro ranghi di cui viene fornita la definizione.

Consideriamo il campione $((X_1, Y_1), (X_2, Y_2), \dots, (X_n, Y_n))$ di numerosità $n$ estratto in modo casuale
dalla popolazione bidimensionale $(X,Y)$.\newline
Ordiniamo in senso crescente prima l’insieme dei dati di tipo $X_i$ e successivamente quelli di tipo $Y_i$ ed è detto
\emph{rango} di ciascun dato la posizione che esso assume nella sequenza così ottenuta di dati dello stesso tipo.\newline
Ad ogni coppia $(X_i, Y_i)$ associamo ora la corrispondente coppia di ranghi $(r_i^X, r_i^Y)$ e denotiamo la loro differenza con
\[ d_i = r_i^X - r_i^Y \].

Si definisce \emph{coefficiente di correlazione dei ranghi $R_s$ di Spearman} la statistica 
\[ R_s = 1 - \frac{6 * \sum _{i = 1}^n d_i^2}{n^3 - n} \]
Il coefficiente $R_s$ soddisfa proprietà simili a quelle di cui gode il coefficiente di correlazione lineare di Pearson
e consente di definire un nuovo tipo di incorrelazione(che chiameremo incorrelazione nel senso di Spearman) 
che si ha quando $R_s$ assume valore zero.\newline
La possibilità di utilizzare $R_s$ in un test di ipotesi deriva dal fatto che indipendentemente dalla forma 
della distribuzione congiunta $(X,Y)$ quando i caratteri $X$ ed $Y$ sono incorrelati (nel senso di Spearman)
e la numerosità del campione è maggiore di 10 allora la statistica
\[ \bar{T_n} = R_s * \sqrt{\frac{n - 2}{1 - R_s^2}} \] 
risulta essere approssimativamente distribuita come una t di Student con $(n-2)$ gradi di libertà.\newline
Dovendo valutare l'ipotesi $H_0: \mbox{i caratteri X e Y sono incorrelati secondo Spearman}$ con ipotesi alternativa
$H_1: \mbox{i caratteri X e Y sono correlati secondo Spearman}$ e possiamo pensare di accettare $H_0$ quando $\bar{T_n}$
assume valori non troppo distanti da zero e di rifiutarla in favore di $H_1$ in caso contrario per cui la nostra ipotesi
$H_0$ viene accettata, con confidenza $\alpha$, se la statistica $\bar{T_n}$ risiede nell'intervallo 
$(-t_{1 - \frac{\alpha}{2}}, +z_{1 - \frac{\alpha}{2}})$ altrimenti si accetta l'ipotesi $H_1$.
