\chapter{Ipotesi non Parametriche}
Nel precedente capitolo abbiamo visto ed analizzato i test di ipotesi effettuati su dei parametri di una popolazione, su
cui si effettuano delle assunzioni sulla tipologia di distribuzioni ma ora ci occupiamo dei test di ipotesi non
parametriche, in cui non si effettua nessuna assunzione sulla distribuzione del campione/popolazione.  

In questo capitolo verranno introdotti i seguenti test:
\begin{itemize}
    \item test per la bontà dell'adattamento, che servono per verificare se una popolazione segue una distribuzione prestabilità
    \item test per confrontare le distribuzioni di due popolazioni, quando queste non sono necessariamente delle normali
    \item test per verificare se sussiste indipendenza o incorrelazione tra due diversi caratteri di una popolazione
\end{itemize}
Anche per questi test valgono le considerazioni viste in merito agli errori di prima e seconda specie ed alle regioni
critiche e di accettazione nel capitolo precedente sulle ipotesi parametriche.

\section{Test per la bontà dell'adattamento}
I test relativi alla bontà dell'adattamento pongono a capire se una popolazione $X$ è distribuita seconda una funzione
di ripartizione $F$, ossia sono dei test la cui ipotesi nulla è $H_0:F_X(t) = F(t) \forall t \in \R$ con ipotesi
alternativa $H_1:F_X(t) \neq F(t) \mbox{ per almeno un t in \R}$.\newline
I due famosi test riguardanti la bontà dell'adattamento sono i seguenti:
\begin{itemize}
    \item \emph{test di Kolmogorov-Smirnov} si basa sulla popolazione empirica, introdotta nel seguito del paragrafo, e
        sulla statistica
        \[ D_n = \sup _{t \in \R} |F(t) - \bar{F}_{X, n}(t)| \]
        che specifica l’estremo superiore delle distanze in valore assoluto tra la funzione di ripartizione che vogliamo
        controllare come possibile per X e quella empirica, ottenuta tramite il campionedisponibile.\newline
        Si può dimostrare che quando vale H0 e $F$ è una funzione continua allora tale statistica è indipendente 
        dalla forma di $F$, ovvero ha la stessa distribuzione qualunque sia $F$ e pertanto è possibile fissare delle 
        regioni di accettazione e critiche al variare di $n$ e del livello di significatività.\newline
        La distribuzione della statistica campionaria $D_n$ al variare di $n$ è stata studiata dagli inventori,
        che hanno fornito apposite tabelle per determinare i quantili.

        Si può notare che è logico aspettarsi che  $D_n$ assume valori piccoli se l’ipotesi nulla è vera, ed assume 
        valori grandi quando $H0$ è falsa.\newline
        Infatti, per ogni ampiezza $\alpha$ del test la regione critica risulta essere
        \[ C = (d_{1 - \alpha}, 1] \]
        dove il quantile $d_{1 - \alpha}$ è quel valore per cui risulta 
        \[ P(D_n \leq d_{1 - \alpha}) = 1 - \alpha \]
        che può essere determinato sulle apposite tavole(si noti che $D_n$ non può sicuramente assumere valori maggiori di $1$)

        Si osservi che la massima distanza tra la funzione di ripartizione $F$ e la funzione di ripartizione empirica 
        viene raggiunta sempre in corrispondenza di uno dei salti della distribuzione empirica.\newline
        Questo fatto può essere utile nella determinazione di $D_n$, quando non è possibile fare uso di rappresentazionigrafiche.
        
        Osserviamo che il test di Kolmogorov-Smirnov non richiede particolari assunti sui dati o sulla distribuzione $F$
        per essere impiegato, se non quello di continuità della $F$ stessa.

        Dato un campione $(X_1, X_2, \dots, X_n)$ estratto da una popolazione, si definisce \emph{funzione di
        ripartizione empirica} della popolazione $X$, basata sul campione $(X_1, X_2, \dots, X_n)$, la funzione aleatoria
        \[ \bar{F}_{X, n}(t) = \frac{1}{n} \sum _{i = 1}^n U_{(-\infty, t]}(X_i) \forall t \in \R \]
        dove si definisce
        \[ U_{-\infty, t]}(X_i) = \begin{cases}
                                    1 \mbox{ se } X_i \in (-\infty, t] \\
                                    0 \mbox{ altrimenti}
                                  \end{cases} \]
        Si osservi che le funzioni di ripartizione empiriche sono sempre delle funzioni crescenti a gradino ed è facile
        rendersi conto poi che al crescere della numerosità $n$ del campione la funzione di ripartizione empirica 
        assomiglia sempre di più alla reale funzione di ripartizione della popolazione, fino a coincidere quando $n$
        corrisponde alla numerosità dell'intera popolazione.
    \item \emph{test del chi-quadro} può essere usato senza porre condizioni sulla funzione di ripartizione $F$. 
          Anche per poter descrivere questo test occorre introdurre alcune nozioni e notazioni e come al solito supporremo
          di poter estrarre un campione $(X_1, X_2, \dots, X_n)$ di numerosità $n$

