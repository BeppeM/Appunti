\chapter{Regressione Lineare}
In molte situazioni c'è una variabile $Y$, chiamata \emph{variabili dipendente} da un insieme di valori di input, chiamati \emph{indipendenti}, $X_1, X_2, \dots, X_r$.\newline
La più semplice relazione tra la variabile dipendente $Y$ e le variabili di input $X_1, X_2, \dots, X_r$ è la relazione lineare, in cui per alcune costanti $\beta_0, \dots, \beta_r$ si ha
\[ Y = \beta_0 + \beta_1x_1 + \dots + \beta_r x_r \]
Se esite una relazione tra $Y$ e $X_i$ allora dovrebbe essere possibile predirre esattamente la risposta per ogni insieme di valori ma in pratica comunque ogni previsione non è sempre
attendibile per cui la relazione più corretta per rappresentare una relazione lineare consiste nell'equazione
\[ Y = \beta_0 + \beta_1x_1 + \dots + \beta_rx_r + e \]
dove $e$ indica l'errore, assunto con media $0$ e varianza $\sigma^2$.\newline
Un altro modo di rappresentare l'equazione precedente è la seguente
\[ E[Y | x] = \beta_0 + \beta_1x_1 + \dots + \beta_rx_r \]
dove $x = (x_1, x_2, \dots, x_r)$ è l'insieme delle variabili indipendenti e $E[Y|x]$ è la risposta attesa per l'input $x$.\newline
L'

