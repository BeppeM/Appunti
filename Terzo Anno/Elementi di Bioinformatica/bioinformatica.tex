\documentclass[a4paper,12pt, oneside]{book}

% \usepackage{fullpage}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{verbatim}
\usepackage{rotating}
\usepackage[safe,extra]{tipa}
\usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{enumerate}
\usepackage{braket}
\usepackage{forest}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage[cache=false]{minted}

\usepackage{tikz}\usetikzlibrary{er}\tikzset{multi  attribute /.style={attribute ,double  distance =1.5pt}}\tikzset{derived  attribute /.style={attribute ,dashed}}\tikzset{total /.style={double  distance =1.5pt}}\tikzset{every  entity /.style={draw=orange , fill=orange!20}}\tikzset{every  attribute /.style={draw=MediumPurple1, fill=MediumPurple1!20}}\tikzset{every  relationship /.style={draw=Chartreuse2, fill=Chartreuse2!20}}\newcommand{\key}[1]{\underline{#1}}
\newcommand{\coloredbox}[1]{\fcolorbox{black}{#1}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}

% forest
\forestset{
  every leaf node/.style={
    if n children=0{#1}{}
  },
  every tree node/.style={
    if n children=0{}{#1}
  },
  edgelabel/.style n args={2}{
    edge label={node[midway,#1]{#2}}
  },
  nodevalue/.style n args={2}{
    label=#1:{{#2}}
  }
}
\forestset{
  suffix tree/.style={
    for tree={
      edge={->},
      every tree node={
        circle, draw, minimum size=1.5em, s sep=1cm,
        s sep+=1em,
        l sep+=1em
      },
      every leaf node={
        rectangle, draw, 
        minimum size=1.5em
      }
    }
  }
}
\forestset{
  highlight ancestors/.style={
    before typesetting nodes={
      for current and ancestors=highlight
    }
  },
  highlight/.style={
    draw=darkgray,
    edge+=red
  }
}
\forestset{
  terminal/.style={
    if n children=0{
      tier=terminal
    }{}
  }
}

\title{Elementi di Bioinformatica}
\author{UniShare\\\\Davide Cozzi\\\href{https://t.me/dlcgold}{@dlcgold}}
\date{}

\pgfplotsset{compat=1.13}
\begin{document}
\maketitle

\definecolor{shadecolor}{gray}{0.80}
\setlist{leftmargin = 2cm}
\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\tableofcontents
\renewcommand{\chaptermark}[1]{%
  \markboth{\chaptername
    \ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\chapter{Introduzione}
\textbf{Questi appunti sono presi a lezione. Per quanto sia stata
  fatta una revisione è altamente probabile (praticamente certo) che
  possano contenere errori, sia di stampa che di vero e proprio
  contenuto. Per eventuali proposte di correzione effettuare
  una pull request. Link: }
\url{https://github.com/dlcgold/Appunti}.\\
\textbf{Grazie mille e buono studio!}
\chapter{Algoritmi di Pattern Matching}
Un po' di notazione per le stringhe:
\begin{itemize}
  \item \textbf{simbolo:} $T[i]$
  \item \textbf{stringa:} $T[1]T[2]\ldotst[n]$
  \item \textbf{sottostringa:} $T[i:j]$
  \item \textbf{prefisso:} $T[:j] = T[1:j]$ (inclusi gli estremi)
  \item \textbf{suffisso:} $T[i:] = T[i:|T|]$ (inclusi gli estremi)
  \item \textbf{concatenazione:} $T_1\cdot T_2 = T_1T_2$
\end{itemize}
In bioinformatica si lavora soprattutto con le stringhe, implementando
algoritmi, per esempio, di pattern matching. Nel pattern maching si ha
un testo T come input e un pattern P (solitamente di cardinalità minore
all'input) da ricercare. Si cerca tutte le occorrenze di P in
T. L'algoritmo banale prevede due cicli innestati e ha complessità
$O(nm)$ con $n$ lunghezza di T e $m$ lunghezza di P. Il minimo di
complessità sarebbe $O(n+m)$ (è il \textbf{lower bound}). Si ragiona
anche sulla costante implicita della notazione O-Grande cercando di
capire quale sia effettivamente l'algoritmo migliore con la quantità
di dati che si deve usare. Bisogna quindi bilanciare pratica e teoria.
\newpage
Ecco l'algoritmo banale:
\begin{shaded}
  \begin{algorithmic}
    \For {$i\gets 1$ \textbf{to} $n$}
    \State $trovato \gets true$
    \For{ $j\gets 1$ \textbf{to} $m$}
    \If{ $T[1+j-1]\neq P[j]$}
    \State $trovato \gets false$
    \EndIf
    \EndFor
    \If{ $trovato$}
    \State $print(i)$
    \EndIf
    \EndFor
  \end{algorithmic} 
\end{shaded}
\section{Bit-Parallel}
È un algoritmo veloce in pratica ma poco performante a livello
teorico, ha complessità $O(nm)$.
Questo algoritmo è facilmente eseguibile dall'hardware del pc. \\
In generale si hanno \textbf{algoritmi numerici} che trattano i numeri
e gli \textbf{algoritmi simbolici} che manipolano testi.\\
Si hanno poi gli\textbf{ algoritmi semi-numerici} che trattano i
numeri secondo la loro rappresentazione binaria, manipolando quest'ultima
con \textit{or $\vee$, and $\wedge$, xor $\oplus$, left-shift $<<$ e
  right-shift $>>$}. Ricordiamo che il left shift sposta di $k$
posizioni a sinistra i bit, scartandone $k$ in testa e aggiumgendo
altrettanti zeri in coda (lo shift a destra sposta a destra, scarta in
coda e aggiunge zeri in testa). Queste sono operazioni bitwise e sono
mappate direttamente sull'hardware, rendendo tutto estremamente
efficiente.\\
\subsection{Algoritmo D\"om\"olki/Baeza-Yates}
Questo algoritmo viene anche chiamato \textbf{algoritmo shift-and} o
anche \textbf{bit parallel string matching}. Questo algoritmo punta a
ridurre il reale tempo di esecuzione delle istruzioni sulla cpu.\\
Si definisce in input una stringa $T$ di cardinalità $n$ e un pattern
$P$ di cardinalità $m$.\\
Si ha una semplificazione rispetto all'algoritmo Shift-And reale,
infatti i caratteri dell'alfabero vengono rappresentati con il loro
valore vero e proprio e non con una bit-mask.\\
Si costruisce una matrice $M$ \textit{ipotetica}, di dimensione
$n\times m$, con un indice $i$ per $P$ e uno $j$ per $T$  dove:
\[M(i,j)=1\,\,sse\,\,P[:i]=T[j-i+1:j],\,0\leq i\leq m,\,\,0\leq j\leq
  n\]
Quindi $M(i,j)=1$ sse i primi $i$ caratteri del pattern sono uguali
alla sottostring lunga $i$ in posizione $j-i+1$ del testo.\\
Questa matrice è veloce da costruire e si ha:
\[M(m,\cdot) = 1,\,\, M(0,\cdot)=1,\,\, M(\cdot, 0)=0\]
\[M(i,j)=1\,\,sse\,\,M(i=1, j=1)\,\,AND\,\, P[i]=T[j]\]
la prima riga saranno tutti 1 ($ M(0,\cdot)=1$) in quanto la stringa
vuota c'è sempre mentre la prima colonna saranno tutti 0 ($M(\cdot,
0)=0$) in quanto un testo vuoto non matcha mai con una stringa non
vuota.\\
Quindi la matrice avrà 1 solo se i primi caratteri del pattern $P[i]$ sono
uguali alla porzione di testo $=T[j-i+1:j]$. Ma in posizione $M(i-1,
j-1)$ mi accorgo che ho 1 se ho un match anche con un carattere in
meno di P e T. Qindi se $M(i-1,j-1)=0$ lo sarà anche $M(i,j)$. Se
invece $M(i-1,j-1)=1$ devo controllare solo il carattere $P[i]$ e
$T[j]$ e vedere se $ P[i]=T[j]$.
Ovvero, avendo $P=assi$ e $T=apassi$ si avrebbe (omettendo la prima
riga e la prima colonna in quanto banali):
\begin{center}
  \begin{tabular}{c c | c c c c c c}
    & j & 1 & 2 & 3 & 4 & 5 & 6 \\
    i & & a & p & a & s & s & i \\
    \hline
    1 & a & 1 & 0 & 1 & 0 & 0 & 0\\
    2 & s & 0 & 0 & 0 & 1 & 0 & 0 \\
    3 & s & 0 & 0 & 0 & 0 & 1 & 0 \\
    4 & i & 0 & 0 & 0 & 0 & 0 & 1
  \end{tabular}
\end{center}
Con un automa non deterministico che accetta una stringa terminante
con $P$ sarebbe:
\begin{center}
  \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
    \node[state,initial] (q_0)   {$-1$};
    \node[state] (q_1) [right=of q_0] {$0$};
    \node[state] (q_2) [right=of q_1] {$1$};
    \node[state] (q_3) [right=of q_2] {$2$};
    \node[state, accepting] (q_4) [right=of q_3] {$3$};
    \path[->]
    (q_0) edge  node {a} (q_1)
    edge [loop above] node {$\varepsilon$} ()
    (q_1) edge  node  {s} (q_2)
    (q_2) edge  node  {s} (q_3)
    (q_3) edge  node  {i} (q_4);
  \end{tikzpicture}
\end{center}
La matrice la costruisco con due cicli e controllo solo l'ultima riga.
Non si ha un guadagno a livello di complessità, dato che rimane
$O(nm)$, ma grazie all'architettura a 64 bit della cpu. Infatti con una word
della cpu posso memorizzare una colonna intera, in quanto vista come numero
binario. Ora lavoro in parallelo su più bit, con un algoritmo
\textbf{bit-parallel}, facendo ogni volta 64 confronti tra binari. In
questo modo crolla la costante moltiplicativa nell'O-grande.\\
Ma come passo da una colonna $C[j]$ a una $C[j-1]$? Con questi step:
\begin{itemize}
  \item la colonna $C[j]$ corrisponde al right shift della colonna $C[j-1]$
  \item aggiungo 1 in prima posizione per compensare lo shift
  \item faccio l'AND con $U[T[j]]$, che è un array binario lungo come
  il pattern dove ho un binario con 1 se è il carattere  di
  riferimento:
  \begin{center}
    P=abca\\
    U[a]=1001\\
    U[b]=0100\\
    U[c]=0010\\
  \end{center}
  \item ragiono sul word size $\omega$ in caso di pattern più grandi
  di 64bit.
\end{itemize}
ottengo:
\begin{center}
  C[j] = ((C[j-1])>>1)| (1 << ($\omega$-1)\&U[T[j]])
\end{center}
Conoscendo una colonna della matrice voglio calcolare la successiva. 
Quindi $M[i,j]=M[i-1,j-1]\,\,AND\,\,P[i]=T[j]$ (per esempio, 
$M[1,j]=TRUE\,\,AND\,\,(p[i]=T[j])$), cioè conta solo il
confronto dei caratteri. \\
Ogni 1 nell'ultima riga corrisponde ad un'occorrenza. \\ 
Questo algoritmo ha il vantaggio di non avere branch if/else, 
però si ha ul limite nella lunghezza del pattern (64 bit) pattern 
e l'uso di più word comporta il riporto sulla colonna
seguente, fattore che si complica all'aumentare della lunghezza del
pattern, soprattutto se arbitraria.
\section{Algoritmo Karp-Rabin}
Vediamo un altro algoritmo di pattern matching che sfrutta una
codifica binaria e che, pur non risultando sempre corretto, è estremamente
più veloce, viene infatti eseguito in tempo lineare.\\
Uso un alfabeto binario e devo fare il match di due stringhe con
ciascuna la sua codifica $H(S)=\sum_{i=1}^{|S|}2^{i-1}H(S[i])$.
Ad ogni carattere di una string si associa un numero nel range $[0,\,
2^{m-1}]$. Praticamente si usano due funzioni hash che trasformano una
stringa in un decimale rappresentate in binario (ogni numero intero è
facilmnete rappresentabile come somma di potenze di 2 e quindi in
binario). Viene quindi facile paragonare le due fingerprints.
Mi muovo sul testo $T$ mediante finestre di ampiezza $m$ pari a quella
del pattern e controllo il fingerprint di quella porzione con quella
del pattern. Inoltre il fingerprint di una finestra è facilmente
calcolabile da quello della precedente. Per farlo elimino il
contributo del carattere della finestra precedente e includo l'unico
aggiunto dalla finestra successiva, in quanto mi sposto di 1:
\[H(T [i + 1 : i + m]) = \frac{H(T [i : i + m - 1]) −T [i]}{2} +
  2^{m-1} T [i + m]\]
Essendo il primo carattere quello meno pesante viene rimosso ad ogni
spostamento sfruttando la divisione per due per lo shift
\\
La sottostringa è uguale al pattern solo se le fingerprint lo sono:
\[T [i : i + m - 1] = P \Leftrightarrow H(T [i : i + m - 1]) = H(P)\]
\textit{Per estendere la codifica binaria in $k$ caratteri avrò la finestra
  che si sposta di $k$ con la divisione per $k$ anziché per 2.}\\
Si ha il problema della lunghezza del pattern in quanto ho un
$2^{m-1}$ che fa esplodere l'algoritmo perché usa un numero di
bit grandissimo. Si ricorda che un'operazione ``costa 1'' solo se sono
piccoli i numeri in gioco, nel nostro caso il costo diventa
proporzionale al numero di bit coinvolti. La soluzione di Karp-Rabin è
di continuare con la logica di sopra ma solo con numeri piccoli,
cambiando la definizione di fingerprint prendendo il resto di quanto
sopra con un numero primo $p$:
\[H(T [i + 1 : i + m]) = \left(\frac{H(T [i : i + m - 1]) −T [i]}{2} +
    2^{m-1} T [i + m]\right)\mod p\]
ma in questo modo la fingerprint non è più iniettiva, con la possibilità che
più stringhe abbiano la stessa fingerprint e di conseguenza si avranno degli 
errori. Si ha che $  2^{m-1} T [i + m]$ viene calcolato iterativamente facendo
$\mod p$ ad ogni passo. Si può quindi avere una sottostringa di T con lo stesso
fingerprint del pattern che però non è uguale al pattern,
è un \textbf{falso positivo}. Non si possono tuttaavia avere falsi negativi,
quindi tutte le occorrenze sono trovate con la possibilità di trovare occorrenze
false in più:
\[H(T [i : i + m - 1]) \mod p = H(P) \mod p \Leftarrow T [i : i + m - 1] = P\]
Se il numero primo $p$ è scelto a caso minore di un certo $I$ so che
l'errore è minore di $O(\frac{nm}{I})$.\\
Vogliamo sfruttare però che si hanno solo falsi positivi e provare ad
eseguire l'algoritmo con due $p$ diverse, le vere occorrenze saranno
trovate da entrambe mentre i falsi positivi probabilmente no. Itero
quindi su $k$ numeri primi e il risultato sarà l'intersezione di tutte
le $k$ iterazioni dell'algoritmo, riducendo moltissimo le probabilità di
avere un risultato errato. Paghiamo quindi un incremento di un prodotto
$k$ delle operazioni (diventa $O(k(n+m))$) per ridurre esponenzialmente
le chances di errore.
\newpage
Proponiamo una versione semplificata dell'algoritmo (lunghezza del
testo = $n$ e del pattern = $m$):
\begin{shaded}
  \begin{algorithmic}
    \Function{$RabinKarp$}{$text,\, pattern$}
    \State $patternHash \gets hash(pattern[1:m])$
    \For{$i\gets 1\,\,\mathbf{to}\,\, n-m+1$}
    \State $textHash \gets hash(text[i : i + m - 1])$
    \If{$textHash = patternHash$}
    \If{$text[i : i + m - 1] = pattern[1:m]$}
    \State $return(i)$
    \EndIf
    \EndIf
    \EndFor
    \State $return(NotFound)$
    \EndFunction
  \end{algorithmic}
\end{shaded}
È quindi un algoritmo probabilistico in quanto i $p$ sono scelti a
caso. Ci sono due categorie di algoritmi probabilistici:
\begin{enumerate}
  \item \textbf{Monte Carlo}, come Karp-Rabin, veloci ma non sempre
  corretti
  \item \textbf{Las Vegas}, sempre corretti ma non sempre veloci, come
  per esempio il quicksort con pivot random (dove il caso migliore è un 
  pivot che è l'elemento mediano mentre il peggiore è che il pivot 
  sia un estremo, portando l'algoritmo ad essere quadratico).
\end{enumerate}
\textit{È possibile rendere Karp-Rabin un algoritmo della categoria Las Vegas
  controllando tutti i falsi positivi (anche se non è una procedura
  utlizzata).} \\
Per riconoscere i falsi positivi posso usare anche le sovrapposizioni,
che distano al amssimo $\frac{m}{2}$ caratteri. Eventauali eccessi
sono gestiti come semiperiodici di periodo $d$. Si ha quindi che
$d=l_2-l_1$, che è il periodo del pattern $P$. Sapendo che
$P=\alpha\beta^{k-1}$, con $\alpha$ suffisso di $\beta$. Ne segue che
ogni cattarete di $T$ è al massimo in due run dell'algoritmo.
\section{Trie}
Si usera la filosofia che prevede il preprocessamento del testo.
\begin{definizione}
  Il \textbf{trie} è una struttura dati ordinata, ovverp un albero,
  con archi etichettati, che, preso un insieme
  di parole, detto dizionario, controlla che
  quella sequenza sia nell'insieme di parole. \\
  La forma più semplice di indice è un array ausiliario
  che contiene il puntatore agli elementi in ordine lessicografico,
  tramite il quale è possibile effettuare una ricerca dicotomica
  ovvero una tecnica \textit{d\&i} che opera selezionando ad ogni
  passo due alternative.  
\end{definizione}
Si tratta di un
\textbf{problema di membership}. Voglio un tempo $O(n)$ con $n$
lunghezza della query. Quindi si preprocessa una volta sola il
dizionario in un albero e poi si procede con le query. Nella pratica
si ha che un percorso radice-foglia deve essere esattamente la query
richiesta. Si ha però un problema, essendo ogni ramo un elemento del
dizionario, non si possono avere parole diverse nel dizionario che
siano l'una prefissa dell'altra (per esempio se ho $abraabra$ non
posso avere anche $abra$ nel dizionario), perché non riuscirei ad
andare da radice a foglia. Introduco quindi il carattere $\textdollar$, il quale
non appartiene all'alfabeto che viene aggiunto alla fine di ogni
stringa, viene infatti detto \textbf{terminatore}, così $abra\textdollar$ non è
prefisso di $abraabra\textdollar$ etc$\ldots$, rimuovendo così ogni
ambiguità.\\ 
Un trie può essere rappresentato con:
\begin{itemize}
  \item un \textbf{array}, dove le celle vuote immagazzinano
  predecessori e successori, con tempo $O(P)$ e spazio $O(\Sigma P)$
  \item un \textbf{balanced binary search tree}, con tempo
  $O(P\log\Sigma)$ e spazio $O(T)$
  \item una \textbf{tabella hash}, che però ``rompe'' il ragioanmento
  sul predecessore e rende impossibile il sorting, con tempo $O(P)$ e
  spazio $O(T)$
  \item un \textbf{compressed trie}, ovvero mediante la compressione
  con un solo figlio in ogni nodo, con i relativi simboli sullo stesso
  arco 
\end{itemize}
\subsection{Suffix Tree}
Usiamo una struttura dati chiamata \textbf{suffix tree}, che è il
compressed trie di tutti i suffissi di $T\textdollar$, che quindi è un insieme più specifico di
stringhe (i suffissi). È una sorta di trie compatto, dove un suffisso
diventa l'etichetta di un arco. Le etichette degli archi uscenti, i
\mathit{figli}, da $X$ inziano con simboli diversi. I suffissi sono il
percorso radice-foglia.
\begin{definizione}
  Un \textbf{suffix tree} per una stringa $T$ di lunghezza $m$ è un albero di
  $m$ foglie indicizzate. Ogni nodo interno ha almeno due figli e ogni
  arco è etichettato con una stringa non vuota di $S$. Si ha che due
  archi che non appartengono allo stesso nodo non possono iniziare con
  lo stesso carattere. Per ogni foglia di indice $i$ si ha che la
  concatenazione delle etichette deglia rchi che portano dalla radice
  alla foglia corrisponde al suffisso della sottostringa che inizia
  alla posizione $i$.
\end{definizione}
\begin{center}
  \includegraphics[scale = 0.5]{img/suf.png}  
\end{center}
\textbf{Un suffisso implica un percorso radice foglia}\\
Una sottostringa è il \textit{prefisso di un suffisso}. Dato un
pattern voglio trovare tutti i suffissi che iniziano col
pattern. Quindi, dato che ogni nodo ha un solo figlio con un certo
prefisso, la procedura di pattern matching nel suffix-tree naviga
nell'albero sequendo il pattern nell'unico arco possibile con quel
pattern, posto che esista. Se non esiste il pattern cercato termina,
così come termina nel momento in cui lo trova raggiungendo un
$\textdollar$. Il compattamento da trie a suffix tree serve a
migliorare le performance di costruzione della struttura dati, non
quelle del pattern matching in sè. \\
Ci sono $|T + 1|$ foglie. Ogni label è una sottostringa $T [i : j]$,
è importante avere in memoria entrambi
gli indici $(i, j)$. Lo spazio di attraversamento dell’albero è $O(T )$,
quindi gli alberi possono essere
utilizzati per risolvere problemi di string matching in tempo
lineare.\\ 
Questo pattern matching ha $O(m)$ con $m$ lungheza del pattern da
ricercare ma poi ho tante foglie $k$ sotto il nodo a cui sono arrivato
quante sono le occorrenze del pattern nel testo, che quindi visito in
$O(k)$. Nel complesso ho la costruzione dell'albero in $O(n)$, $n$
lunghezza testo, matching in $O(m)$ e visita finale delle foglie in
$O(k)$, quindi nel complesso ho
\[O(n+m+k)\]
Se il pattern termina prima del passaggio ad un nodo successivo non mi
interessa in quanto i suffissi corrispondono (se ho $NA$ e mi fermo a
$N$ va bene lo stesso, in quanto i suffissi di $NA$ sono dello stesso
numero di quelli di $N$). Nelle foglie ho indicato l'indice dove
inizia ogni occorrenza.\\
Si ha che la \textbf{label(X)} è la concatenazione delle sottostringhe
che etichettano gli archi di un nodo.
\\
Definisco la \textbf{path-label(X)} di un nodo X è la concatenazione delle
stringhe fino a quel nodo. Definisco invece \textbf{string-depth(X)} di
un nodo X la lunghezza del path-label, che è calcolabile in $O(n)$. La
string-depth di X sarà la lunghezza dell'etichetta sommata alla
string-depth del padre.
\\
\textit{Molti algoritmi sfruttano molte visite per arricchire le
  informazioni dell'albero ai fini di rendere semplice la risoluzione
  di un problema.}\\
Non posso fare lo stesso ragionamento per la path-label perché,
concatendando quella del padre alla propria raggiungerei quasi
$O(n^2)$. Si usa quindi una tecnica basata su puntatori al testo e non
su stringhe, questo fa si che ogni arco sia etichettato da una coppia
di numeri (posizione di inizio e lunghezza), quindi raggiungo tempo
costante per etichettare ogni albero.
\begin{definizione}
  Si definisce \textbf{suffix tree generalizzato} un suffix tree
  derivante da un insieme di stringhe, che sono caratterizzati da un
  unico delimitatore
\end{definizione}
\subsection{Pattern Matching con Suffix Tree}
Dato un pattern $P$ di lunghezza $n$ e un testo $T$ di lunghezza $m$,
si vogliono trovare tutte le occorrenze di $P$ in $T$.\\
Usando i suffix tree risolvo il problema in tempo lineare.\\
Si ha che la rierca in un trie permette di capire se una stringa è
suffisso di un'altra, si estende questa logica alle sottostringhe,
carpendo anche indici e numero di occorrenze. \\
Cerchiamo di esprimere una sottostringa in termini di suffisso. Si ha
che: \textit{un suffisso è sicuramente una sottostringa, e ogni
  sottostringa è il prefisso di un suffisso}, quindi, dato che ogni suffisso è
in realtà un perocrso radice-foglia univoco, bisogna cercare i
percorsi iniziali. \\
Tutte le occorrenze del pattern $P$ sono le porzioni iniziali dei
suffissi che iniziano con $P$. Nel caso invece di ricerca di stringhe
a metà tra due archi l’etichetta non viene consumata interamente.\\
Quindi ci sono due alternative:
\begin{enumerate}
  \item la stringa non è presente in $T$
  \item ogni foglia del sotto-albero sottostante l’ultimo match è
  enumerata con un punto di partenza di $P$ in $T$, e ogni punto di
  partenza corrisponde a una foglia. 
\end{enumerate}
Si ha quindi $O(T)$ per processare la stringa, e i match si trovano in
$O(n+k)$, con $k$ numero di occorrenze. Una volta trovata una
ricorrenza si risale verso l'alto fino al primo suffisso che non
inizia con $P$. In aggiunta si ha un tempo $O(n)$ per creare l'albero.\\
Si ha che \textbf{ogni occorrenza di P in T è un prefisso di qualche
  suffisso in T}.\\
Vediamo il procedimento (\textbf{si ringraziano gli appunti Ilaria e
  Adi (link nel readme)} partendo dal primo carattere del pattern e dalla radice dell’albero:
\begin{enumerate}
  \item per il carattere corrente del pattern, se c’è una
  corrispondenza con la stringa dell’arco, allora 
  si prosegue verso il basso
  \item se il pattern non esiste nell’arco corrente, allora esso non
  esiste in assoluto
  \item si continua fino ad arrivare a una o più foglie
  dell’albero. Il valore delle foglie indicheranno 
  l’indice al quale si trovano le corrispondenze 
\end{enumerate}
\begin{center}
  \includegraphics[scale = 0.7]{img/su.png}
\end{center}
\section{Suffix Array}
Il grande problema del suffix-tree è lo spazio occupato, circa $20n$
byte, quindi per il genoma umano servono 60gb di memoria. Per
risolvere questo problema si usano i \textbf{suffix array (SA)}, che occupa
meno spazio, ed è \textit{l'array dei suffissi in ordine
  lessicografico}. Il suffix array non permette il pattern matching in
tempo lineare quindi viene usato il suffix array per costruire il
suffix tree. Tutto quello che si può fare sul suffix tree si può fare
sul suffix array con tempi diversi ma simili. Non memorizzo
esplicitamente i suffissi ma memorizzo le posizioni iniziali del
suffisso. Al suffix array si aggiunge l'array ausiliario \textit{LCP
  (longest common prefix)} con la lunghezza del prefisso comune $LPC[i]$
tra $SA[i]$ e $SA[i+1]$.
\begin{center}
  \includegraphics[scale = 0.7]{img/sa.png}
\end{center}
\begin{center}
  \includegraphics[scale = 0.7]{img/sa2.png}
\end{center}
il primo valore dell'\textit{LCP} rappresenta il terminatore, che non
ha sottostringhe comuni col suo elemento precedente, in quanto
inesistente.\\
\textit{Nel pattern matching, tutte le occorrenze di P corrispondono a
  una porzione dell’array Lcp, dove 
  gli elementi sono tutti minori o uguali della lunghezza del
  pattern. La scansione di Lcp permette di 
  trovare il pattern in tempo costante.}\\
Lo spazio diventa $4n$ bytes, quindi per il genoma 12gb.\\
Passiamo ora dal suffix tree al suffix array. Facciamo una visita depth-firts
(pre-order) del suffix array, assumento che a sinistra ci siano
etichette in ordine lessicografico minore (a sinistra ho suffissi con
una lettera dell'alfabeto iniziale ``precedente'').\\
Ho quindi che $LPC[i]$ è la string-depth di $LCA(i, i+1)$, ovvero
\textit{least common ancestor}, che quindi mi indica dove due percorsi
divergono e vedo quanto vale lì la string-depth.\\
Calcolare LPC costa un tempo quadratico. Ma possiamo fare un altra
cosa. Ogni arco viene visitato almeno due volte in qaunto una volta arrivati
ad una foglia si torna indietro. Mi salvo la string depth dell'LCA
nell'array ogni volta che ho un cambio di direzione nella visita dell'albero
lavorando quindi in tempo lineare. Nel disegno guardiamo due foglie
consective e contiamo i nodi che hanno in comune sopra esclusa la radice.\\
\textit{Dati SA e LCP passiamo ora a costruire l'albero}. Gli LCP nulli
partizionano il SA e corrispondono ai figli della radice del suffix
array aggiungendo 1 (tre zeri corrispondono 4 figli). Di questi 4
figli prendo i valori no nulli e saranno un cammino che prosegue
ripetutamente sul valore minimo.\\
\textit{Quindi se l'array è 013002 avrò 4 figli della radice, uno 0, uno 13,
  uno 00 (che conta come 0) e uno 2. Il 13 avrà una foglia a sinistra e
  poi scenderà di un nodo. Avendo solo un numero (3) avrà due
  foglie. Infine avrò due, che essendo un solo numero, avrà solo 2
  foglie.}\\

\begin{center}
  \includegraphics[scale = 0.5]{img/suf3.png}
\end{center}
\begin{center}
  \includegraphics[scale = 0.5]{img/suf4.png}
\end{center}
\begin{center}
  \includegraphics[scale = 0.5]{img/suf5.png}
\end{center}
\begin{center}
  \includegraphics[scale = 0.5]{img/suf6.png}
\end{center}

In pratica, viene calcolato il minimo $Lcp$ dell’albero o della porzione
considerata, e l’albero viene 
diviso in $k + 1$ regioni (dove k sono le occorrenze del numero). Alla
fine della ricorsione ci saranno 
k regioni di un singolo elemento, le quali corrispondono a foglie che
saranno collegate al rispettivo $SA[i]$.
Schematicamente si ha:
\begin{enumerate}
  \item si ricerca il minimo dell'\textit{LCP} e si contano le
  occorrenze
  \item si divide l'albero in $k+1$ regioni, con $k$ numero di
  occorrenze
  \item si considera gni regione specifica compresa tra ogni coppia di
  occorrenze, e ricerca del minimo
  \item si asseganno i sottoalberi (considerando che \$ è sempre a
  sinistra, in base all’ordine lessicografico)
  \item si itera fino all'esaurimento dell'array
  \item si fa la visita depth-first e si assegnano le etichette in
  ordine a ogni foglia
\end{enumerate}
Un \textbf{suffix tree generalizzato} rappresenta un insieme di
testi. Prendo x stringhe con terminatore, le concateno in un unico
testo e ne genero il suffix tree ma nelle foglie avremo le coppie
(numero stringa, posizione inizio suffisso) che potrebbero essere una
per ogni stringa e lo costruisco in tempo lineare alla lunghezza del
testo completo.
\begin{center}
  \includegraphics[scale = 0.5]{img/suf7.png}
\end{center}
Cerco ora la sottostringa comune più lunga tra due
stringhe sfruttando il suffix tree generalizzato. Una sottostringa
comune corrisponde ad un cammino tra una radice e un nodo X e tra i
discendenti di X devo avere almeno un discendente  per ogni
stringa. Ovvero ogni stringa rappresenta un suffisso di una delle due
stringhe (o entrambe). La più lunga sarà quindi quella che arriva al nodo con
string-depth massimo. Visito quindi due volte l'albero, alla prima
vedo se ha le foglie giuste e la seconda per cercare il max delle
string-depth. La prima visita la faccio dalle foglie verso la radice,
creo un array di booleani per il nodo X lungo quanto il numero di
stringhe i cui elementi valgono true sse esiste un disendente di X tale che è
suffisso della stringa S. Alla fine faccio l'and tra tutti i valori
dell'array, se è true significa che ho un discendente per ogni
stringa. Se X è nodo interno faccio l'or con l'array del nodo
discendente per determinare se X va bene. Se è foglia determino il
vettore leggendone l'etichetta. La seconda visita può essere in
qualsiasi direzione e dove ho nodi con la proprietà di essere comune
alle stringhe e trovo quello con la string-depth max, ottengo quindi
un tempo che è $O(k\cdot n)$.
\subsection{Pattern Matching su Suffix Array}
Il suffix array contiene le posizioni di inizio dei suffissi ordinati
lessicograficamente e viene utilizato insieme all'LCP. Il pattern
matching in questo caso non sarà ottimale ma si
otterrà $O(m\log n)$, con $m$ lunghezza del pattern. Si sfrutta la
ricerca dicotomica, che si basa sulla ricerca a partire dall'elemento
mediano di un array, per poi cercare su una sola delle metà
dell'array, dimezzando di volta in volta l'array. La ricerca
dicotomica funziona sul suffix array ma ogni iterazione consisite nel
confrontare il pattern con i primi $m$ caratteri del suffisso mediano,
fermandosi ovviamente al primo carattere discordante per capire poi su
quale metà del suffix array continuare a cercare, basandosi sui
singoli caratteri che non matchano. Quest'ultimo passaggio è possibile
grazie al fatto che il suffix array è ordinato lessicograficamente. Si
avranno 3 aggiustamenti all'algoritmo, detti \textbf{acceleranti}, che
abbasseranno la complessità del caso pessimo (col terzo si arriva a
$O(m+\log n)$). Con gli acceleranti si evitano confronti inutili.
\\Partiamo col \textbf{primo accelerante}.\\
Si parte dal presupposto che ci sia un ordine lessicografico posso
individuare regioni, dal $L$ a $R$, in cui tutti i possibili pattern
iniziano con gli stessi caratteri iniziali.
\begin{center}
  \includegraphics[scale = 0.5]{img/arr.png}
\end{center}
Formalmente si ha che tutti i fuffissi in $SA(L,R)$ iniziano con lo
stesso prefisso lungo $Lcp(SA[L], SA[R])$ e quindi potrò non
controllare i primi $Lcp(SA[L], SA[R])$ caratteri.
\\ Vediamo il \textbf{secondo accelerante}.\\
Si ha un elenco di casi che tengono in considerazione il pattern. Si
indica con $l$ l'Lcp tra il pattern e il primo suffisso
dell'intervallo e $r$ l'Lcp tra il primo e l'ultimo suffisso
dell'intervallo. Queste due variabili vengono aggiornate man mano e ho:
\begin{itemize}
  \item \textbf{caso 1:} se $l>r$ quindi il pattern somiglia più al
  primo suffisso che all'ultimo
  \begin{center}
    \includegraphics[scale = 0.5]{img/arr2.png}
  \end{center}
  Quindi dovrò poi controllare nella seconda metà del suffix array
  quindi scarterò in tempo costante tutta la prima metà solo se
  $Lcp(L,M)>l$, assegnando a $L$ il valore di $M$.\\
  % grafico
  Se fosse minore farei uguale scartando la seconda metà, assegnando a $R$
  il valor edi $M$ e ad $r$ l'$Lcp(M,L)$ (perché dovrò fermarmi a metà).\\
  % grafico
  Se fossero uguali dovrei confrontare $P[l+1:]$ e $M[l+1:]$ per
  decidere quale metà scartare, se avrò un carattere che non matcha
  tra il suffisso mediano e il pattern con l'ultimo carattere del
  suffisso mediano più piccolo allora cercherò nella seconda metà,
  altrimenti nella prima (sempre grazie all'ordine lessicografico).
  A differenza dei due sottocasi sopra questo
  impiega un tempo che dipende dal numero di caratteri uguali che
  trovo, più sono e più impiego tempo, anche se per assurdo più
  caratteri uguali trovo più mi sto stringendo alla regione che
  contiene il match del mio pattern. \textbf{Si ha consumo di tempo
    solo nel caso in cui mi stia avvicinando alla soluzione, se
    ``pago'' tanto in un'iterazione avrò la certezza di ``pagare''
    meno la volta seguente. Al massimo potro avere un costo pari a
    $2m$ perché ogni volta incremento o $l$ o $r$, che sommati sono
    per forza $\leq 2m$, quindi alla peggio ho complessitò $O(2m)$}.
  % grafico
  \textit{In ogni caso $l$ e $r$ potrebbero restare invariati o crescere in
    base al numero di caratteri che matchano e al massimo avranno valore
    pari alla lunghezza del pattern $m$}
  \item \textbf{caso 2:} se $l=r$. Se ho $Lcp(L,M)>l$ tengo la prima
  metà mentre se ho l'opposto   $Lcp(L,M)>l$ tengo seconda metà come
  nel primo caso.   Se invece $Lcp(L,M)=Lcp(M,R)=l$
  ragiono come nel terzo sottocaso del primo caso
  \item \textbf{caso 3:} se $l<r$ faccio lo speculare del primo caso 
\end{itemize}
\begin{esempio}
  ho i seguenti suffissi: \textit{\$, A\$, ANA\$, ANANA\$, BANANA\$,
    NA\$ e NANA\$} e pattern \textit{BA} ho quindi $L=0$, $R=6$ e $M=3$.\\
  $Lcp(L,M)=0$ e $Lcp(R,M)=0$.\\
  Confronto il primo carattere del pattern e il primo del mediano
  $B\neq A$ e $B>A$ quindi ho $L\gets 3$ mentre $R$ resta uguale (cosi
  come $r$ e $l$). Il nuovo mediano è tra 3 e 6, quindi, arrotonando,
  5.\\
  Confronto $B$ e $N$ e quindi cerco nella prima metà, che ha un solo
  elemento, quindi controllo i caratteri e scopro che, in questo caso,
  ho il match con \textit{BANANA}.
\end{esempio}
\begin{esempio}
  ho i seguenti suffissi: \textit{\$, A\$, ABRA\$, ABRACADABRA\$, ACADABRA\$,
    ADABRA\$, BRA\$, BRACADABRA\$, CADABRA\$, DABRA\$, RA\$ e
    RACADABRA\$} e pattern \textit{BRACA} ho quindi $L=0$, $R=11$ e $M=6$.\\
  $Lcp(L,M)=0$ e $Lcp(R,M)=0$.\\
  Confronto con \textit{BRA\$} e vedo che ci sono 3 caratteri uguali,
  quindi ora ho $L=6$, $R=11$, $M=9$, $l=3$ (i 3 caratteri uguali) e
  $r=0$.\\
  Confronto con \textit{DABRA\$} e vedo che il primo carattere non
  matcha e $D>B$ quindi cerco nella
  metà sopra. Quindi ora ho $L=6$, $R=9$, $M=8$, $l=3$ (i 3 caratteri uguali) e
  $r=0$.\\
  Confronto con \textit{CADABRA\$} e vedo che il primo carattere non
  matcha e $C>B$ quindi cerco nella  metà sopra.
  quindi ora ho $L=6$, $R=8$, $M=7$, $l=3$ (i 3 caratteri uguali) e
  $r=0$.\\
  Confronto con \textit{BRACADABRA\$} e vedo che ho $Lcp(L,M)=3$
  (prima era sempre stata nulla cosìccome $Lcp(R,M)$) e
  quindi sono nel terzo caso e confronto solo a partire dal terzo
  carattere escluso, trovando il match (di 5 caratteri).
\end{esempio}
Vediamo ora il \textbf{terzo accelerante (DA SISTEMARE)}.\\
Con questo accelerante studiamo il calcolo dell'Lcp. Alla prima
iterazione avrò $L=1$ e $R=n$, alla seconda avrò o $L=1$ e
$=\frac{n}{2}$ oppure $L=\frac{n}{2}$ e $R=n$ (a seconda della metà
scelta). Ad interazione $k$ avrò una forma del tipo $L=h\frac{n}{2^{k-1}}$ e
$R=(h+1)\frac{n}{2^{k-1}}$ con $h$ che rappresenta l'indice
dell'intervallo (al primo giro ho $h=\{1,2\}$, al secondo in
$h=\{1,2,3,4\}$ $\ldots$ $h=\{1,\ldots, 2^{k}\}$). Al termine
della mia ricerca dicotimica ho intervalli di cardinalità due. Sempre
alla fine della ricerca dicotomica si avranno al massimo $n$
intervalli. Entra quindi in gioco il terzo accelerante, che consiste
nel preprocessare tutti gli intervalli, quindi precalcola tutti i vari
Lcp. \textbf{Questo accelerante è di solo interesse teorico}. Voglio
calcolare abbastanza velocemente questi Lcp. Per ogni intervallo
calcolo l'Lcp ma in realtà per intervalli consecutivi (quindi nel caso
k-simo) ho già l'array Lcp e per ottenere gli altri aggrego i
risultati dell'interazione. Quindi Lcp tra un $L$ e un $R$ è il minimo
tra l'Lcp della prima metà, l'Lcp della seconda meta e l'Lcp tra $M$ e
$M+1$ che è nell'array Lcp (mentre quelli delle due metà sono già
calcolate).
% aggiungo formula
Ottengo quindi un $O(n)$.\\
Ho quindi ottenuto l'algoritmo per trovare un'occorrenza, che non è
per forza la prima. Voglio ora espandere per trovare tutte le
occorrenze in un tempo proprorzionale al numero stesso di
occorrenze. So che tutte le occorenze sono in un intorno
dell'occorrenza trovata. Cerco quindi i suffisi precedente e seguenti
fino a non trovare alcun match. Parto con l'occorrenza precedente e
uso l'array Lcp, in quanto controllo suffissi consecutivi, con l'array
vedo quanti caratteri iniziali condidono e se ne condividono almeno
$m$ (cardinalità del pattern) allora ho trovato un'altra
occorrenza. Dovendo solo leggere un valore ho tempo costante e quindi
nel complesso ho un $O(n+m+k)$ con $k$ numero di occorrenze.\\
In totale calcolo suffix array, array Lcp, preprocessamento e
scansione per trovare tutte le occorrenze, il tutto in $O(m\log n)$
\subsubsection{Pattern Matching}
Per calcolare la sottostringa comune più lunga ho i seguenti step:
\begin{enumerate}
  \item calcolo il suffix tree generalizzato della stringa $S$
  \item cerco il nodo $x$ tale che:
  \begin{enumerate}
    \item per ogni stringa $s_i \in S$ esiste un discendente di $x$
    corrispondente ad un suffisso di una delle stringhe $s_i$
    \item abbia la string-depth di massima fra tutti i nodi con la
    stessa caratteristica
  \end{enumerate}
\end{enumerate}
Dobbiamo ``rimappare'' questo ragionamento sul suffix array.\\
Una volta che i suffissi sono ordinati lessicograficamente essi
appaiono consecutivamente nel suffix tree (di $n$ nodi), come se
fossero una porzione contigua del suffix array. Però dal suffix array
posso estrarre $n\choose 2$ intervalli.\\
Parto quindi da un suffix array generalizzato (col suo array
LCP). Estraggo un generico intervallo $[i:j]$ del suffix array,
provando ad estrarli tutti, mi salvo il prefisso comune $p$ di tutti i
suffissi in quell'intervallo e se quell'intervallo contiene almeno un
suffisso per ogni stringa $s_i\in S$ e la lunghezza del prefisso
estratto $p$ è maggiore di $t$ (maggior lunghezza fino a quel momento)
allora mi salvo quella $p$ in $t$. Ma questa tecnica non può essere
lineare. Posso quindi diminuire il numero di intervalli da considerare
e devo calcolare velocemente la lunghezza del prefisso comune in $p$
(nel tree era facile calcolarlo in quanto si usava la string-depth).\\
Iniziamo col vedere come ridurre gli intervalli, cercando quelli
inutili. Fissato un punto finale $j$ guardo tutti gli intervalli che
finisono in $j$. Per ogni $j$ che controllo voglio considerare solo un
punto di inizio $i$. Non voglio intervalli con suffissi di una sola
della stringhe in ingresso. Presi un $l\leq i \leq j \leq q$ e
considero il $SA[i:j]$ e $SA[l:q]$ con il primo intervallo quindi
incluso nel secondo. Se $SA[l:q]$ non contiene nemmeno un suffisso per ogni
stringa in ingresso allora anche $SA[i:j]$ farà lo stesso. Sia quindi
$t_1$ un prefisso comune di $SA[i:j]$ e $t_2$ di $SA[l:q]$ allora la
lunghezza di $t_1$ è $\geq$ di quella di $t_2$. Cerco quindi l'unico
punto di inizio per ogni fine $j$ che ha senso considerare, ovvero
calcolo il massimo $i$ tale per cui $SA[i:j]$ contiene almeno un
suffisso per ogni stringa in ingresso, ho quindi un numero lineare di
intervalli da considerare. Per calcolare $i$ tengo traccia, per ogni
stringa in ingresso, di ogni ultima volta che ho un suffisso (creo
quindi un array $last$). Fissato $j$ quindi prendo l'intervallo minimo
che finisce in $j$ e sfrutto l'array $last$. Quindi $i$ sarà il
massimo valore tale che $SA[i:j]$ contiene almeno un suffisso per ogni
stringa in ingresso.
\textbf{capire}
% aggiungere disegno
Quindi considero un intervallo $[i:j]$ e la lunghezza del
prefisso comune a tutti i suffissi che è uguale al minimo dell'LCP tra
$i$ e $j-1$\\
Questo sistema comporta un tempo quadratico e bisogna arrivare ad un
tempo lineare.\\
Io ho un array $A$ di $n$ elementi e voglio calcolare il minimo in un
certo intervallo. È il \textbf{range minimun query}. Vogliamo
indicizzare in tempo $O(n\log n)$ e calcoleremo, grazie a questo
preprocessamento, il minimo in di un intervallo in tempo costante
$O(1)$. Il sistema consiste nel precalcolare i minimi per alcuni
intervalli che bastano per tutti. Il calcolo del minimo sarà poi un
confronto tra due intervalli. Se voglio il minimo in un intervallo e
ho precalcolato il minimo di due intervalli che insieme coprono
l'intero intervallo allora mi basta confrontare i due minimi dei
sottointervalli pre calcolati. \\
Preprocesso quindi piccoli intervalli ma che nel complesso mi coprano,
a coppie, tutto l'intervallo completo. Partiamo cercando un $z$ tale
che $2^z\leq j-i+1$ e prendo poi due intervalli, uno $[i:i+2^z-1]$ e
uno $[j-2^z+1:j]$. Ho quindi che $z=\left\lfloor{\log_2
    j-i+1}\right\rfloor$. Uso quindi la programmazione dinamica con
una matrice $B[x,y]=\min_{x\leq z<x+2^{y}}A[z]$:
\[
  \begin{cases}
    B[x,0]=A[x]\\
    B[x,y]=\min\{B[x,y-1], B[x+2^{y-1},y-1]\} \mbox { se } y\geq 0\\
  \end{cases}
\]
nel complesso ho quindi $n\log n$ elementi. Costruita questa matrice
vado a prendere il:
\[\min\{B[i,wr],B[j-2^w+1,w]\}\]
dove $w$ è la più grande potenza di 2 minore o uguale a $j-i+1$.
\textbf{finire}\\
\chapter{Allineamenti}
Ovviamente in bioinformatica uno dei problemi principali è il
confronto di sequenze biologiche per lo studio, per esempio, delle
proteine, alla ricerca di similitudini tra sequenze. Una sequenza
biologica si basa su un alfaberto formato dalle basi azotate:
\[\Sigma=\{A,C,T,G\}\]
In biologia molecolare si ha che \textbf{se due sequenze sono
  strutturalmente simili allora hanno anche una funzione simile}.
Si ha quindi la \textit{ricerca di omologie}, anche per lo studio
dell'evoluzione.
\begin{definizione}
  Si definsice \textbf{distanza di Hamming} il numero totale di
  caratteri differenti, a parità di posizione, tra due stringhe:
  \[d:S\times S\to\mathbb{R}^+\]
  con $S\times S$ insieme delle coppie di stringhe.\\
  La distanza di Hamming gode di:
  \begin{itemize}
    \item \textbf{riflessività}: $d(x,y)=0\Longleftrightarrow x=y,\,\,\forall
    x,y\in S$
    \item \textbf{simmetria}: $d(x,y)=d(y,x),\,\,\,\forall
    x,y\in S$
    \item \textbf{diseguaglianza triangolare}: $d(x,y)+d(y,z)\leq
    d(x,z),\,\,\,\forall x,y\in S$
  \end{itemize}
  \textbf{La distanza di Hamming è definita unicamente per stringhe di
    egual lunghezza}
\end{definizione}
\newpage

\section{Allineamento globale}
Si analizza l'allineamento di due intere stringhe, si ha quindi un
problema di \textbf{ottimo gloable}. Si analizzano due stringhe di
lunghezza arbitraria e non uguale tra di loro. Ci si riconduce all'uso
della distanza di Hamming inserendo spazi fino ad ottenere due
stringhe di egual lunghezza per poter lavorare colonna per colonna.
Si usano caratteri \textbf{indel} (\textit{in} inserisco e
\textit{del} tolgo). Si hanno delle proprietà:
\begin{itemize}
  \item non si possono avere colonne di soli indel
  \item le stringhe estese con gli indel devono essere lunghe uguali
\end{itemize}

\begin{esempio}
  Vediamo alcuni possibili allineamenti tra $s_1=ABRACADABRA$ e
  $s_2=BANANA$ usando gli indel:
  \begin{center}
    \includegraphics[scale = 0.5]{img/al.png}
  \end{center}
  \begin{center}
    \includegraphics[scale = 0.7]{img/indel.png}
  \end{center}
\end{esempio}
Un buon allineamento deve contenere pochi indel, molti caratteri
allineati e pochi non allineati.\\
\begin{definizione}
  Un problema di ottimizzazione richiede:
  \begin{itemize}
    \item un'istanza che è un insieme infinito di casi
    \item un insieme di soluzioni ammissibili verificabili in tempo
    polinomiale
    \item una funzione obiettivo, che è una soluzione ammissibile che
    mappa in $\mathbb{Q}$
    \item una soluzione che massimizza (massimizza il valore) o
    minimizza (minimizza il costo) la funzione obiettivo
  \end{itemize}
\end{definizione}
Si usa quindi la programmazione dinamica per stabilire uno score che
cresce quando i valori delle colonne coincidono e quindi si cerca di
massimizzare questo valore.\\
Cerchiamo quindi l'equazione di ricorrenza di questo problema di
massimizzazione.\\
Innazitutto abbiamo come variabile una matrice di score:
\[d:\left(\Sigma\cup\{-\}\right)\times\left\(\Sigma\cup
    \{-\}\right)\to \mathbb{Q}\]
Quindi date due stringhe $s_1$ e $s_2$ si ha che:
\[M[i,j]=\mbox{ottimo su} s_1[:i],\,s_2[:j]\]
con la seguente equazione di ricorrenza, detta di
\textbf{Needleman-Wunsch} (si indicano con ``-'' gli indel):
\[M[i,j]=\max
  \begin{cases}
    M[i-1,j-1]+d(s_1[i],s_2[j]) & \mbox{se non ho indel}\\
    M[i,j-1]+d(-,s_2[j]) & \mbox{se ho indel solo in }s_1\\
    M[i-1,j]+d(s_1[i],-) & \mbox{se ho indel solo in }s_2
  \end{cases}
\]
con le seguenti condizioni a contorno:
\[
  \begin{cases}
    M[0,0]=0\\
    M[i,0]=M[i-1,0]+d(-,s_2[j])\\
    M[0,j]=M[0,j-1]+d(-,s_2[j])
  \end{cases}
\]
Si ha quindi un doppio ciclo for e un tempo pari a $O(nm)$.
\subsection{Allineamento Locale}
Fino ad ora si è parlato di \textbf{allineamento globale}. Si
definisce invece \textbf{allineamento locale} l'individuazione di due
sottostringhe $t_1\subseteq s_1$ e $t_2\subseteq s_2$. Date altre due
sottostringhe $u_1,u_2$ delle 2 stringhe si ha che:
\[M[t_1,t_2]\geq M[u_1,u_2]\]
Un algoritmo banale avrebbe tempi assurdi: $O(n^3m^3)$.\\
Questo problema si può risolvere velocemente con il metodo
\textbf{Smith-Waterman}.\\
Con questo metodo l'allineamento viene ricercato tra tutte le
sottostribghe terminanti nelle posizioni $i$ e $j$, ovvero
$t_1=s_1[h,i]$ e $t_2=s_2[k,j]$, con $h$ e $k$ incogniti e con massimo
valore di $M[t_1,t_2]$. Si richiede inoltre che l'allineamento
ottimo abbia un'ultima colonna senza indel. Grazie a questa clausola
si ha che $s_1[h,i-1]$ e $s_2[h,j-1]$ è l'allineamento ottimale delle
sottostrighe terminanti un carattere prima di $i$ e $j$. Supponiamo
che queste ultime due sottostringhe inizino da $a$ e $b$ e che:
\[M(s_1[a,i-1],s_2[b,j-1])>M(s_1[h,i-1],s_2[k,j-1])\]
Aggiungiamo ora anche la colonna $i$ e quella $j$. Se si hanno
caratteri uguali nell'ultima colonna si ha che:
\[M(s_1[a,i],s_2[b,j])>M(s_1[h,i],s_2[k,j])\]
Ma questo è un assurdo essendo $M(s_1[h,i],s_2[k,j]$ massimo per
ipotesi.\\
Questo ragionamento si estende anche ai casi in cui ci sia un'indel a
termine di una delle due stringhe. Si introduce anche un nuovo caso,
aggiuntivo rispetto a quelli di Needleman-Wunsch: il caso in cui
nessuna sottostringa ha un allineamento positivo. In questo caso se
precedentemente non ci sono match si setta la casella a 0 e si evitano
i valori negativi. Una conseguenza diretta è che:
\[M[0,0]=M[i,0]=M[0,j]=0\]
Dopo aver costruito la matrice cerco il valore massimo e le coordinate
$x,y$ indicano le posizioni finali delle sottostringhe mentre le
coordinate del primo 0 indicano le posizioni iniziali.\\
Definiamo quindi l'equazione di ricorrenza:
\[M[i,j]=\max
  \begin{cases}
    M[i-1,j-1]+d(s_1[i],s_2[j]) & \mbox{se non ho indel}\\
    M[i,j-1]+d(-,s_2[j]) & \mbox{se ho indel solo in }s_1\\
    M[i-1,j]+d(s_1[i],-) & \mbox{se ho indel solo in }s_2\\
    0 & \mbox{altrimenti}
  \end{cases}
\]
Si è quindi raggiunto un $O(nm)$.
\newpage
Graficamente si ha:
\begin{center}
  \includegraphics[scale = 0.5]{img/sw.png}
\end{center}
\begin{definizione}
  La \textbf{distanza di Edit} è la trasformazione di una stringa in
  un'altra tramite l'inserimento, la cacncellazione o la modifica di
  un carattere. Inoltre si ha che la distanza di edit è un caso
  particolare dell'allineamento globale tra due sequenze.\\
  \textbf{L'allineamento globale racchiude tutte le possibili
    casistiche delle distenze di Edit}.\\
  Si ha la seguente equazione di ricorrenza:
  \[M[i,j]=\min
    \begin{cases}
      M[i-1,j-1] & \mbox{se } s_1[i]=s_2[j]\\
      M[i-1,j] & \mbox{se viene cancellato un carattere}\\
      M[i,j-1] & \mbox{se viene aggiunto un carattere}\\
      M[i-1,j-1]+1 & \mbox{se viene modificato un carattere}
    \end{cases}
  \]
\end{definizione}
Sulla stringa si procede da sinistra a destra.\\
Quindi il costo di ogni mismatch è pari a 1. \\
La distanza di Edit ha tempo $O(nm)$
\textbf{rivedere completamente}
\begin{shaded}
  Con LCS abbiamo un problema di massimizzazione dove mettiamo nella
  matrice di score 1 in corrispondenza di caratteri uguali e 0 negli
  altri casi. Con la distanza di Edit abbiamo 0 in caso di uguali
  caratteri e 1 nelle altre posizioni. Le due matrici di score sono
  quindi complementari. 
\end{shaded}
Facciamo un altro ragionamento. Penso ad un allineamento di minimo
costo dove ``taglio'' un prefisso e un suffisso delle due stringhe,
ovviamente un prefisso o un suffisso di una delle due stringhe.
Quindi, dato che parliamo di minimizzazione, quanto taglio ha
costo 0. Bisogna trovare un modo per imporre costo 0 per i caratteri
scartati ragionando sull'equazione di ricorrenza \textbf{senza toccare la
  matrice di score}. Basta modificare le condizioni a contorno,
mettendo a 0 la prima riga e la prima colonna:
\[
  \begin{cases}
    M[0,0]=0\\
    M[i,0]=0\\
    M[0,j]=0
  \end{cases}
\]
\textbf{Abbiamo quindi una nuova variante dell'allineamento globale
  dove minimizziamo il costo e non massimiziamo la validità, scartando
  un prefisso di una delle due stringhe}.\\
\textit{Questo proedimento è comodo se si hanno stringhe di lunghezza
  molto diversa, trovando un allineamento buono in modo che dove si
  allineano si abbiano pochi indel}.\\
Abbiamo visto come scartare un prefisso, ora ragioniamo sul suffisso.
Cerco quindi il minimo nell'ultima riga e nell'ultima colonna e
decido di conseguenza quale suffisso scartare e i quale stringa (se è
nell'ultima riga scarto da $s_2$, se nella colonna è in $s_1$).\\
Un altra variante prevede una lunghezza $k$ del prefisso o suffisso
da scartare. Per il suffisso ragiono solo le ultime $k$ posizioni
nell'ultima riga e dell'ultima colonna, mentre per il prefisso metto 0
solo nelle prime k posizioni della prima riga e della prima colonna.\\
In ogni caso per ricorstruire risalgo fino ad un 0.\\
Analizziamo un ulteriore approccio. Questa volta si è interessati solo
al costo di un allineamento globale, cercando però di ottimizzare
sullo spazio. Per calcolare $M[i,j]$, essendo una LCS, mi bastano solo
il valore a sinistra $M[i-1,j]$, quello sopra $M[i,j-1]$ e quello
precedente sulla diagonale $M[i-1,j-1]$. Quindi mi basta tenere la
riga o la colonna in cui sono e la riga o la colonna precedente,
scegliendo la riga o la  colonna in base alla loro lunghezza, che
corrisponde quindi alla lunghezza della stringa (se $s_2$ è più corta
scelgo la riga altrimenti, se $s_1$ è più corta, la colonna). Non
serve quindi tenere tutta la matrice e posso quindi risparmiare
spazio.\\
Ora restringiamo ancora l'allineamento, cercando di risolvere
l'allineamento su stringhe molto simili, in maniera ancora più
ottimizzata sfruttando la similarità. In altre parole si ha la
distanza di Edit con un numero di operazioni che è al massimo un
valore $k$, tutto in $O(kn)$. Voglio ridurre il tempo di calcolo dela
distanza di Edit sapendo che vale al massimo $k$. Avrò quindi al
massimo $kn$ caselle.\\
Sapendo che ho la distanza di Edit tra le due stringhe  vale al
massimo $k$ e che le due stringhe sono lunghe $n$ e $m$ so che:
\[|n-m|\leq k\]
Si ha quindi una matrice di programmazione dinamica che è quasi
quadrata. Si ha che le parti che non costano corrispondono ad uno
spostamento sulla diagonale, quindi al diminuire della distanza di
Edit mi avvicino alla diagonale principale. Posso quindi non
considerare le caselle lontane della diagonale. È l'\textbf{allineamento
  mediante distanza di Edit con banda}, ovvero seleziono un intorno
della diagonale principale (appunto una banda) da analizzare, perché
fuori dalla stessa avrò solo soluzioni non ottimali.\\
\textbf{disegno}\\
Si arricchiscono ancora le condizioni a contorno della ricorrenza
della distanza di Edit, ponendo a $-\infty$ le caselle in cui
$|i-j|\geq k$ (nel codice si avrebbe solo questo controllo).
Si potrebbe però non conoscere $k$ o si potrebbe avere un $k$
sbagliato, portando ad una soluzione non ottima (magari perché questa
soluzione vorebbe ``uscire'' nell'area fuori dalla banda). Si procede
quindi raddoppiando $k$ e mi fermo quando la distanza di Edit $edit$ è,
per un certo $h$ che alla fine varrà $h^*=\log_2 edit$:
\[2^{h-1}<edit<2^h\]
Ho quindi tempo complessivo pari a:
\[\sum_{i=0}^{h^*}O(2^in)=O(n)\cdot\sum_{i=0}^{h^*}O(2^i)=O(n)\cdot
  (2^{h^*+1}-1)\sim O(n)\cdot (2^{h^*}-1)\]
\[\Downarrow\]
\[O(n)\cdot (2\cdot edit -1)=O(n\cdot edit)\]
Abbiamo quindi un buon tempo, rispetto a $O(nm)$.\\
\textit{Si raddoppia per essere comodi coi conti con il logaritmo.}\\
\textbf{Ovviamente questa strategia funziona bene sse le stringhe sono
  abbastanza simili, con $k<<n$ e $k<<m$.}\\
Vediamo un'altra variante di allineamento. Ogni volta che si
introducono indel si ha un forte ``significato'' biologico. Quando si
aggiunge un indel si sposta il frame di lettura e si shiftano di uno i
codoni. 
\begin{definizione}
  Si definisce \textbf{gap} una sequenza contigua di indel
\end{definizione}
Si ha che tra inserire un indel e un gap si ha poca differenza. Si ha
quindi che ogni gap è associato ad un costo legato alla sua lunghezza
$P(l)$. Per l'allineamento separo quindi in match e mismatch tra
caratteri e in costi in presenza di gap.\\
Si vuole ottenere che allungare di uno un gap deve avere costo
costante (allungare un gap da 1 a 2 deve costare come da 11 a 12).\\
L'ultima colonna dell'allineamento ottimo quindi varia rispetto a
quella standard dell'allineamento globale.\\
Quindi $M[i,j]$ è l'allineameno ottimo si $s_i[1:]$ e $s_2[:j]$.\\
Cerco quindi la mia equazione di ricorrenza dove si devono considerare
i gap. Ma non posso scrivere un'unica equazione di ricorrenza che
consideri tutte le possibili lunghezze dei gap. Cerco quindi l'ultima
componente per la matrice della programmazione dinamica, che sono
l'ultima colonna in assenza di gap o la posizione dell'ultimo
gap. Ottengo quindi:
\[M[i,j]=\max
  \begin{cases}
    M[i-1,j-1]+d(s_i[i],s_2[j]) & \mbox{se non ho gap} \\
    \max_{l>0} M[i,j-l]+P(l) & \mbox{se ho gap in }$s_1$ \\
    \max_{l>0} M[i-l,j]+P(l) & \mbox{se ho gap in }$s_2$
  \end{cases}
\]
Ponendo come condizione a contorno che:
\[M[0,0]=0\]
\[M[i,0]=P(i)\]
\[M[0,j]=P(j)\]
Per i tempi ho, con un'analisi iniziale grezza, sapendo che riempire
una casella costa $i+j+1$:
\[\sum_{i=0}^n\sum_{j=0}^m(i+j)\leq nm(n+m)=n^2m+nm^2\]
E non si può scendere sotto questo caso pessimo. Ipotizziamo di
dividere in 4 la matrice, del quadrante in basso a destra ho $i\geq
\frac{n}{2}$ e $j\geq\frac{m}{2}$ e quindi ho
\[t\geq \frac{nm}{4}\left(\frac{n}{2}+\frac{m}{2}\right)\geq
  \frac{1}{8}nm(n+m)\]
tutto questo solo nell'ultimo quadrante. È quindi impossibile scendere
sotto quel caso pessimo. \textbf{Questo, $O(nm(n+m))$, è il ``prezzo
  da pagare'' per avere costo di gap generico, dove un gap lungo $n$
  costa meno di $n$ gaps lunghi 1}.\\
Bisogna quindi cambiare approccio per migliorare l'algoritmo,
riducendo i valori da calcolare, in quanto ora leggo tutti i valori
sopra, tutti quelli a sinistra e quello precedente sulla diagonale. Si
passa al \textbf{gap affine o lineare}, dove si ha una penalità $P_o$
di apertura del gap più $l$ volte quella di estenione del gap $P_e$:
\[costo=P_o+l\cdot P_e,\,\,\,P_e,P_o<0\]
Quindi la creazione costa un valore che tiene conto di $P_o$ e $P_e$
mentre l'aumento del gap varia solo su $P_e$. Vista questa situazione
cerco le casistiche dell'ultima componente da guardare. Alla
situazione di prima si aggiungono dei casi. Si cerca l'allineamento
ottimo di $s_1[i-1]$ e $s_2[j:]$ sotto la condizione che l'ultima
colonna di tale allineamento abbia un indel per $s_2$ (e si cerca
anche il problema simmetrico a questo per $s_1$). Si hanno quindi
ottimi vincolati.
\newpage
Si ha quindi: 
\[M[i,j]=\mbox{ ottimo su } s_1[:i],\,s_2[:j]\]
\[E_1[i,j]=\mbox{ ottimo su } s_1[:i],\,s_2[:j] \mbox{ con estensione
    di gap finale in }s_1\]
\[E_2[i,j]=\mbox{ ottimo su } s_1[:i],\,s_2[:j] \mbox{ con estensione
    di gap finale in }s_1\]
\[E_1[i,j]=\mbox{ ottimo su } s_1[:i],\,s_2[:j] \mbox{ con apertura
    di gap alla fine di }s_1\]
\[N_2[i,j]=\mbox{ ottimo su } s_1[:i],\,s_2[:j] \mbox{  con apertura
    di gap alla fine di }s_2\]
quindi:
\[M[i,j]=\max
  \begin{cases}
    M[i-1,j-1]+d(s_1[i],s_2[j])\\
    E_1[i,j],E_2[i,j]\\
    N_1[i,j],N_2[i,j]
  \end{cases}
\]
\[E_1[i,j]=\max
  \begin{cases}
    E_1[i,j-1]+P_e\\
    N_1[i,j-1]+P_e
  \end{cases}
\]
\[E_2[i,j]=\max
  \begin{cases}
    E_2[i-1,j]+P_e\\
    N_2[i-1,j]+P_e
  \end{cases}
\]
\[N_1[i,j]=M[i,j-1]+P_o+P_e\]
\[N_2[i,j]=M[i-1,j]+P_o+P_e\]
Si ha quindi tempo costante per ogni casella e ottengo tempo pari a
$O(nm)$ anche se ho una costante superiore (5) che moltiplica $nm$ da
considerare i termini di spazio.\\
\textit{Si può volendo compattare in 3 matrici.}\\
\textbf{Ho programmazione dinamica in termini di soluzioni di
  sottoistanze ottime di altri problemi collegati al primo}\\
\subsection{Allineamento Multiplo}
Estendiamo il problema dell'allineamento avendo ora $k$ stringhe in
input, oltre alla matrice di score. Si nota che l'allineamento globale
è solo un sottocaso dei quello multiplo, ovvero è un allineamto
multiplo con $k=2$.\\
Per risolvere il problema cerco gli allineamenti per ogni coppia
possibile di stringhe. Chiamiamo $t_i$ le stringhe estese. Si hanno
alcune proprietà necessarie:
\begin{itemize}
  \item $|t_1|=\cdots=|t_n|$, ovvero tutte le stringhe estese hanno
  lunghezza uguale
  \item non ho colonne di soli indel
\end{itemize}
Si ha quindi quindi l'equazione per il caso passo della ricorrenza:
\[\sum_{i=1}^h\sym_{j=1}^h\sum_{n=0}^kd(t_i[j], t_n[j])\]
Il tempo è $O(2^kn^k)$ e si ha uno spazio di $O(n^k)$. Avendo $k$
arbitrario quindi si ha a che fare con un problema NP-completo.
\subsection{Matrici di Score}
Si hanno matrici di score ``famose'' per valutare gli allineamenti. I
loro dati possono essere letti come le probabilità che avvenga una
transizione. Vediamo le più importanti:
\begin{itemize}
  \item \textbf{PAM (\textit{Point/Percent Accepted Mutation})}. Viene
  usata per capire la ``distanza tra due sequenze'' usando la
  quantità di mutazioni. In realtà si hanno più matrici PAM pvvero
  $PAMk$, dove $k$ indica l'accuratezza. Queste matrici si calcolano
  numericamente prendnedo varie seguenze distanti $kPAM$ e
  allineandole. SI ha che 1PAM è il numero di mutazioni:
  $\frac{1}{k}|s_i|$. Sia $f(i)$ la frequenza di ogni carattere e sia
  $f(i,j)$ la frequenza di ogni coppia di caratteri. \textit{Ogni
    casella, quindi, indica la probabilità che l’aminoacido di quella
    riga sia stato sostituito con l’aminoacido di quella colonna
    attraverso il tempo.}\\
  Si ha quindi la formula:
  \[PAM_k(i,j)=\log\frac{f(i,j)}{f(i)f(j)}\]
  e il risultato è detto \textbf{Logs Odd Ratio}
  \item \textbf{BLOSUM (\textit{BLOck SUbstitution Matrix})} si usano,
  a differenza delle PAM, per allineare sequenze lontane, infatti
  divide i blocchi di regioni conservate da quelle mutate,
  scegliendole manualmente. Anche le BLOSUM sfruttano il Logs Odd
  Ratio. Si hanno anche qui varie $BLOSUMk$ e la
  più usata è la \textit{BLOSUM62}
\end{itemize}
Tra i tool più usati nominiamo invece \textbf{BLAST (\textit{Basic
    Local Alignment Search Tool})} che confronta sequenze, allineate
con \textit{Smith Watermn} in un database sfruttando i \textit{seed},
ovvero pattern matching con una sottostringa di lunghezza
3. L'algoritmo usato su basa su \textbf{HSP   (\textit{High-scoring
    Segment Pair})} che è l'estensione di un seed che massimizza i
valori dell'allineamento. Si tengono solo gli HSP significativi mentre
gli altri vengono fusi tra loro.\\
I risultati di BLAST vengono analizzati statisticamente con le
\textbf{statistiche Karlin-Altschul}, che assegnano un punteggio ad
ogni query. Queste statistiche tengono conto di simboli indipendenti
ed equiprobabili, sequenze infinitamente lunghe e allineamenti senza gap.
Gli HSP con massimo score sono distribuiti casualmente, e si può
considerare che la probabilità che questi punteggi superino una
determinata soglia segua una distribuzione di Poisson.\\
Vedno $E$ apri al numero di allineamenti senza gap, $k$ costante
dipendente dallo score, $n$ numero di caratteri nel database, $m$
lunghezza della stringa di cui si fa la query e $\lambda S$ il
punteggio normalizzato (secondo Poisson) si ha:
\[E=kmne^{-\lamda S}\]
\chapter{Assemblaggio di Genomi}
Si parla di assemblaggio da 0. Nella prima metà degli anni 90 c'è
stato il \textbf{progetto genoma umano}, che si proponeva di
ricostruire la sequenza del genoma umano, lungo circa 3 miliardi di
basi. Non si ha una tecnologia per leggere tutto il genoma ma ho
delle tecnologie che ne leggono pezzetti, detti \textbf{read}, che con
la tecnologia di ``prima generazione`` erano lunghi
tra i 50 e i 10000 basi (\textit{base pairs}, nucleotidi), con un
errore del $\sim 1\%$ ma un costo economico elevatissimo.
Le read ``di seconda generazione'' sono  molto
più precise (errore sotto all'uno per mille) ma molto corte, circa 200
basi. La ``terza generazione'' offre lunghezze estese, circa 10000
basi, ad un costo d'errore elevatissimo ($10\%-15\%$). Estratte le
read non so da quale porzione del genoma arrivino. Si ha quindi una
grande collezione di read che devono essere assemblate per ottenere il
genoma, a partire da un numero di read vincolate solo dai costi
economici (superano comunque in lunghezza il genoma, magari avendo
un totale di 60 miliardi di basi). Il rapporto tra read archiviate e
lunghezza del genoma originale è detto \textbf{copertura}.\\
Si sfruttano le sovrapposizioni delle read per ottenere il genoma
(come se fosse un puzzle).\\
Spesso le read sono prodotte a coppie e vengono dette \textit{mate
  pairs}, derivate dalla natura a doppio filamento del del DNA e la
\textbf{legge di complementarietà di Dawson-Crick}, infatti una base
ha la suia complementare nell'altro filamento. In modo deterministico
quindi si ottengono due sequenze che si sa essere ``vicine''. \\
Formalmente le sovrapposizioni si riducono a cercare un suffisso di
una read che sia il prefisso di un'alra read, è la tecnica
dell'\textbf{overlap}. La differenza di una singola base (o comunque
una differenza minima) può essere data dal tasso d'errore del
macchinario che produce la sequenza oppure si ha che è una conseguenza
della natura diploide dell'uomo, ovvero che si hanno due copie
genomiche, una ereditata da ciascun genitore. Magari si ha a che fare
quindi con un confronto tra queste due copie che genera queste minime
differenze.
\subsection{Grafi di Overleap}
Iniziamo a vedere come ricostruire il genoma. Il primo approccio
potrebbe essere quello di trovare una stringa, detta
\textit{superstringa} $T$ tale che ogni
$s_i \in S\,\,S=\{s_1,\ldots s_n\}$ sia sottostringa di
$T$. Praticamente si ha che $T$ è il genoma (con $|T|$ che rappresenta
la nostra funzione obiettivo) e le $s_i$ le reads. Si
hanno però diverse ripetizioni e grandi regioni di basi ripetute.
\begin{esempio}
  Si ha:\\
  ng\_lon \_long\_ a\_long long\_l ong\_ti ong\_lo long\_t g\_long
  \textbf{g\_time ng\_tim}\\
  ng\_time ng\_lon \textbf{\_long\_} a\_long long\_l ong\_ti ong\_lo long\_t
  \textbf{g\_long}\\  
  ng\_time g\_long\_ ng\_lon a\_long long\_l \textbf{ong\_ti} ong\_lo
  \textbf{long\_t}\\ 
  ng\_time long\_ti g\_long\_ \textbf{ng\_lon} a\_long long\_l
  \textbf{ong\_lo}\\
  \textbf{ng\_time} ong\_lon \textbf{long\_ti} g\_long\_ a\_long long\_l
  \textbf{ong\_lon} long\_time g\_long\_ a\_long \textbf{long\_l}
  long\_lon \textbf{long\_time g\_long\_} a\_long\\
  \textbf{long\_lon g\_long\_time} a\_long\\
  \textbf{long\_long\_time a\_long}\\
  a\_long\_long\_time
\end{esempio}
Si procede con un algoritmo greedy fondendo le due stringhe com massimo
overleap iterativamente finchè non si ottiene una sola stringa. Questa
è una soluzione \textbf{NP-hard} oltre al fotto che si hanno regioni
estremamente simili che verranno considerate solo una volta.
\\
Si passa quindi ad un ragionamento che sfrutta i grafi. In un
\textbf{grafo di overleap} i nodi sono rappresentati da dalle read. I
nodi sono collegati sse l'overleap è abbastanza lungo e l'arco tra un
vertice $A$ e uno $B$ è etichettato con la sottosequenza di $A$ che
precede la sovrapposizione. Si tiene conto anche dell'ordine di
lettura delle reads.
Vediamo un esempio avendo:
\[read=\{ACGTGTG,\,\, CGTGTGC,\,\, GTGCCA,\,\, CCACG\}\]
\begin{center}
  \includegraphics[scale = 0.6]{img/gra.png}
\end{center}
\newpage
semplificando:

\begin{center}
  \begin{tikzpicture}[shorten >=1pt,node distance=6cm,on grid,auto]
    \node[state] (q_0)   {$CGTGTGC$};
    \node[state] (q_1) [right=of q_0] {$CCACG$};
    \node[state] (q_2) [below=of q_0] {$ACGTGTG$};
    \node[state] (q_3) [right=of q_2] {$GTGCCA$};
    \path[->]
    (q_0) edge  node {$CGTGTG$} (q_1)
    (q_0) edge  node  {$CGT$} (q_3)
    (q_2) edge  node  {$A$} (q_0)
    (q_2) edge  node  {$ACGT$} (q_3)
    (q_3) edge  node [right]  {$GTG$} (q_1);
  \end{tikzpicture}
\end{center}
Dal grafo precedente si passa allo \textbf{string graph}
rimuovendo gli archi transitivi, ottenendo:
\begin{center}
  \begin{tikzpicture}[shorten >=1pt,node distance=4cm,on grid,auto]
    \node[state] (q_0)   {$CGTGTGC$};
    \node[state] (q_1) [right=of q_0] {$CCACG$};
    \node[state] (q_2) [below=of q_0] {$ACGTGTG$};
    \node[state] (q_3) [right=of q_2] {$GTGCCA$};
    \path[->]
    (q_0) edge  node  {$CGT$} (q_3)
    (q_2) edge  node  {$A$} (q_0)
    (q_3) edge  node [right]  {$GTG$} (q_1);
  \end{tikzpicture}
\end{center}
\subsubsection{TSP e Superstringa}
Per inizare ad approcciarci ai grafi vediamo il probolema del
\textbf{Traveling Salesman Problem, \textit{TSP}}.\\
Dato un grafo orientato $G=(V,A)$ con archi pesati seocndo $w:A\to
\mathbb{Q}^+$. Si cerca una permutazione $\Pi=\{\pi_1,\ldots,\pi_n\}$
di $V$ di costo (peso totale degli archi che vengono attraversati)
minimo che visiti tutti i nodi e torni al punto di partenza. Si ha la
seguente funzione obiettivo:
\[\min z=w(\pi_n,\pi_1)+\sum_{i=1}^nw(\pi_i, \pi_{i+1})\]
Si dimostra che questo è un problema \textbf{NP-Complete} risolvibile
grazie all'hardware.\\
\textbf{Una soluzione è un percorso che tocca ogni città esattamente
  una volta e torna al punto di partenza}\\
Torniamo ora al problema originario della superstringa. Ipotizziamo
che una read non sia altro che un città del TSP. Però si ha che
l'asseblaggio non è il ``ciclo'' tra le città e la lunghezza della
stringa non è il costo del percorso TSP. Le stringhe in ingresso
vengono mappate nel grafo di TSP, diventando nodi. Gli archi devono
avere peso minimo, quindi diventano la parte della stringa che non è
sovrapposta. 
e si ha quindi:
\[|T|=\sum_{i=1}^n|s_i|-\sum_{i=1}^{n-1}|overleap(s_i,s_{i+1})|\]
\textit{Per permettere di tornare all'inizio della stringa dalla fine
  si usa il carattere \$ a fine stringa che viene collegata al suo
  inizio.}\\
\textbf{Si mappano quindi tutti i possibili cammini e si individua il
  percorso ottimo.}
\subsection{OLC}
L'\textbf{OLC (\textit{Overlap, Layout, Consensus})} è una tecnica che
si usa per ridurre il grafo di overleap e si basa su 3 passaggi:
\begin{enumerate}
  \item \textbf{overleap,} dove si calcolano le sovrapposizioni e si
  costruisce il grafo di overleap. Si usa il suffix array per ottenere
  un \textit{metodo esatto} oppure la programmazione dinamica (che può
  comportare errori). Usando il primo metodo si calcola il suffix tree
  generalizzato di tutte le read  \textit{L’albero viene visitato
    carattere per carattere, usando la read come pattern, e cercando
    tutti i nodi da cui esce un simbolo di terminazione che non sia
    l’ultimo.} \\
  Usando invece la programmazione dinamica si definisce un problema
  che prende in input due stringhe $s$ e $t$ si cercano i suffissi $x$
  di $s$ e $y$ di $t$ tali che si abbia:
  \[\max\{|x|+|y|-2edit(x,y)\}\]
  dove il $2\cdot$ è stato introdotto in modo arbitrario per dare un
  maggior peso alle sovrapposizioni. Si confrontano quindi un prefisso
  e un suffisso e si ha che $m[i,j]$ è l'ottimo del prefisso lungo $j$
  di $t$ e del suffisso lungo $i$ di $s$
  \item \textbf{layout,} dove si fondono i cammini per ottenre i
  cosiddetti \textbf{contigs}, ovvero sottoseuqneze continue. In
  questa fase vengono rimossi i \textit{branching nodes}, ovvero le
  ripetizioni e con una sola visita determinare per ogni prefisso se
  esso sia anche un suffisso
  \item \textbf{consensus,} dove si calcolano i nucleotidi
\end{enumerate}
\subsection{SBH}
Si ha che un \textit{k-mero} è una sottostringa di lunghezza $k$.
\textbf{SBH (\textit{Sequencing By Hybridation})} è una vecchia
tecnologia che analizza gli \textit{oligonucleotidi} (ovvero sequenze
nucleotidiche di un numero di basi da 6 a 10). Per ogni
\textit{k-mero}, che viene chiamato \textbf{chip}, con $k\approx 8$, si
conosce se appare nel genoma. Il processo è chiamato DNA chip perché
la logica di fondo è la stessa che viene usata per i chip, ognuno di
essi può tenere migliaia di oligonucleotidi. Trascurando tutta la
parte tecnica e passando a quella algoritmica si ha che SBH si
differenzia dal grafo di overleap in quanto in \textit{k-meri} hanno
tutti la stessa lunghezza (al conbtrario delle reads) e quindi si
hanno sovrapposizioni di lunghezza $k-1$
\subsection{Grafi di De Bruijin}
Si ha che ogni k-mero viene diviso in \textit{(k-1)-meri}. \\
In un \textbf{grafo di De Bruijin} ogni vertice corrisponde ad un
\textit{(k-1)-mero} e un arco corrisponde ad un \textit{k-mero}. Due
archi sono collegati sse ci sono sovrapposizioni. Avendo stringhe
tutte di lunghezza $k$ che sono presenti in almeno una read si
costruisce facilmente il grafo. I \textit{(k-1)-meri} identici vengono
eliminati ottenendo unicamente nodi distinti. \\
Si definisce \textbf{cammino Euleriano} un cammino che attraversa ogni
nodo una e una sola volta.\\
Si definisce \textbf{ciclo} un cammino che torna al nodo di partenza.\\
Si definisce \textbf{ciclo sempice} un cammino che torna al nodo di partenza
passando una e una sola volta per ogni nodo.\\
Trovando il cammino Euleriano dei grafi di De Bruijin si ottiene il
genoma.
\begin{esempio}
  Si abbiano le seguenti reads:
  \[reads=\{ACGTGTG,\,\,CGTGTGC,\,\,GTGCCA,\,\,CCACG\}\]
  Scegliamo $k=4$ e otteniamo i \textit{4-meri}:
  \[ACGT,\,\,CACG,\,\,CCAC,\,\,CGTG,\,\,GCCA,\,\,GTGC,\,\,GTGT,
    \,\,TGCC,\,\,TGTG\]
  e i seguenti 3.meri unici:
  \[ACG,\,\,CAC,\,\,CCA,\,\,CGT,\,\,GCC,\,\,GTG,\,\,TGC,\,\,TGT\]
  e quindi:
  \begin{center}
    \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
      % uncomment if require: \path (0,248); %set diagram left start at 0, and has height of 248

      % Shape: Circle [id:dp5662414203096264] 
      \draw   (81,53) .. controls (81,39.19) and (92.19,28) .. (106,28) .. controls (119.81,28) and (131,39.19) .. (131,53) .. controls (131,66.81) and (119.81,78) .. (106,78) .. controls (92.19,78) and (81,66.81) .. (81,53) -- cycle ;
      % Shape: Circle [id:dp13985089677551477] 
      \draw   (81,129) .. controls (81,115.19) and (92.19,104) .. (106,104) .. controls (119.81,104) and (131,115.19) .. (131,129) .. controls (131,142.81) and (119.81,154) .. (106,154) .. controls (92.19,154) and (81,142.81) .. (81,129) -- cycle ;
      % Shape: Circle [id:dp3574816316500795] 
      \draw   (15,93) .. controls (15,79.19) and (26.19,68) .. (40,68) .. controls (53.81,68) and (65,79.19) .. (65,93) .. controls (65,106.81) and (53.81,118) .. (40,118) .. controls (26.19,118) and (15,106.81) .. (15,93) -- cycle ;
      % Shape: Circle [id:dp6367614117013465] 
      \draw   (162,97) .. controls (162,83.19) and (173.19,72) .. (187,72) .. controls (200.81,72) and (212,83.19) .. (212,97) .. controls (212,110.81) and (200.81,122) .. (187,122) .. controls (173.19,122) and (162,110.81) .. (162,97) -- cycle ;
      % Shape: Circle [id:dp8286860889666687] 
      \draw   (236,135) .. controls (236,121.19) and (247.19,110) .. (261,110) .. controls (274.81,110) and (286,121.19) .. (286,135) .. controls (286,148.81) and (274.81,160) .. (261,160) .. controls (247.19,160) and (236,148.81) .. (236,135) -- cycle ;
      % Shape: Circle [id:dp04605576669499256] 
      \draw   (236,45) .. controls (236,31.19) and (247.19,20) .. (261,20) .. controls (274.81,20) and (286,31.19) .. (286,45) .. controls (286,58.81) and (274.81,70) .. (261,70) .. controls (247.19,70) and (236,58.81) .. (236,45) -- cycle ;
      % Shape: Circle [id:dp9155218336299027] 
      \draw   (303,43) .. controls (303,29.19) and (314.19,18) .. (328,18) .. controls (341.81,18) and (353,29.19) .. (353,43) .. controls (353,56.81) and (341.81,68) .. (328,68) .. controls (314.19,68) and (303,56.81) .. (303,43) -- cycle ;
      % Shape: Circle [id:dp766738912437283] 
      \draw   (303,123) .. controls (303,109.19) and (314.19,98) .. (328,98) .. controls (341.81,98) and (353,109.19) .. (353,123) .. controls (353,136.81) and (341.81,148) .. (328,148) .. controls (314.19,148) and (303,136.81) .. (303,123) -- cycle ;
      % Straight Lines [id:da9646625679785032] 
      \draw    (61.75,79.5) -- (82.55,66.56) ;
      \draw [shift={(84.25,65.5)}, rotate = 508.11] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

      % Straight Lines [id:da8830983152823235] 
      \draw    (106,78) -- (106,102) ;
      \draw [shift={(106,104)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

      % Curve Lines [id:da7743744950450349] 
      \draw    (131,129) .. controls (138.6,137.33) and (149.07,134.14) .. (172.31,119.42) ;
      \draw [shift={(173.75,118.5)}, rotate = 507.41] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

      % Curve Lines [id:da701157029011428] 
      \draw    (162,97) .. controls (156.37,91.61) and (140.89,82.86) .. (122.86,107.45) ;
      \draw [shift={(121.75,109)}, rotate = 304.92] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

      % Curve Lines [id:da9534415928320956] 
      \draw    (328,148) .. controls (184.94,235.12) and (18.86,163.46) .. (39.32,119.33) ;
      \draw [shift={(40,118)}, rotate = 479.11] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

      % Curve Lines [id:da9799576589537213] 
      \draw    (106,154) .. controls (123.33,180.73) and (219.06,188.84) .. (259.78,159.89) ;
      \draw [shift={(261,159)}, rotate = 503.13] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

      % Straight Lines [id:da2213126680718589] 
      \draw    (261,109) -- (261,72) ;
      \draw [shift={(261,70)}, rotate = 450] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

      % Straight Lines [id:da05405134063421202] 
      \draw    (328,96) -- (328,68) ;

      \draw [shift={(328,98)}, rotate = 270] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
      % Straight Lines [id:da7142573004014958] 
      \draw    (286,45) -- (301.01,43.23) ;
      \draw [shift={(303,43)}, rotate = 533.29] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;


      % Text Node
      \draw (40,94) node  [align=left] {ACG};
      % Text Node
      \draw (106,55) node  [align=left] {CGT};
      % Text Node
      \draw (107,129) node  [align=left] {GTG};
      % Text Node
      \draw (187,99) node  [align=left] {TGT};
      % Text Node
      \draw (261,45) node  [align=left] {GCC};
      % Text Node
      \draw (328,43) node  [align=left] {CCA};
      % Text Node
      \draw (261,137) node  [align=left] {TGC};
      % Text Node
      \draw (329,125) node  [align=left] {CAC};
    \end{tikzpicture}
  \end{center}
\end{esempio}
tornando sul discorso dei cammini diamo qualche definizione:
\begin{definizione}
  \begin{itemize}
    \item \textbf{grafo Semi-Euleriano}: dato un grafo orientato
    $G=(V,A)$ esso è definitito semi-Euleriano se esistono due
    vertici $s$ e $t$ tali che:
    \[N_G^- (s) = N_G^+(s) + 1, N_G^-(t) = N_G^+(v) - 1\]
    mentre per ogni altro vertice $w$ si ha che:
    \[N_G^-(w) = N_G^+(w)\]
    Si ottiene che ogni nodo avrà lo stesso numero di archi entranti
    e uscenti, tranne due nodi, uno che ne avrà uno uscente in più e
    l'altro uno entrante meno
    \item \textbf{grafo Euleriano:} dato un grafo orientato
    $G=(V,A)$ esso è definitito Euleriano se:
    \[N_G^-(w) = N_G^+(w)\]
    ovvero ogni vertice ha lo stesso numero di archi entranti e
    uscenti.
    \begin{teorema}
      Dato un grafo orientato $G=(V,A)$ e sia $C$ un suo cilo. Sia
      $G_1$ il grafo senza gli archi di $C$. $G_1$ è Euleriano
    \end{teorema}
    \begin{teorema}
      Un grafo connesso $G=(V,A)$ ha un cammino Euleriano sse $G$ è
      Semi-Euleriano, inoltre $G$ ha un ciclo Euleriano sse $G$ è Euleriano
    \end{teorema}
    \begin{teorema}
      Dato un grafo Semi-Euleriano $G=(V,A)$ e sia $P$ un cammino
      tra $s$ e $t$. Sia $G_1$ il grafo ottenuto togliendo a $G$ gli
      arhci di $P$. Si ha che $G_1$ è Euleriano
    \end{teorema}
    \item \textbf{ciclo Euleriano:} è un assemblaggio in un grafo
    orientato e connesso che attraversa ogni \textbf{arco} esattamente una
    volta. Si possono avere cicli Euleriani semplici
    \item \textbf{ciclo Hamiltoniano} è un cammino che attraversa
    ogni \textbf{vertice} esattamente una volta. Non si possono
    avere cicli Hamiltoniani semplici
  \end{itemize}
\end{definizione}
% \subsection{Reverse and Complement}

\chapter{Filogenesi}
Non esiste un unico modello di evoluzione sensato. \\
\textbf{Il processo di sviluppo nell’ambiente (di qualsiasi tipo) è
  definito pressione selettiva: le mutazioni 
  dominanti sono quelle che hanno garantito un vantaggio sul numero di
  figli sani. Riconoscere i 
  cambiamenti è utile per ricostruire la storia evolutiva.}\\
Ci sono diverse tipologie di mutazione, tra cui traslazioni di DNA e
replicazioni di genoma.
\begin{definizione}
  Si definisce \textbf{carattere} in due maniere:
  \begin{enumerate}
    \item un \textbf{carattere genetico} è la presenza o meno di
    una specifica mutazione nel DNA
    \item un \textbf{carattere fenotipico} è un carattere visibile
    esteriormente, quindi \textit{morfologico}
  \end{enumerate}
\end{definizione}
Per semplicità si assume il \textbf{modello binario}, ovvero:
\textit{ogni carattere è stato acquisito esattamente una volta in tutta la
  storia, e una volta acquisito non viene mai perso (avere quel
  carattere risulta vantaggioso).}
\section{Filogenesi Basata su Caratteri}
\textit{Il problema della filogenesi è definito con una matrice le cui
  righe rappresentano gli individui, e le 
  colonne rappresentano i caratteri. Un 1 indica che quella determinata
  specie ha un determinato carattere, e viceversa}\\
Cerchiamo un albero che rappresenti tale matrice. Questo algoritmo si
costruisce in tempo lineare usando il \textit{radix sort} delle
colonne decrementando il numero di 1 e inserendo le specie 
una alla volta.\\
Si crea quindi un albero $T$ che possiede $n$ foglie per ogni specie,
$m$ archi per ogni carattere. Per ogni oggetto $p$, i caratteri che
etichettano gli archi nel percorso dalla radice alla foglia 
hanno come stato 1. Per evitare archi non etichettati da un carattere
si usano solo foglie della matrice
$M$, che hanno la stessa etichetta dei genitori (interni), senza
corrispondenza $1 : 1$. La radice non ha nessuno degli $m$
caratteri. \\
Se si può effettivamente costruire un albero di questo tipo si parla
di \textbf{filogenesi perfetta}.\\
Un certo carattere si trasmette tra nodo padre e nodo figlio tramite
un arco, quindi solo i figli possiedono quel carattere. \\
Definiamo $l(c)$ l’insieme di specie che hanno il carattere $c$:
\[l(c)=\{s:M[s,c]=1\}\]
Si ha il seguente lemma:
\begin{lemma}
  Sia $M$ una matrice con filogenesi perfetta $T$ con due caratteri
  $c_1$ e $c_2$. Si ha che vale uno e uno solo tra:
  \begin{itemize}
    \item $l(c_1)\cap l(c_2)=\emptyset$
    \item $l(c_1)\subseteq l(c_2)$
    \item $l(c_1)\supseteq l(c_2)$
  \end{itemize}
\end{lemma}
L’algoritmo è semplice, e impiega un tempo computazionale di $O(nm)$,
considerando che i confronti sono effettuati in tempo costante. Esso
si basa sull’ordinamento di $M$.
\\
Vediamo quindi come costruire l'albero in tempo $O(nm)$. Questo
algoritmo si basa sull'ordinamento della matrice $M$ e su confronti in
tempo costante. Si ordinano le colonne in modo che quelle che ne
includono altre siano messe prima e per fare questo le colonne vengono
interpretate come interi binari.
\\
\textit{La matrice M ha un albero filogenetico se e solo se per ogni
  coppia di colonne i, j, gli insiemi delle caratteristiche
  corrispondenti sono disgiunti o uno un sottoinsieme dell’altro.} \\
Ogni specie quindi viene collegata subito sotto l'ultimo carattere
che possiede e se un carattere ne include un altro viene collocato a
sinistra (praticamente si fa il \textbf{radix sort}). Ricapitolando si
ha:
\begin{itemize}
  \item ogni colonna viene ordinata con un radix sort decrescente
  \item per ogni riga della matrice ottenuta si costruisce la stringa
  dei caratteri della riga in ordine crescente
  \item si costruisce l'albero per le stringhe ottenute inserendole
  iterativamente
  \item se verifica la filogenesi perfetta dell'albero
\end{itemize}
\subsubsection{Backmutation}
È il caso in cui una mutazione viene persa, ovvero quando si ha un
cambiamento dello stato di un arco da 1 a 0. In una filogenesi
perfetta non si hanno backmutation. Si hanno quindi diversi casi:
\begin{itemize}
  \item \textbf{filogenesi persistente}, in cui ogni carattere viene
  acquisito una sola volta e perso al più una volta nell’albero
  \item \textbf{parsimonia di Dollo}, dove si acquisisce un carattere
  una e una sola volta ma senza limite di numero di perdite dello
  stesso
  \item \textbf{Camin-Sokal}, dov gni carattere può essere acquisito
  più volte e non viene mai perso 
\end{itemize}
In questi casi non si garantisce l'unicità dell'albero ma, negli
ultimi due casi si garantisce l'esistenza di almeno un albero. 
\subsection{Tumori}
Tante mutazioni in un ridotto lasso temporale comportano un tumore.
Un tumore contiene cellule sane e cancerose, inoltre è un miscuglio di
\textit{subclones}, ovvero sotto-popolazioni omogenee diverse. Quando
si preleva una campione da analizzare se ne preleva uno con diversi
subclones al suo interno. Inoltre i subclones appaiono in numerosità
diversa. Tutte queste situazioni complicano lo studio dei tumori.\\
Studiamo quindi l'evoluzione tumorale. Innanzitutto si misura la
frequenza di ogni mutazione in un campione (che ha un numero di
cellule stimato). Si ha quindi una matrice di frequenze, sulle colonne
si ha il tipo di frequenza e sulle righe i campioni. Ovviamente nelle
celle si avranno le frequenze. Ovviamente è un ragionamento
\textbf{approssimato}, che si perfeziona all'aumentare delle reads,
che vengono allineate al genoma.\\
Si crea poi un albero in cui i nodi vengono collegati se si hanno
mutazioni comuni. L'algoritmo per calcolare questo albero è
estremamente complesso (\textit{NP-hard}) e non verrà trattato. 
\section{Approcci Basati su Parsimonia}
La funzione obiettivo è il cercare di minimizzare le differenze
cercando la soluzione più semplice e \textbf{\textit{parsimoniosa}}.
Si applica il \textbf{rasoio di Occam}.\\
Si introduce quindi una forma di costo e, per un albero, si ha che
\textit{per ogni arco si ha costo pari al numero di caratteri che
  cambiano stato}.\\
% aggiungere disegni
Ci sono molti alberi che spiegano la stessa matrice e devo scegliere
quello che costa meno.
\subsection{Grande Parsimonia}
Si cerca la filogenesi (si cerca quindi un albero sconosciuto)
che minimizza il numero di cambiamenti di stato. Si ha quindi una
matrice non binaria in quanto ogni carattere può avere più stati. Si
parla di un problema NP-complete che però può essere approssimato al
problema della \textbf{piccola parsimonia} (che ha in input la matrice
e la topologia dell'albero). \textit{Si vuole determinare
  quale specie etichetta ogni foglia, e quali passaggi di stato sono
  rappresentati da ogni arco.}
\subsection{Piccola parsimonia}
Si ha una matrice triangolare $M$ di $n$ specie e $|C|$ caratteri che
associa ad ogni coppia uno stato. Si ha poi un albero $T$ le cui
foglie sono le specie di $M$ di cui si assume la topologia. Infine si
ha un costo $w_c,\,\,\forall c\in C$ fra coppie 
di stati. Le tre sopra elencate sono le nostre istanze. \\
Le soluzioni ammissibili sono quelle etchettature $\lambda_c$,
$\forall c\in C$, che assegnano ad ogni nodo uno degli stati
possibili per $C$.\\
Vediamo ora la funzione obiettivo, con $E(T)$ insieme dei lati di $T$:
\[\min\{\sum_{c\in C}\sum_{(x,y)\in
    E(T)}w_c(\lambda_c(x),\lambda_c(y))\}\]
si hanno $k^n$ (???) stati possibili.\\

\begin{esempio}
  \textbf{Esempio preso dagli appunti di Ila e Adi}\\
  \begin{center}
    \begin{tabular}{l | *{6}{c}}
      \# arti & 0 & 1 & 2 & 4 & 6 & 8 \\
      \hline
      0 & 0 & 1000 & 2000 & 4000 & 6000 & 8000 \\
      1 & ~ & 0 & 60 & 150 & 190 & 232 \\
      2 & ~ & ~ & 0 & 30 & 70 & 120 \\
      4 & ~ & ~ & ~ & 0 & 25 & 80\\
      6 & ~ & ~ & ~ & ~ & 0 & 10\\
      8 & ~ & ~ & ~ & ~ & ~ & 0
    \end{tabular}
  \end{center}
  Una matrice di questo tipo è parte dell’istanza:
  \begin{center}
    \begin{tabular}{l | c c}
      M & \# arti & Gene X \\
      \hline
      Topo & 4 & 0 \\
      Uomo & 4 & 0 \\
      Ragno & 8 & 1 \\
      Mosca & 6 & 0 \\
      Antilope & 4 & 0
    \end{tabular}
  \end{center}
  Nella piccola parsimonia bisogna capire qual è l’etichettatura dei
  nodi interni (grigio, rosso e 
  verde) in modo che la somma totale dei costi su ogni arco sia minimizzata.
\end{esempio}
Spostiamo ora il ragionamento sull'uso di un solo carattere, che viene
gestito separatamente, in quanto ogni carattere è indipendente.
\subsubsection{Algoritmo di Sankoff}
È un algoritmo riolutivo del problema della piccola
parsimonia. Vogliamo determinare l'etichettatura ottimale. Sfruttiamo
la programmazione dinamico visitando l'albero dalle foglie alla radice
e usando un sottoinsieme di sottoistanze ordinate. Si etichetta
dinamicamente ogni nodo con l'ultima componente della soluzione dei
nodi precedenti. L’ordinamento nell’albero è parziale, ma è
sufficiente esaminare le sottoistanze formate dai sottoalberi. Per
ogni nodo $x$ si definsice $sol(x,y)$ quel valore nella matrice che è
la soluzione ottimale del sottoalbero $T$ che ha radice $x$, con
etichetta di $x$ uguale $y$:
\[sol(x,y)=\min_{z_1,\ldots,z_n}=\{\sum_{i=1}^hsol(f_i,z_1)+d(z_i,y)\}\]
con con $f_i$ indichiamo i figli del nodo $x$ e con $z_i$ tutte le
etichette di questi figli. Si ha quindi:\\
$M[x,z]$ è la soluzione ottimale del sottoalbero di $T$ che ha
radice $x$, sotto la condizione che $x$ abbia etichetta $z$. Nel dettaglio
\begin{itemize}
  \item $M[x,z]=0$ se $x$ è una foglia con etichetta conosciuta e
  presente nella matrice $z$
  \item $M[x,z]=+\infty$ se $x$ è una foglia con etichetta diversa da
  $z$ e quindi lo imposto ad un valore ``sentinella''
  \item  $M[x,z]=\sum_{f\in f(x))}\min_s\{M[r,s]\}$ con $f(x)$
  l’insieme dei figli in $x$ di $T$, se $x$ è un nodo interno.
\end{itemize}
La soluzione ottimale è quindi:
\[\min_s\{M[r,s]\]
con $r$ radice di $T$.\\
Per ogni figlio dell’albero viene calcolata la funzione min (tutte le
possibilità), e la soluzione del padre viene ricavata da quella dei
figli. Ogni cambiamento ha un costo: se i figli sono già etichettati
nel modo desiderato allora esso sarà 0. Il costo dell’arco $(x, f_1
)$ è $0$ se l’etichetta è uguale a $y$, 1 altrimenti. Quindi $z$
rappresenta ogni etichetta diversa da $y$.
\subsubsection{Algoritmo di Fitch}
Questo algoritmo funziona unicamente con un albero binario in un caso
non pesato. Si hanno le seguenti casistiche:
\begin{itemize}
  \item $S(x)=\delta_c(x)$ se $x$ è una foglia
  \item $S(x)=S(f_l)\cap S(f_r)$ se $S(f_l)\cap S(f_r)\neq \emptyset$
  con $f_i$ nodi figlio destro e sinistro del nodo $x$. Questo
  significa che si etichetta il padre con l'intersezione dei figli
  \item $S(x)=S(f_l)\cup S(f_r)$ se $S(f_l)\cap S(f_r)= \emptyset$
  con $f_i$ nodi figlio destro e sinistro del nodo $x$. Questo
  significa che si etichetta il padre con l'unione figli
\end{itemize}
La soluzione $B(x)$ è l'insieme degli stati $z$ tale che $M[x,z]$ (costo di
$x$ con stato $z$) sia minimo, con $B(x)=S(x)$. Quindi $B(x)$ è lo stato
migliore che posso ottenere con un determinato nodo $x$.\\
\textit{er generalizzare, si usa un’etichettatura con caratteristiche
  (colori) non pesate, di cui ognuna vale 
  1, e si cerca l’insieme degli stati che compare il maggior numero di
  volte.}

\section{Filogenesi su Distanze}
\begin{definizione}
  Si definisce distanza come la misura della differenza del materiale
  genetico tra distinte specie o individui della stessa specie.\\
  Si ha che minore è la distanza e maggiore è la similitudine tra due
  specie.
  \newpage
  Formalmente si ha $S\times S\to \mathbb{R}^+$, con $S$
  insieme delle specie, con le seguenti proprietà:
  \begin{itemize}
    \item $d(a,b)=0\Longleftrightarrow a=b,\,\,\forall a,b\in S$
    \item $d(a,b)=d(b,a),\,\,\forall a,b\in S$, proprietà simmetrica
    \item $d(a,b)\leq d(a,c)+d(c,b),\,\,\forall a,b,c\in S$,
    diseguaglianza triangolare
  \end{itemize}
\end{definizione}
Si ha quindi una matrice simmetrica contentente la stima delle
distanze tra specie. Ovviamente la diagonale principale sarà nulla per
la prima proprietà. Ogni arco è etichettato con la misura del tempo
trascorso dall’evento di speciazione a oggi. Si ottengono quindi alberi
senza radice.
\begin{esempio}
  \textbf{Esempio preso dagli appunti di Adi e Ila:}\\
  \begin{center}
    \begin{tabular}{l | *{5}{c}}
      M		& M & U & R & T & A \\ \hline
      Mosca	& - & 10 & 2 & 12 & 9 \\
      Uomo	& ~ & - & 9 & 2 & 3 \\
      Ragno	& ~ & ~ & - & 13 & 14 \\
      Topo	& ~ & ~ & ~ & - & 4 \\
      Antilope & ~ & ~ & ~ & ~ & -
    \end{tabular}

    \begin{forest}
      for tree={circle, draw,minimum size=1.5em, s sep=1cm}
      [~,
      [~, edgelabel={left}{8},
      [~, edgelabel={left}{6}, fill=purple, nodevalue={south}{Mosca}],
      [~, edgelabel={right}{2}, fill=green, nodevalue={south}{Ragno}]
      ],
      [~, edgelabel={right}{1},
      [~, edgelabel={left}{3}, fill=gray, nodevalue={south}{Topo}],
      [~, edgelabel={right}{4}, fill=blue, nodevalue={south}{Antilope}]
      ],
      [~, edgelabel={right}{2}, fill=red, nodevalue={south}{Uomo}]
      ]
    \end{forest}
  \end{center}
\end{esempio}
l problema da risolvere è: data una matrice numerica $M$ di distanze,
trovare un albero T la cui matrice di distanza indotta $M_T$ abbia
distanza minima da $M$ (cosa non sempre possibile).\\
Si introduce una misura temporale che parte dalla radice. Se il tempo
è misurato correttamente, la distanza tra tutti i percorsi da una foglia
qualunque alla radice deve essere uguale. Questo deve valere anche per
ogni nodo interno: la distanza fra ogni nodo e tutte le sue foglie
deve essere uguale.\\
Ma non si ha una misura corretta del tempo in quanto si possono
confrontare solo le sequenze a diposizione. Si ha però
l\textbf{ipotesi dell'orologio molecolare} cioè l’ipotesi che il
numero di variazioni genomiche sia una 
misura precisa dello scorrere del tempo. È, come si può intuire,
un'ipotesi troppo semplificativa e quindi utile solo per poter
calcolare le distanze. \textit{Il caso è ideale, non reale, altrimenti
  ogni coppia di figli dovrebbe avere la stessa etichetta che li
  collega al proprio genitore (il che è improbabile).} \\
Si studia quindi l'\textbf{ultrametrica}, ovvero, prese tre specie, si
considerano tutte le distanze tra coppie di queste e si ha che:
\[\max{d(a,b),d(a,c),d(c,b)},\,\,\forall a,b,c\in S\]
è ottenuto da almeno due casi. Quest'ultima proprietà si aggiunge a
quelle normali della distanza sopra definite. Si ha, con quest'ultima
considerazione, che due specie saranno molto vicine tra loro mentre il
resto avrà valori massimi uguali, questo per l'\textit{uguaglianza
  triangolare}. Si hanno quindi matrici ultrametriche con cui si può
ottenere l'unico albero associato in quanto esse non sono più
simmetriche. Gli alberi con radice danno delle storie evolutive
profondamente diverse: è necessario quindi trovare il modo di capire
quale dei nodi interni sia il miglior candidato a radice. Per poterlo
fare, bisogna aggiungere dei cosiddetti \textbf{outgroup}, specie che non
c’entrano niente con il soggetto da analizzare. \\
Si possono, a necessità, aggiungere più outgroup.\\
Vediamo ora come ricostruire l'albero. Se l'albero è binario si parla
di \textbf{distanza additiva} quando si parla della matrice delle
distanze, in quanto la speciazione può essere vista come una divisione
in 2. Si ha che la matrice associata $D$ ultrametriche delle distanze
totali è simmetrica.  
Si ha la \textbf{condizione dei 4 punti}:
Si prendono 4 punti e si calcola:
\[
  \begin{cases}
    D[v,w]+D[x,y]\\
    D[v,x]+D[w,y]\\
    D[v,y]+D[w,x]
  \end{cases}
\]
Il massimo dei tre valori è ottenuto da esattamente due dei 3 casi
sopra. Due saranno circa uguali e maggiori della terza. Se questa
proprietà è soddifatta si ha una distanza additiva.\\
\textit{n un albero, vengono estratte quattro specie (foglie),
  considerando soltanto la porzione di cammini che li collega. Essendo
  l’albero binario, la struttura sarà sempre la stessa. Sono poi
  considerate tutte le coppie di distanze: due di esse avranno un
  valore più elevato, e uno sarà minore. Questo accade perché per due
  coppie non sarà necessario attraversare il cammino intermedio. Per
  calcolare le distanze è sufficiente sfruttare la struttura
  dell’albero, non necessariamente i valori. Se un cammino ha
  etichetta 0, la disuguaglianza diventa uguaglianza, e per
  posizionarlo si contrae un arco}:
\newpage
\textbf{esempio preso dagli appunti di Adi e Ila}
\begin{center}
  

  \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

  \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
    % uncomment if require: \path (0,180); %set diagram left start at 0, and has height of 180

    % Shape: Rectangle [id:dp4575426560517115] 
    \draw   (10,20) -- (54.55,20) -- (54.55,45.45) -- (10,45.45) -- cycle ;
    % Shape: Rectangle [id:dp6927715472240803] 
    \draw   (156.36,20) -- (200.91,20) -- (200.91,45.45) -- (156.36,45.45) -- cycle ;
    % Shape: Rectangle [id:dp155118006605103] 
    \draw   (10,134.55) -- (54.55,134.55) -- (54.55,160) -- (10,160) -- cycle ;
    % Shape: Rectangle [id:dp8400261930370232] 
    \draw   (156.36,134.55) -- (200.91,134.55) -- (200.91,160) -- (156.36,160) -- cycle ;
    % Shape: Circle [id:dp9765420472983006] 
    \draw   (80,88.73) .. controls (80,85.92) and (82.28,83.64) .. (85.09,83.64) .. controls (87.9,83.64) and (90.18,85.92) .. (90.18,88.73) .. controls (90.18,91.54) and (87.9,93.82) .. (85.09,93.82) .. controls (82.28,93.82) and (80,91.54) .. (80,88.73) -- cycle ;
    % Shape: Circle [id:dp0875174655899531] 
    \draw   (120.73,88.73) .. controls (120.73,85.92) and (123.01,83.64) .. (125.82,83.64) .. controls (128.63,83.64) and (130.91,85.92) .. (130.91,88.73) .. controls (130.91,91.54) and (128.63,93.82) .. (125.82,93.82) .. controls (123.01,93.82) and (120.73,91.54) .. (120.73,88.73) -- cycle ;
    % Curve Lines [id:da46139202218941167] 
    \draw [color={rgb, 255:red, 0; green, 134; blue, 255 }  ,draw opacity=1 ]   (54.55,45.45) .. controls (80.95,60.73) and (60.59,77.27) .. (80,88.73) ;


    % Curve Lines [id:da6780938508393746] 
    \draw [color={rgb, 255:red, 255; green, 0; blue, 254 }  ,draw opacity=1 ]   (156.36,45.45) .. controls (127.41,54.36) and (138.86,64.55) .. (130.91,88.73) ;


    % Curve Lines [id:da5564598577024364] 
    \draw [color={rgb, 255:red, 0; green, 255; blue, 27 }  ,draw opacity=1 ]   (90.18,88.73) .. controls (115.64,69.64) and (95.27,107.82) .. (120.73,88.73) ;


    % Curve Lines [id:da7296276339072194] 
    \draw [color={rgb, 255:red, 206; green, 0; blue, 0 }  ,draw opacity=1 ]   (54.55,134.55) .. controls (80,115.45) and (54.55,107.82) .. (80,88.73) ;


    % Curve Lines [id:da3486369957330029] 
    \draw [color={rgb, 255:red, 255; green, 173; blue, 0 }  ,draw opacity=1 ]   (130.91,88.73) .. controls (156.36,69.64) and (130.91,153.64) .. (156.36,134.55) ;



    % Text Node
    \draw (32.27,32.73) node  [align=left] {A};
    % Text Node
    \draw (178.64,32.73) node  [align=left] {C};
    % Text Node
    \draw (32.27,147.27) node  [align=left] {B};
    % Text Node
    \draw (178.64,147.27) node  [align=left] {D};


  \end{tikzpicture}
  \begin{align*}
    d(A, B) + d(C, D) & \quad \coloredbox{blue} \coloredbox{red} \coloredbox{magenta} \coloredbox{yellow} \\
    d(A, C) + d(B, D) & \quad \coloredbox{blue} \coloredbox{red} \coloredbox{magenta} \coloredbox{yellow} \coloredbox{green} \coloredbox{green} \\
    d(A, D) + d(B, C) & \quad \coloredbox{blue} \coloredbox{red} \coloredbox{magenta} \coloredbox{yellow} \coloredbox{green} \coloredbox{green}
  \end{align*}
\end{center}
ci sono due modi di procedere:
\begin{enumerate}
  \item \textbf{procedura con minimo valore}: si prende in input una
  matrice $D$ ultrametrica. Si considera quindi la distanza minima $d$
  tra due specie che avranno un genitore in comune. Si ha che, per
  ogni nodo, la distanza fra esso e tutte le sue foglie dev’essere
  uguale a $d$. Le etichette tra una specie e il padre saranno
  etichettate con $\frac{d}{2}$ e si rimuove una delle due specie
  dalla matrice e si cerca un nuovo $d$. Se un nodo ha già un
  genitore, non si deve dimezzare la distanza, ma si deve tenere in
  considerazione l’etichetta esistente. Sia quindi $a$ il valore
  dell'arco, questo viene sovrascritto con l'etichett $\frac{d}{2}-a$
  \item \textbf{procedura con minimo squilibrio}: è una procedura
  simile alla precedente ma basata sulla diseguaglianza
  triangolare. Si ha che tuttele foglie sono incidenti (collegate) su
  un arco solo, e si vuole trovare l’etichetta minima. Si toglie
  quindi un certo $\delta$ da ogni peso di archi incidenti su foglie
  ottenendo un nuovo albero. Di conseguenza la matrice avrà un
  $-2\delta$ su ogni elemento. Prendiamo quindi una tripla di specie
  con minimo squilibrio $\delta$, si ha che una di quelle specie sarà
  inclusa in un nodo interno. Si itera fino ad ottenere un solo nodo e
  infine si ricostruisce l'albero. Quindi si ha un albero contratto
  (anche non conosciuto) in cui ogni percorso foglia-foglia attraversa
  esattamente due archi ridotti. Si ha che, tra le tre specie:
  \[D(a,b)\leq D(a,c)+D(b,c)\]
  \[\Downarrow\]
  \[D(a,b)\geq D(a,c)-D(b,c)\]
  Il valore dello squilibrio quindi è il doppio della lungheza
  dell'arco incidente sulla foglia in cui la lettera non appare in
  $D(a,b)$.\\
  \textbf{domani spiega meglio questa parte}
\end{enumerate}


\end{document}
