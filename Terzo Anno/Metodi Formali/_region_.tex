\message{ !name(metodi.tex)}\documentclass[a4paper,12pt, oneside]{book}

% \usepackage{fullpage}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage[safe,extra]{tipa}
\usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{fontspec}
\usepackage{enumerate}
\usepackage{braket}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage[cache=false]{minted}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}
\usepackage{tikz}\usetikzlibrary{er}\tikzset{multi  attribute /.style={attribute
    ,double  distance =1.5pt}}\tikzset{derived  attribute /.style={attribute
    ,dashed}}\tikzset{total /.style={double  distance =1.5pt}}\tikzset{every
  entity /.style={draw=orange , fill=orange!20}}\tikzset{every  attribute
  /.style={draw=MediumPurple1, fill=MediumPurple1!20}}\tikzset{every
  relationship /.style={draw=Chartreuse2,
    fill=Chartreuse2!20}}\newcommand{\key}[1]{\underline{#1}} 


\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}


\title{Metodi Formali}
\author{UniShare\\\\Davide Cozzi\\\href{https://t.me/dlcgold}{@dlcgold}}
\date{}

\pgfplotsset{compat=1.13}
\begin{document}

\message{ !name(metodi.tex) !offset(-3) }

\maketitle

\definecolor{shadecolor}{gray}{0.80}
\setlist{leftmargin = 2cm}
\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}
\tableofcontents
\renewcommand{\chaptermark}[1]{%
  \markboth{\chaptername
    \ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%
\chapter{Introduzione}
\textbf{Questi appunti sono presi a lezione. Per quanto sia stata fatta
  una revisione è altamente probabile (praticamente certo) che possano
  contenere errori, sia di stampa che di vero e proprio contenuto. Per
  eventuali proposte di correzione effettuare una pull request. Link: }
\url{https://github.com/dlcgold/Appunti}.\\
\textit{\textbf{Si segnala che le immagini sono tratte dalle slide del
    corso}}.\\  
\textbf{Grazie mille e buono studio!}
\section{Contenuti del Corso}
\textit{Il corso tratta di metodi e tecniche formali per specificare, disegnare e
  analizzare sistemi complessi, in particolare sistemi concorrenti e distribuiti
  costituiti da componenti che operano in modo indipendente e che interagiscono
  tra loro.} \\ \\
Si usa un linguaggio logico che spiega il comportamento di tali sistemi e fa
riferimento alla \textbf{logica temporale} di tali sistemi, in quanto le
proprietà di tali sistemi sono tali per cui evolvono con il cambiamento di stato
del sistema e quindi serve una logica che descriva le proprietà dell'evoluzione
del comportamento.\\
Si parlerà delle \textbf{Reti di Petri}, ovvero uno strumento per modellare tali
sistemi concorrenti e distribuiti. Questo modello ha intrinsechi dei teoremi
matematici atti a studiare il comportamento di tali sistemi.\\
In laboratorio si studieranno algoritmi e strumenti software per la modellazione
e l'analisi di tali sistemi.\\
Si introducono a che sistemi dinamici a tempi discreti, come gli \textbf{automi
  cellulari}.
\chapter{Sviluppo di Modelli e Sistemi}
Si hanno diverse fasi di sviluppo di \textit{sistemi complessi} (nel nostro caso
\textbf{concorrenti} e \textbf{distribuiti}). Si hanno 4 grandi fasi (che
riprendono le generiche fasi dello sviluppo software), che non
seguono una rigida sequenza cronologica tra di loro:
\begin{enumerate}
  \item specifica del problema e delle proprietà della soluzione
  \item modellazione della soluzione
  \item implementazione
  \item verifica, validazione e collaudo, sia sul modello che
  implementazione (con eventuali modifiche)
\end{enumerate}
\textbf{Queste fasi possono alternarsi a vicenda}.\\
\textit{I metodi formali possono svolgere una parte rilevante in tutte queste 4
  fasi e hanno la prerogativa di sviluppare questi sistemi in maniera corretta e
  persistente.} \\
Ci si focalizza sulla modellazione e sulla specifica delle proprietà. Si studia
inoltre la verifica delle proprietà sul modello costruito. \textit{In questo
  corso si lascia un attimo da parte l'aspetto implementativo, che comunque
  seguirebbe alla verifica e alla validazione del metodo}.\\
% note sull'utilità dei metodi formali su elearning e sul loro uso
Si hanno diversi modelli di sistemi concorrenti e distribuiti, presenti in
letteratura: 
\begin{itemize}
  \item \textbf{Algebre di Processi}, ovvero una miriade di diversi linguaggi,
  studiate inizialmente da Milner, che introdusse il calcolo dei sistemi
  comunicanti, un calcolo algebrico utile alla semantica della
  concorrenza. Inoltre Hoare ha introdotto i \textbf{processi sequenziali
    comunicanti} come un nucleo di linguaggio di programmazione, usato come
  linguaggio macchina per le prime macchine parallele. Queste algebre si basano
  sul paradigma di avere un forte aspetto della \textbf{composizionalità}, in
  quanto un sistema viene visto come costituito da diverse componenti autonome
  (sia hardware, che software, che umane) che interagiscono tra loro
  sincronizzandosi (in modo sincrono, \textit{handshaking}, sfruttando un ``canale di comunicazione''
  che viene modellato come un processo) e scambiandosi messaggi. Questo
  paradigma è anche alla base dello sviluppo di molti linguaggi di
  programmazione specificatamente dedicati alla concorrenza.
  \item \textbf{Automi a Stati Finiti}. Un modello concorrente e distribuito
  viene spesso rappresentato attraverso \textbf{sistemi di transizioni
    etichettati}, che sono una derivazione del modello degli automi a stati
  finiti, già usati in letteratura per modellare reti neurali, progettare
  circuiti asincroni, modellare macchine a stati finiti, riconoscere linguaggi
  regolari (il teorema di Kleene ci ricorda che \textit{ad un automa a stati
    finiti è possibile associare un'espressione regolare}) e per la modellazione
  di protocolli di comunicazione. 
  \item \textbf{Reti di Petri}, introdotte da Petri con la \textbf{teoria
    generale delle reti di Petri} nella sua tesi di dottorato. Questa teoria
  parte da una critica al modello a stati finiti dove il focus è su stati
  globali e trasformazione di stati globali. Petri cercava invece una teoria
  matematica (fondata sui principi della fisica moderna della relatività e della
  quantistica) che fosse una teoria dei sistemi in grado di descrivere sistemi
  complessi in cui mettere al centro il flusso di informazione e che potesse
  permettere di analizzare l'organizzazione dal punto di vista del flusso di
  informazione che passa da una componente all'altra. Non si ha il focus,
  quindi, su ``macchine calcolatrici'' ma come supporto alla comunicazione in
  organizzazioni complesse. Si hanno quindi diversi elementi chiave:
  \begin{itemize}
    \item la comunicazione
    \item la sincronizzazione tra componenti
    \item il flusso di informazione che passa tra le varie componenti
    \item la relazione di concorrenza e l'indipendenza causale tra i vari eventi
    che comportano i cambiamenti di stato. Ci si concentra su stati locali
    e non sulla visione di una sequenza di azioni e di uno stato globale
  \end{itemize}
  La teoria delle reti di Petri è stata poi sviluppata e ha avuto diverse
  applicazioni. Sono stati sviluppati diversi linguaggi, ovvero diverse
  \textbf{classi di reti di Petri} per descrivere un sistema complesso a livelli
  differenti di astrazione. \\
  Sono state anche sviluppate tecniche formali di analisi e di verifica del
  modello (disegnato mediante reti di Petri), basate sulla teoria dei grafi e
  sull'algebra lineare.\\
  Le reti di Petri hanno avuto un notevole utilizzo in diversi ambiti
  applicativi anche estranei all'informatica pura e allo studio della
  concorrenze, come la modellazione di sistemi biologici o la modellazione di
  reazioni chimiche. Mediante una classe di reti particolare, le \textbf{reti
    stocastiche} si può valutare le prestazioni di un determinato modello.
\end{itemize}
\subsubsection{Sistemi di Transizioni Etichettati}
\begin{definizione}
  I sistemi di transizione etichettati sono definiti come gli automi a stati
  finiti ma senza essere visti come riconoscitori di linguaggi infatti un sistema
  è formato da un insieme, solitamente finito, di stati globali $S$. Si ha poi un
  \textit{alfabeto} delle possibili azioni che può eseguire il sistema. Si hanno
  anche delle relazioni di transizioni, ovvero delle transizioni che permettono di
  specificare come, attraverso un'azione, si passa da uno stato ad un altro. Le
  transizioni si rappresentano con archi etichettati tra i nodi, che rappresentano
  gli stati. Le etichette degli archi rappresentano le azioni necessarie alla
  trasformazione. L'insieme delle azioni viene chiamato $E$ mentre $T\subseteq
  S\times E\times S$ è l'insieme degli archi etichettati. Può essere,
  opzionalmente, individuato uno stato iniziale $s_0$. Un sistema non è obbligato
  a ``terminare'', quindi non si ha obbligatoriamente uno stato finale.\\
  Riassumendo quindi un sistema di transizione etichettato è un quadrupla:
  \[A=(S,E,T,s_0)\]
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/ste.jpg}
    \caption{Esempio di sistema di transizione etichettato}
  \end{figure}
\end{definizione}
La critica di Petri è che in un sistema distribuito non sia individuabile uno
\textbf{stato globale}, che in un sistema distribuito le trasformazioni di stato
siano \textbf{localizzate} e non globali, che non esista un sistema di
riferimento temporale unico (si possono avere più assi temporali in un sistema
distribuito). Quindi la simulazione sequenziale non deterministica (emantica a
``interleaving'') dei sistemi distribuiti è una forzatura e non rappresenta le
reali caratteristiche del comportamento del sistema, ovvero la località, la
distribuzione degli eventi e la relazione di dipendenza causale e non causale
tra gli eventi.
\section{Sistemi Elementari}
Per introdurre i sistemi elementari delle reti di Petri, ovvero una classe molto
semplice e astratta partiamo da un esempio:
\begin{esempio}
  Vediamo l'esempio del \textit{Produttore e del Consumatore}.\\
  Si ha un sistema con una componente Produttore che produce elementi e li
  deposita in un buffer che ha un'unica posizione (quindi o è pieno o è vuoto) e
  con un consumatore che preleva dal buffer un elemento per poi consumarlo ed
  essere pronto a prelevare un altro elemento. Si ha un comportamento
  ciclico. Usiamo quindi le reti di Petri, col modello dei sistemi elementari,
  per rappresentare questo modello. Bisogna quindi individuare le proprietà
  fondamentali locali del sistema.\\
  Partiamo dal produttore, che può avere 2 stati locali:
  \begin{enumerate}
    \item pronto per produrre
    \item pronto per depositare
  \end{enumerate}
  Usiamo i \textbf{cerchi} per rappresentare condizioni locali che sono
  associabili a delle proposizioni della logica che possono essere vere o
  false. Queste preposizioni sono quindi stati locali. Gli eventi locali vengono
  invece rappresentati con un \textbf{rettangolo}. Un evento ha un arco entrante
  da uno stato che rappresenta le \textit{precondizioni} di quell'evento (che
  devono essere vere per permettere l'occorrenza dell'evento). L'occorrenza
  dell'evento rende false le precondizioni e rende vere le
  \textit{postcondizioni} (che sono stati raggiungibili con un arco uscente da
  un evento). Si ha quindi che il produttore può depositare solo se il buffer
  non è pieno, quindi le postcondizioni di un evento devono essere false
  affinché l'evento possa occorrere (oltre alle precondizioni vere).\\
  Passiamo al consumatore che estrae solo se il buffer è pieno ed è pronto a
  prelevare. Si procede poi con la stessa logica del produttore di cambiamento
  tra vero e falso delle varie condizioni locali.\\
  In questo esempio si hanno quindi condizioni che sono preposizioni booleane e
  rappresentano stati locali.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/prod.jpg}
    \caption{Produttore e Consumatore}
  \end{figure}
  Lo stato globale del sistema è dato da una
  collezione di stati locali. Per segnare tali condizioni mettiamo un punto
  pieno dentro il cerchio e queste condizioni ``abilitano'' i vari eventi:
  \begin{figure}[h]
    \centering
    \includegraphics[scale = 0.7]{img/prod1.jpg}
    \caption{Uno stato globale Produttore e Consumatore dove l’evento\textit{
        produce} è l’unico abilitato} 
  \end{figure}
  Si può arrivare ad una \textit{configurazione} dove, per esempio, sia l'evento
  \textit{produce}, del produttore, che l'evento \textit{preleva}, del
  consumatore, sono abilitati. Si ha quindi che i due eventi possono occorrere
  in modo \textbf{concorrente} infatti i due eventi sono \textbf{indipendenti}
  in quanto condizionati da \textbf{precondizioni e postcondizioni completamente
    disgiunte}. Due eventi che occorrono in maniera concorrente lo possono fare in
  qualsiasi ordine, non si ha infatti una sequenza temporale specifica tra i
  due. \\
  In questo sistema quindi siano solo stati locali ed eventi localizzati e non
  stati ed eventi globali. Un evento dipende solo dalle sue precondizioni e
  dalle sue postcondizioni.\\
  \textit{Se rappresentiamo con delle marche le condizioni vere possiamo
    simulare il comportamento del sistema con il \textit{gioco delle marche} che
    mostra come l'evoluzione delle condizioni avviene all'occorrenza degli
    eventi.} \\
  La simula di un tale sistema può comunque avvenire con un sistema di
  transizioni etichettato, ovvero con un automa a stati finiti, che rappresenta
  gli stati globali corrispondenti alle diverse combinazioni di stati locali che
  di volta in volta sono veri. Gli archi vengono etichettati con gli eventi che
  comportano un cambiamento di stato globale:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{img/prod3.jpg}
    \caption{Semplificazione della nomenclatura del sistema per praticità}
    \includegraphics[scale = 0.8]{img/prod2.jpg}
    \caption{Rappresentazione del sistema con un automa a stati finiti che
      rappresenta stati globali}
  \end{figure}
\end{esempio}
Passiamo ora alla formalizzazione di questi aspetti.\\
\begin{definizione}
  Una \textbf{rete elementare} è definita come una tripla:
  \[N=(B,E,F)\]
  dove:
  \begin{itemize}
    \item $B$ è un insieme finito di \textbf{condizioni}, ovvero \textit{stati
      locali}, preposizioni booleane etc$\ldots$. Vengono rappresentate con un
    cerchio 
    \item $E$ è un insieme finito di \textbf{eventi}, ovvero trasformazioni
    locali di stato e \textit{transizioni locali}. Vengono rappresentate con un
    quadrato 
    \item $F$ è una \textbf{relazione di flusso} che connette condizioni ad
    eventi ed eventi a condizioni. Si ha quindi che:
    \[F\subseteq (B\times E)\cup(E\times B)\]
    Le relazioni di flusso sono rappresentate da archi orientati. Inoltre la
    relazione di flusso è tale per cui non esistano \textbf{elementi isolati},
    in quanto non avrebbero senso, in un tale sistema, eventi isolati (che non
    modificherebbero mai una condizione) o condizioni isolate (che non
    verrebbero mai modificate da un evento). Si ha, formalmente, che: 
    \[dom(F)\cup ran(F)=B\cup E\]
    \textbf{chiedere per formula sopra}
  \end{itemize}
  Si ha che:
  \[B\cap E = \emptyset\]
  \[B\cup E \neq \emptyset\]
  Ovvero gli insiemi delle condizioni e degli eventi sono tra loro disgiunti e
  non vuoti.\\
  Sia ora $x$ un elemento qualsiasi della rete, ovvero $x$ può essere o una
  condizione o un evento, formalmente:
  \[x\in B\cup E\]
  si ha che:
  \begin{itemize}
    \item $^\bullet x=\{y\in X:\,(y,x)\inF\}$ rappresenta l'insieme di tutti gli
    elementi $y$ che sono connessi dalla relazione di flusso ad $x$, ovvero si
    ha un arco da $y$ a $x$. Sono quindi i \textbf{pre-elementi} di $x$, ovvero
    le \textit{precondizioni}, se $x$ è un evento, o i \textit{pre-eventi}, se
    $x$ è una condizione
    \item $x^\bullet=\{y\in X:\,(x,y)\inF\}$ rappresenta l'insieme di tutti gli
    elementi $y$ che sono connessi dalla relazione di flusso a partire da $x$,
    ovvero si ha un arco da $x$ a $y$. Sono quindi i \textbf{post-elementi} di
    $x$, ovvero le \textit{postcondizioni}, se $x$ è un evento, o i
    \textit{post-eventi}, se $x$ è una condizione
  \end{itemize}
  Posso estendere questa notazione ad insiemi di elementi. Sia $A$ un insieme
  qualsiasi di elementi, che possono quindi essere sia condizioni che eventi:
  \[A\subseteq B\cup E\]
  Si ha quindi che i pre-elementi dell'insieme $A$ sono rappresentati con:
  \[^\bullet A=\cup_{x\in A} ^\bullet x\]
  ovvero l'unione dei pre-elementi di ogni singolo elemento dell'insieme $A$.\\
  Analogamente si ha che i post-elementi dell'insieme $A$ sono rappresentati
  con: 
  \[A^\bullet=\cup_{x\in A} x^\bullet\]
  ovvero l'unione dei post-elementi di ogni singolo elemento dell'insieme $A$.
  \textit{Nelle reti c'è sempre una relazione di \textbf{dualità} tra due elementi, per
    esempio tra condizioni ed eventi, tra pre-eventi e post-eventi, tra
    pre-condizioni e post-condizioni. Inoltre si ha la caratteristica della
    \textbf{località}, quindi si hanno stati locali e trasformazioni di stato
    locali}
\end{definizione}
La rete $N=(B,E,F)$ descrive la \textit{struttura statica del sistema}, il
comportamento é definito attraverso le nozioni di \textbf{caso (o
  configurazione)} e di \textbf{regola di scatto (o di transizione)}.\\
Una rete può anche essere suddivisa in sotto-reti, seguendo l'esempio sopra si
potrebbe avere una sotto-rete per il produttore, una per il consumatore e anche
una per il buffer.\\
\begin{definizione}
  Un \textbf{caso} (o \textbf{configurazione}) é un insieme di condizioni
  $c\subseteq B$ che rappresentano l’insieme di condizioni vere in una certa
  configurazione del sistema, un insieme di \textbf{stati locali} che
  collettivamente individuano lo \textbf{stato globale} del sistema.\\
  Graficamente le condizioni vere presentano un puntino in mezzo al cerchio
  mentre le condizioni false solo un cerchio vuoto
\end{definizione}
\begin{definizione}
  Sia $N=(B,E,F)$ una rete elementare e sia $c\subseteq B$ una certa
  configurazione (non serve quindi necessariamente conoscere tutto lo stato del
  sistema). La \textbf{regola di scatto} mi permette di stabilire quando
  un evento $e\in E$ è abilitato, ovvero può occorrere, in $c$ sse:
  \[^\bullet e\subseteq c \mbox{ e } e^\bullet \cap c = \emptyset\]
  ovvero sse tutte le precondizioni dell'evento sono vere (e quindi sono
  contenute nella configurazione $c$) e sse tutte le postcondizioni sono false
  (quindi non si hanno intersezioni tra le postcondizioni e la
  configurazione). \\
  L'occorrenza (l'abilitazione) di $e$ in $c$ si denota con la scrittura:
  \[c[e >\]
  Se un evento $e$ è abilitato in $c$, ovvero $c[e >$, si ha che quando $e$
  occorre in $c$ genera un nuovo caso $c'$ e si usa la notazione:
  \[c[e > c'\]
  Si ha quindi che $c'$ è così calcolabile:
  \[c'=(c-^\bullet e)\cup e^\bullet\]
  Ovvero togliendo da $c$ tutte le precondizioni dell'evento $e$ e aggiungendo
  quindi tutte le postcondizioni di $e$
\end{definizione}
Le reti si basano sul \textbf{principio di estensionalità}, ovvero sul fatto che
il cambiamento di stato è locale:
\begin{center}
  \textit{un evento è completamente caratterizzato dai cambiamenti che produce
    negli stati locali, tali cambiamenti sono indipendenti dalla particolare
    configurazione in cui l’evento occorre.}
\end{center}
L'importante è che le precondizioni di un evento siano vere e le postcondizioni
false (siamo comunque interessati solo alla validità delle condizioni che
riguardano l'evento).
\newpage
\begin{esempio}
  Vediamo un esempio esplicativo dove l’evento $e$ è l'unico abilitato, ovvero
  le sue precondizioni sono vere e le sue postcondizioni sono false.\\
  Lo scatto di $e$ rende le precondizioni false e le postcondizioni vere,
  mentre le altre condizioni rimangono inalterate:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/est.jpg}
  \end{figure}
  Si nota quindi che lo scatto dell'evento $e$ riguarda solo le precondizioni e
  le postcondizioni di quel dato evento, come ci ricorda il principio di
  estensionalità 
\end{esempio}
\begin{definizione}
  Sia $N=(B,E,F)$ una rete elementare. Possiamo definire due tipologie di rete:
  \begin{enumerate}
    \item $N$ è definita \textbf{semplice} sse:
    \[\forall x,y \in B\cup E,\,\, (^\bullet x = ^\bullet y)\wedge (x^\bullet =
      y^\bullet)\Rightarrow x = y\]
    Ovvero per ogni coppia di elementi (che siano quindi eventi o condizioni) se
    i loro pre-elementi e i loro post-elementi coincidono allora non ha senso
    distinguere $x$ e $y$.
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.5]{img/sem.jpg}
      \caption{Esempi di reti \textbf{non} semplici}
    \end{figure}
    \newpage
    \item $N$ è definita \textbf{pura} sse:
    \[\forall e \in E:\,\,^\bullet e \cap e^\bullet = \emptyset\]
    Ovvero se per ogni evento non esiste una precondizione che sia anche
    postcondizioni. Si ha quindi un \textbf{cappio} (detto anche \textbf{side
      condition}) tra un evento e una condizione. Avere questa situazione
    comporta che l'evento non può scattare in quanto la condizione che per lui è
    sia una precondizione che una postcondizioni non può essere
    contemporaneamente vera e falsa, l'evento non potrà mai scattare e quindi
    non potrà mai essere osservato. Non avrebbe quindi senso modellarlo
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.5]{img/pura.jpg}
      \caption{Esempio di rete \textbf{non} pura}
    \end{figure}
  \end{enumerate}
\end{definizione}
\begin{definizione}
  Data una rete elementare $N=(B,E,F)$ e sia $U \subseteq E$ un sottoinsieme di
  eventi e siano $c,c_1,c_2\in B$ tre configurazioni. Si ha che:
  \begin{itemize}
    \item $U$ è un \textbf{insieme di eventi indipendenti} sse:
    \[\forall e_1,e_2\in U:\,\,e_1\neq e_2\Rightarrow (^\bullet e_1\cup
      e_1^\bullet)\cap  (^\bullet e_2\cup e_2^\bullet) = \emptyset\]
    ovvero per ogni coppia distinta di eventi nell'insieme $U$ si ha che le
    precondizioni e le postcondizioni dei due eventi sono completamente
    disgiunte.
    \item $U$ è un \textbf{passo abilitato}, ovvero un insieme di \textit{eventi
      concorrenti} in una certa configurazione $c$, che si indica con:
    \[c[U>\]
    sse:
    \[U \mbox{ è un insieme di eventi indipendenti } \wedge\,\, \forall e\in
      U:\,\, c[e>\]
    $U$ quindi deve essere un insieme di eventi indipendenti e ogni evento in
    $U$ è abilitato in $c$, quindi le sue precondizioni sono vere e le sue
    postcondizioni sono false. Si ha quindi che $U$ è un insieme di eventi
    abitati in maniera concorrente in $c$
    \item $U$ è un \textbf{passo} dalla configurazione $c_1$ alla configurazione
    $c_2$, che si indica con:
    \[c_1[U > c_2\]
    sse:
    \[(c_1[U) \wedge \Big(c_2=(c_1-^\bullet U)\cup U^\bullet\Big)\]
    ovvero sse $U$ è un passo abilitato in $c_1$ e lo scatto degli eventi in $U$
    porta alla configurazione $c_2$ che si ottiene togliendo da $c_1$ l'insieme
    delle precondizioni degli eventi in $U$ e aggiungendo quindi l'insieme delle
    postcondizioni degli eventi in $U$
  \end{itemize}
\end{definizione}
\begin{esempio}
  Riprendiamo l'esempio del produttore e del consumatore.\\
  Sia dato il sistema $\Sigma$ che modella produttore e consumatore
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/prod5.jpg}
  \end{figure}
  Si hanno:
  \begin{itemize}
    \item $\{p,e\},\{p,c\},\{d,c\}$ esempi di insiemi di eventi indipendenti
    \item $\{p,e\}$ che è un passo abilitato in $\{P_1,B,C_1\}$
    \item $\{P_1,B,C_1\}[\{p,w\}>\{P_2,C_2\}$ ovvero lo scatto del passo
    $\{p,e\}$ ci porta in $\{P_2,C_2\}$
  \end{itemize}
\end{esempio}
Diamo ora una definizione formale di \textbf{sistema elementare}.
\begin{definizione}
  Un \textbf{sistema elementare} $\Sigma=(B,E,F;c_{in})$ è definito come una
  rete $N=(B,E,F)$ e a cui è associato un caso iniziale, una configurazione
  iniziale, ovvero un sottoinsieme di condizioni che rappresentano lo stato
  iniziale da cui inizia la computazione e l'evoluzione del sistema. Formalmente
  il caso iniziale si indica con $C_{in}\in B$ 
\end{definizione}
\begin{definizione}
  Dato un sistema elementare $\Sigma=(B,E,F;c_{in})$ si indica con $C_\Sigma$
  l'insieme dei \textbf{casi raggiungibili} da tale sistema a partire dal caso
  iniziale $c_{in}$.\\
  Formalmente l'insieme dei casi raggiungibili è il di piccolo sottoinsieme
  dell'insieme delle parti di $B$, ovvero $2^B$, tale che:
  \begin{itemize}
    \item $c_{in}\in C_\Sigma$, ovvero sicuramente il caso iniziale appartiene
    all'insieme dei casi raggiungibili
    \item se $c\in C_\Sigma,\,U\subseteq E$ e $c'\subseteq B$ sono tali che
    $c[U>c'$ allora $c'\in C_\Sigma$, ovvero se ho un generico caso $c$ che
    appartiene ai casi raggiungibili, se ho un insieme di eventi $U$ tale che
    questo insieme di eventi (che abbiamo visto essere indipendenti, per la
    definizione di passo abilitato) è abilitato
    in $c$ in un unico passo e la sua occorrenza mi porta in $c'$, allora anche
    $c'$ appartiene a $C_\Sigma$. 
  \end{itemize}
  Questa è una definizione data per \textbf{induzione strutturale}, nel primo
  punto si ha la base, nel secondo l'ipotesi e la conseguenza
\end{definizione}
\begin{definizione}
  Dato un sistema elementare $\Sigma=(B,E,F;c_{in})$ si indica con $U_\Sigma$
  l'\textbf{insieme dei passi} di $\Sigma$, ovvero di tutti i possibili insiemi
  di eventi indipendenti che possono occorrere in qualche caso. Formalmente:
  \[U_\Sigma =\{U\subseteq E\,|\,\exists\, c,c'\in C_\Sigma :\, c[U>c'\}\]
  Ovvero l'insieme dei sottoinsiemi di eventi tali per cui esistano due casi
  raggiungibili in $C_\Sigma$ e $U$ è abilitato in $c$ e il suo scatto mi porta
  in $c'$.
\end{definizione}
Definiamo ora il comportamento dei sistemi elementari.
\begin{definizione}
  Sia $\Sigma=(B,E,F;c_{in})$ un sistema elementare e siano $c_i\in C_\Sigma$ ed
  $e_i\in E$.\\
  Definiamo;
  \begin{itemize}
    \item un \textbf{comportamento sequenziale} come una sequenza di eventi che
    possono occorrere dal caso iniziale. Facendo scattare in maniera sequenziale
    gli eventi uno alla volta in $c_n$:
    \[c_{in} [e_1 > c_1 [e_2 > \ldots[e_n > c_n\]
    Scrittura che può essere alleggerita in:
    \[c_{in} [e_1 e_2 \ldots e_n > c_n\]
    Possiamo dire di avere a che fare con una \textbf{simulazione sequenziale
      non deterministica, detta anche \textit{semantica a interleaving}},
    infatti ho più eventi abilitati da prendere uno alla volta
    \item un \textbf{comportamento non sequenziale}, in quanto possiamo anche
    considerare insiemi di eventi, ovvero passi. Considero quindi sequenze di
    passi, avendo a che fare con la \textbf{step semantics}. Non ho quindi una
    simulazione sequenziale non deterministica in quanto dal caso iniziale
    faccio scattare un insieme di eventi, in maniera concorrente (e quindi senza
    ordine specificato), per poi far scattare un altro insieme di eventi fino ad
    arrivare a $c_n$: 
    \[c_{in} [U_1 > c_1 [U_2 > \ldots [U_n > c_n\]
    Scrittura che può essere alleggerita in:
    \[c_{in} [U_1 U_2 \ldots U_n > c_n\]
    Gli insiemi $U_i$ non sono insiemi massimali abilitati ma sottoinsiemi
    indipendenti e abilitati in $c_{in}$.\\
    Posso avere anche un altro tipo di \textbf{comportamento non sequenziale},
    definito da Petri stesso, in una \textbf{semantica ad ordini parziali} in
    cui si definiscono processi non sequenziali. Il comportamento di tale
    sistema viene registrato in una rete di Petri
  \end{itemize}
  In ogni caso si considerano sia sequenze finite che infinite (con cicli) di
  eventi o passi. 
\end{definizione}
\newpage
\begin{esempio}
  Dato il sistema elementare $\Sigma$:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{img/seq.jpg}
  \end{figure}
  si ha, per esempio, la seguente sequenza di occorrenza di eventi:
  \[\{1, 2\}[a > \{3, 2\}[b > \{3, 4\}[c > \{1, 2\}[b > \{1, 4\}[d > \{5\}\]
  arrivati in ``5'' abbiamo un caso finale, ovvero una situazione di
  \textbf{deadlock}, in quanto il sistema non può evolvere ulteriormente.\\
  Vediamo anche la seguente possibile sequenza di passi. In ``1'' e ``2'' sia
  ``a'' che ``b'' sono indipendenti e sono entrambi abilitati (scattano in
  maniera concorrente in un unico passo$\ldots$ ovviamente posso avere passi con lo
  scatto di un solo evento):
  \[\{1, 2\}[\{a, b\} > \{3, 4\}[\{c\} > \{1, 2\}[\{b\} > \{1, 4\}\]
  Come ricordato posso finire in una sequenza infinita.
\end{esempio}
Vediamo ora come modellare e registrare il comportamento del sistema. Un modo è
usando il \textbf{grafo dei casi raggiungibili}.
\begin{definizione}
  Il \textbf{grafo dei casi raggiungibili} di un sistema elementare
  $\Sigma=(B,E,F;c_{in})$ è il sistema di transizioni etichettato:
  \[CG_\Sigma=(C_\Sigma, U_\Sigma, A, c_{in})\]
  dove:
  \begin{itemize}
    \item $C_\Sigma$ è l'insieme dei nodi del grafo, ovvero gli stati globali
    sono i casi raggiungibili dal sistema $\Sigma$    
    \item $U_\Sigma$, è l'alfabeto, ovvero i passi del sistema rappresentano
    l'alfabeto 
    \item $A$ è l'insieme di archi etichettati, formalmente definito come:
    \[A=\{(c,U',c')|\,c,c'\in C_\Sigma, U\in U_\Sigma, c[U>c']\}\]
    ovvero sono archi che connettono uno caso $c$ con un caso $c'$ e sono
    etichettati con un passo $U$ sse $U$ è abilitato in $c$ e porta in
    $c'$. Ovviamente $c$ e $c'$ sono devono essere raggiungibili e $U$ deve
    appartenere all'insieme dei passi di $\Sigma$
  \end{itemize} 
\end{definizione}
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/seq1.jpg}
  \caption{il sistema $\Sigma$}
  \includegraphics[scale = 0.5]{img/seq2.jpg}
  \caption{Grafo dei casi del sistema $\Sigma$}
\end{figure}
\newpage
\subsection{Diamond Property}
Dato un sistema elementare $\Sigma = (B,E,F;c_{in})$ e il suo grafo dei casi
$CG_\Sigma=(C_\Sigma, U_\Sigma, A, c_{in})$ si ha 
che il grafo soddisfa una particolare proprietà, detta \textbf{diamond
  property}, tipica solo dei sistemi elementari.
\begin{definizione}
  La \textbf{diamond property} stabilisce una proprietà della struttura del
  grafo della rete elementare, ovvero, dati $U_1,U_2\in U_\Sigma$ tali che:
  \begin{itemize}
    \item $U_1\cap U_2=\emptyset$
    \item $U_1\neq\emptyset$
    \item $U_2\neq\emptyset$
  \end{itemize}
  e dati $c_i\in C_\Sigma$ allora vale, per esempio:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/diam.jpg}
  \end{figure}
  ovvero se posso rilevare come sottografo una struttura come quella a sinistra
  nell'immagine allora sicuramente tale sottografo contiene anche l'arco gli
  archi per ottenere l'immagine di destra. Il discorso vale anche all'opposto.
\end{definizione}
Si possono fare delle prove:
\begin{enumerate}
  \item \textbf{prima prova:}\\
  Dimostriamo che possiamo passare all'immagine di destra da quella di sinistra
  aggiungendo i due archi mancanti.\\
  Per semplicità diciamo che $U_i$ è un singolo evento $e_i$, con $i=1,2$. Siano
  inoltre $c_1,c_2\in C_\Sigma$, ovvero sono casi raggiungibili, ed $e_1,e_2\in
  E$ tali che $c_1 [e_1 > c_2 [e_2 > \mbox{ e } c_1 [e_2 >$, i due eventi quindi
  sono abilitati in sequenza e da $c_1$ è anche abilitato $c_2$ . Si vuole
  dimostrare che: 
  \[(^\bullet e_1\cup e_1^\bullet)\cap(^\bullet e_2\cup e_2^\bullet)=\emptyset\]
  ovvero che i due eventi sono indipendenti, che sono entrambi abilitati e che
  sono eseguibili in qualsiasi ordine.
  \newpage
  Da $c_1 [e_1 > \mbox{ e }c_1 [e_2 >$ segue che:
  \begin{itemize}
    \item $^\bullet e_1\cap e_2^\bullet=\emptyset$
    \item $^\bullet e_2\cap e_1^\bullet=\emptyset$
  \end{itemize}
  infatti se $e_1$ e $e_2$ sono entrambi abilitati in $c_1$, le loro
  pre-condizioni sono vere e le post-condizioni false, e quindi non è possibile
  che una condizione sia contemporaneamente precondizione di $e_1$ (vera) e
  anche postcondizione di $e_2$ (falsa), e viceversa. Quindi le precondizioni
  di un evento sono disgiunte dalle postcondizioni dell'altro.\\
  Inoltre dal fatto che ho $c_1 [e_1 > c_2 [e_2$, ovvero che da $c_1$ è
  abilitato $e_1$ e che dopo lo scatto di $e_1$ è ancora abilitato $e_2$
  possiamo dire che:
  \begin{itemize}
    \item $e_1^\bullet\cap e_2^\bullet=\emptyset$
    \item $^\bullet e_1\cap\, ^\bullet e_2=\emptyset$
  \end{itemize}
  in $c_2$, infatti, le pre-condizioni di $e_1$ sono false mentre le
  precondizioni di $e_2$ sono vere e quindi $e_1$ e $e_2$ non possono avere
  precondizioni in comune; inoltre sempre in $c_2$ le postcondizioni di $e_1$
  sono vere, mentre quelle di $e_2$ sono false, e quindi $e_1$ e $e_2$ non
  possono avere post-condizioni in comune. Quindi le precondizioni dei due
  eventi sono disgiunte, come del resto anche le postcondizioni, in quanto i due
  eventi sono sequenziali.\\
  Si è quindi dimostrato che i due eventi hanno precondizioni e postcondizioni
  completamente disgiunte e quindi la tesi è verificata
  \item \textbf{seconda prova:}\\
  Analizzando la situazione:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.45]{img/diam2.jpg}
  \end{figure}
  \newpage
  Si supponga che $U_1\cup U_2\in U_\Sigma$ e che si abbiano:
  \begin{itemize}
    \item $U_1\cap U_2=\emptyset$, ovvero sono disgiunti
    \item $U_1\neq\emptyset$
    \item $U_2\neq\emptyset$
  \end{itemize}
  allora se $c_1[(U_1\cup U_2)>c_3$, quindi è abilitato il passo $U_1\cup U_2$
  in $c_1$, sicuramente si ha che sono abilitati anche i singoli passi:
  \begin{itemize}
    \item $c_1[U_1>$
    \item $c_1[U_2>$
  \end{itemize}
  resta da dimostrare che dopo lo scatto di $U_1$ è ancora abilitato $U_2$ in
  $c_2$. Ma se $U_1\cup U_2$ è un passo abilitato significa che posso eseguirli
  in qualsiasi ordine, quindi anche prima $U_1$ e poi $U_2$, e questo comporta
  sicuramente che $U_2$ è abilitato e che porta a $c_3$. Analogamente invertendo
  $U_1$ e $U_2$, formalmente: 
  \begin{itemize}
    \item $c_1[U_1>c_2[U_2>c_3$
    \item $c_1[U_2>c_4[U_1>c_3$
  \end{itemize}
  Si dimostra così che l'immagine di sinistra comporta quella di destra.
\end{enumerate}
Grazie alla diamond property possiamo non considerare il grafo dei casi
raggiungibili ma solo il \textbf{grafo dei casi sequenziale}:
\begin{definizione}
  Un \textbf{grafo dei casi sequenziale} del sistema elementare
  $\Sigma=(B,E,F;c_{in})$ è una quadrupla:
  \[SCG_\Sigma=(C_\Sigma,E,A,c_{in})\]
  dove le etichette sono i singoli eventi (mentre il resto rimane definito come
  nel grafo dei casi raggiungibili). Formalmente si ha quindi che:
  \[A=\{(c,e,c')|,\c,c'\inC_\Sigma,e\in E:\, c[e>c'\}\]
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/seq3.jpg}
    \caption{Esempio di grafo dei casi sequenziale}
  \end{figure}
  Si registra quindi l'occorrenza di un evento alla volta. Il grafo dei casi
  sequenziale è quindi il sistema di transizione con gli archi etichettati dai
  singoli eventi.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{img/seqq.jpg}
    \caption{Esempio di grafo dei casi sequenziale dell'esempio c0on produttore
      e consumatore}
  \end{figure}
\end{definizione}
Riprendendo l'immagine precedente si ha che per la diamond property possono
aggiungere l'arco ``centrale'' che trasformerebbe nuovamente il grafo dei casi
sequenziale in quello dei casi raggiungibili quindi:
\textit{per la diamond property, nei sistemi elementari il grafo dei casi e il
  grafo dei casi sequenziale sono \textbf{sintatticamente equivalenti}, ovvero
  possono essere ricavati a vicenda.\\
  Questo implica il fatto che due sistemi elementari hanno grafi dei casi
  \textbf{isomorfi} sse hanno grafi dei casi sequenziali isomorfi.}
\subsection{Isomorfismo tra Sistemi di Transizione Etichettati}
Si ricorda che:
\begin{center}
  \textit{Si parla di isomorfismo quando due strutture complesse si possono
    applicare l'una sull'altra, cioè far corrispondere l'una all'altra, in modo
    tale che per ogni parte di una delle strutture ci sia una parte
    corrispondente nell'altra struttura; in questo contesto diciamo che due
    parti sono corrispondenti se hanno un ruolo simile nelle rispettive
    strutture.}
\end{center}
Diamo ora una definizione formale di isomorfismo tra sistemi di transizione
etichettati, che possono quindi essere grafi dei casi o grafi dei casi
sequenziali.
\begin{definizione}
  Siano dati due sistemi di transizione etichettati:\\
  $A_1 = (S_1,E_1,T_1,s_{01})$ e $A_2 = (S_2 , E_2 , T_2 , s_{02})$.\\
  e siano date due \textbf{mappe biunivoche}:
  \begin{enumerate}
    \item $\alpha:S_1\to S_2$, ovvero che passa dagli stati del primo sistema a
    quelli del secondo
    \item $\beta:E_1\to E_2$, ovvero che passa dagli eventi del primo sistema a
    quelli del secondo
  \end{enumerate}
  allora:
  \[\langle \alpha,\beta\rangle:A_1= (S_1 , E_1 , T_1 ,s_{01})\to A_2 = (S_2 ,
    E_2 , T_2 , s_{02})\]
  è un \textbf{isomorfismo} sse:
  \begin{itemize}
    \item $\alpha(s_{01})=s_{02}$, ovvero l'immagine dello stato iniziale del
    primo sistema coincide con lo stato iniziale del secondo
    \item $\forall s,s'\in S_1,\forall e\in E_1:\,(s,e,s')\in T_1
    \Leftrightarrow (\alpha(s),\beta(e),\alpha(s'))\in T_2$ ovvero per ogni
    coppia di stati del primo sistema, tra cui esiste un arco etichettato $e$,
    vale che esiste un arco, etichettato con l'immagine di $e$, nel secondo
    sistema che va dall'immagine del primo stato considerato del primo sistema
    all'immagine del secondo stato considerato del secondo sistema, e viceversa
  \end{itemize}
\end{definizione}
\begin{definizione}
  Si definiscono due \textbf{sistemi equivalenti} sse hanno grafi dei casi
  sequenziali, e quindi di conseguenza anche grafi dei casi, \emph{isomorfi}.\\
  Due sistemi equivalenti accettano ed eseguono le stesse sequenze di eventi
\end{definizione}
\subsection{Il Problema della Sintesi}
Si presenta ora un problema tipico dell'informatica, il \textbf{problema della
  sintesi}, ovvero dato un comportamento, o meglio una sua specifica, decidere
se esiste un'implementazione di tale specifica, ovvero un modello, che abbia
esattamente quel comportamento.\\
In questo caso dato un sistema di transizioni etichettato $A=(S,E,T,s_0)$, con:
\begin{itemize}
  \item $S$ insieme degli stati
  \item $E$ insieme delle etichette, ovvero degli eventi
  \item $T$ insieme delle transizioni
  \item $s_0$ stato iniziale
\end{itemize}
ci si propone di stabilire se esiste un sistema elementare
$\Sigma=(B,E,F;c_{in})$, tale che l'insieme degli eventi del sistema esattamente
l'insieme delle etichette di $A$ e tale che il suo grafo dei casi $SCG_\Sigma$
sia isomorfo ad $A$. Ci si propone anche di costruirlo.\\
Il problema è stato risolto mediante la cosiddetta \textbf{teoria delle regioni}
(che però non verrà trattato nel corso). Si può però dire che $A$ dovrà
soddisfare la diamond property, in quanto altrimenti non sarebbe un sistema di
transizioni che potrebbe corrispondere al comportamento di un sistema
elementare.
\subsection{Contatti}
\begin{definizione}
  Sia $\Sigma = (B,E,F;c_{in})$ un sistema elementare e siano $e\in E$ un evento
  e $c\in C_\Sigma$ un caso raggiungibile dal caso iniziale. Allora si ha che
  $(e,c)$ è un \textbf{contatto} sse:
  \[^\bullet e\subseteq c \wedge e^\bullet \cap c \neq\emptyset\]
  Ovvero, in termini pratici, siamo nel caso in cui un evento $e$ ha le
  precondizioni vere, si ha quindi che $^\bullet e\subseteq c$, e l'evento non
  ha tutte le postcondizioni false, quindi $e^\bullet \cap c \neq\emptyset$,
  allora si dice che l'evento $e$ è in una situazione di contatto e quindi non
  può scattare
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/con.jpg}
    \caption{Esempio dove l'evento \emph{deposita} è in una situazione di
      contatto}
  \end{figure}
\end{definizione}
\begin{definizione}
  Sia $\Sigma = (B,E,F;c_{in})$ un sistema elementare. Si dice che il sistema è
  \textbf{senza contatti} sse:
  \[\forall e\in E,\,\forall c\in C_\Sigma\mbox{ si ha che } ^\bullet
    e\subseteq c\Rightarrow e^\bullet\cap c=\emptyset\]
  ovvero per ogni evento e per ogni caso raggiungibile dal caso iniziale succede
  sempre che se le precondizioni sono vere, ovvero $^\bullet e\subseteq c$,
  allora le postcondizioni sono false, ovvero disgiunte dal caso considerato
  ($e^\bullet\cap c=\emptyset$)
\end{definizione}
Ci si chiede se sia possibile trasformare un sistema elementare $\Sigma$, con
contatti, in uno $\Sigma'$, senza contatti, senza però modificarne il
comportamento.\\
La risposta a questo quesito è affermativa e la procedura consiste
nell'aggiungere a $\Sigma$ il complemento di ogni condizione che crea situazione
di contatto, ottenendo così un sistema $\Sigma'$ con grafo dei casi isomorfo a
quello di $\Sigma$.\\ 
Per aggiungere il complemento, data la condizione $x$, si aggiunge la condizione
$not\,\, x$ che sarà vera tutte le volte che $x$ è falsa e viceversa. Per
ottenere questo risultato la nuova condizione avrà come pre-eventi i
post-eventi di $x$ e come post-eventi i pre-eventi di $x$. Ovvero
connetto la nuova condizione agli stessi eventi di quella vecchia ma con archi
orientati in senso opposto. Ovviamente le inizializzazioni delle due condizioni
dovranno essere opposte (una vera e l'altra falsa).
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.6]{img/con2.jpg}
  \caption{Esempio con l'ottenimento del complemento di un sistema}
\end{figure}
Se un sistema è senza contatti si ha una regola di contatto semplificata:
\begin{definizione}
  Sia $\Sigma = (B,E,F;c_{in})$ un sistema elementare \textbf{senza
    contatti}. Sapendo che se le precondizioni di un evento sono vere allora
  sicuramente le postcondizioni di quell'evento sono false in quel caso. Dato
  che questo avviene per ogni evento e per ogni caso raggiungibile dal caso
  iniziale per verificare che un evento $e$ sia abilitato in un caso
  raggiungibile $c$ è sufficiente verificare che le precondizioni di $e$ siano
  vere (in quanto automaticamente le postcondizioni saranno false). In
  maniera formale quindi si ha che: 
  \[c[e\mbox{ sse } ^\bullet e\subseteq c,\,\,\mbox{ con } e\in E,c\in
    C_\Sigma\]
  semplificando di molto la \textbf{regola di scatto}
\end{definizione}
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.6]{img/con3.jpg}
  \caption{Esempio con il complemento del sistema produttore-consumatore (dove è
    stato aggiunto solo il complemento di ``buffer-pieno'', ottenendo così sia
    $B1$ che $B2$, in quanto le altre condizioni avevano già il loro
    complemento). In aggiunta si ha anche il grafo dei casi sequenziale
    corrispondente al nuovo sistema senza contatti (grafo che è isomorfo a
    quello ottenibile al sistema con contatti)} 
\end{figure}
\subsection{Situazioni Fondamentali}
\subsubsection{Sequenza}
\begin{definizione}
  Sia $\Sigma = (B,E,F;c_{in})$ un sistema elementare, con contatti o meno
  e siano $c\in C_\Sigma$ un caso raggiungibile dal caso iniziale e $e_1,e_2\in
  E$ due eventi.\\
  Si ha che $e_1$ ed $e_2$ sono \textbf{in sequenza} nel caso
  raggiungibile $c$ sse:
  \[c[e_1>\wedge\, \neg c[e_2\wedge c[e_1e_2>\]
  ovvero in $c$ è abilitato $e_1$ ma non $e_2$ ma, dopo lo scatto di $e_1$,
  $e_2$ diventa abilitato. Quindi in $c$ è possibile attivare prima $e_1$ e poi
  $e_2$ in sequenza.\\
  Si ha quindi una relazione di \textbf{dipendenza causale tra $e_1$ ed $e_2$},
  ovvero qualche postcondizione di $e_1$ è precondizione di $e_2$ (che quindi
  può occorrere solo se precedentemente è occorso $e_1$).
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/se.jpg}
    \caption{Esempio di sequenza tra $e_1$ ed $e_2$}
  \end{figure}
\end{definizione}
\subsubsection{Concorrenza}
\begin{definizione}
  Sia $\Sigma = (B,E,F;c_{in})$ un sistema elementare, con contatti o meno
  e siano $c\in C_\Sigma$ un caso raggiungibile dal caso iniziale e $e_1,e_2\in
  E$ due eventi. \\
  Si ha che i due eventi sono \textbf{concorrenti} nel caso
  raggiungibile $c$ sse:
  \[c[\{e_1,e_2\}>\]
  ovvero se possono essere abilitati in unico passo o, detto in maniera
  diversa, se sono indipendenti ed entrambi abilitati in $c$
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/conc.jpg}
    \caption{Esempio di concorrenza tra $e_1$ ed $e_2$}
  \end{figure}
\end{definizione}
\subsubsection{Conflitto}
\begin{definizione}
  Sia $\Sigma = (B,E,F;c_{in})$ un sistema elementare, con contatti o meno
  e siano $c\in C_\Sigma$ un caso raggiungibile dal caso iniziale e $e_1,e_2\in
  E$ due eventi.\\
  Si ha che $e_1$ ed $e_2$ sono in conflitto sse:
  \[c[e_1>\wedge c[e_2 \wedge\,\neg c[\{e_1,e_2\}>\]
  ovvero i due eventi sono entrambi abilitati (quindi le precondizioni sono vere
  mentre le postcondizioni son false) ma l'occorrenza di uno disabilità
  l'altro, quindi non possono essere abilitati in un unico passo, in quanto non
  sono indipendenti. Ci sono due casi:
  \begin{enumerate}
    \item i due eventi hanno una precondizione in comune, e in tal caso si parla
    di \textbf{conflitto forward} (ovvero \textit{in avanti})
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.7]{img/conf1.jpg}
      \caption{Esempio di conflitto forward tra $e_1$ ed $e_2$}
    \end{figure}
    \item i due eventi hanno una postcondizione in comune, e in tal caso si parla
    di \textbf{conflitto backward} (ovvero \textit{all'indietro})
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.7]{img/conf2.jpg}
      \caption{Esempio di conflitto backward tra $e_1$ ed $e_2$}
    \end{figure}
  \end{enumerate}
  Si ha quindi una situazione di \textbf{non determinismo}, non essendo
  specificato quale dei due eventi scatterà prima (e lo scatto di uno impedisce
  lo scatto dell'altro).
  \newpage
  Posso ritrovarmi nel caso in cui effettivamente un evento scatta, cambiando lo
  stato del sistema. In tal caso, in un'ottica completamente deterministica, si
  deve assumere che \textbf{l'ambiente} abbia fornito un'informazione
  riguardo il conflitto, ovvero c'è stato qualcosa di esterno che ha permesso ad
  uno dei due eventi di scattare ugualmente. Ho quindi guadagnato
  dell'informazione. 
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/conf3.jpg}
    \caption{Esempio di conflitto con l'intervento dell'ambiente tra $e_1$ ed
      $e_2$ (con conseguente guadagno di informazione)} 
  \end{figure}
  Ci sono però casi in cui in ogni caso non si può avere informazione su quale
  esempio sia scattato. Si può ipotizzare che tale informazione fosse presente
  nello stato precedente del sistema (una sola condizione attiva, per
  esempio). Quindi l'informazione, finita nell'ambiente (ricevuta
  dall'ambiente), si è persa.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/conf3.jpg}
    \caption{Esempio di conflitto tra $e_1$ ed
      $e_2$ con conseguente perdita di informazione (si può ipotizzare, per
      esempio, che nello stato precedente la precondizione di $e_1$ fosse attiva
      mentre quella dio $e_2$ fosse inattiva)} 
  \end{figure}
  Il modello, nell'ottica di Petri, non è quindi un \textbf{modello chiuso} ma è
  in grado di comunicare con l'ambiente (in sintonia con le teorie della
  fisica). 
\end{definizione}
\newpage
\subsubsection{Confusione}
\begin{definizione}
  La situazione di \textbf{confusione} è una \textit{mistura} di situazioni di
  concorrenza e di conflitto. Si hanno 2 tipi di confusione, entrambe
  ammissibili: 
  \begin{enumerate}
    \item detta \textbf{confusione asimmetrica} considera il fatto di avere un
    caso raggiungibile e due eventi abilitati, nella figura $e_1$ ed $e_2$, in
    maniera concorrente in $c$, che nella figura consiste nel caso
    $\{b_1,b_2,b_3\}$. I due eventi sono quindi indipendenti. Nella figura lo
    scatto dei due eventi porterebbe allo stato $c'=\{b_4,b_5\}$. Bisogna
    analizzare però nel dettaglio il sistema. Se prima occorre $e_1$ non si ha
    alcun conflitto mentre se occorre prima $e_2$ (che porterebbe in
    $\{b_1,b_3,b_4\}$) si crea un conflitto tra $e_1$ ed $e_3$, che viene
    risolto a favore di $e_1$ e a sfavore di $e_3$. \textbf{Non è possibile
      stabilire oggettivamente se è stato sciolto un conflitto}. Sono quindi in
    una situazione di confusione in quanto non so se è stata effettuata o meno
    una scelta.
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.7]{img/confu.jpg}
      \caption{Esempio di confusione asimmetrica tra $e_1$ ed $e_2$} 
    \end{figure}
    Una soluzione che vedremo sarà la scomposizione in più componenti del
    sistema 
    \item detta \textbf{confusione simmetrica}, nome dovuto al fatto che la rete
    risulta disegnata in modo simmetrico, comportando delle
    problematiche. Prendiamo nell'immagine il caso raggiungibile $c=\{b_1,b_2\}$
    e i due eventi $e_1$ ed $e_3$, abilitati in maniera concorrente. Si ha che
    $c[\{e_1,e_3\}>c'$, con $c'=\{b_3,b_5\}$. Anche in questo caso si hanno dei
    conflitti, infatti sia $e_1$ che $e_3$ sono in conflitto con $e_2$ (in
    quanto se uno dei due viene eseguito $e_2$ non può più occorrere). Anche in
    questo caso non posso stabilire se il conflitto è stato risolto nel momento
    in cui arrivo in $c'$ (ovvero se si è deciso di fare $e_1$ piuttosto che
    $e_2$ o $e_3$ piuttosto che $e_2$).\\
    Anche in questo caso potremo dividere in componenti (una che esegue $e_1$ ed
    $e_2$ e un'altra che esegue $e_1$ ed $e_3$, con $e_2$ che è una
    sincronizzazione tra le due componenti).\\
    Non si può dire chi ha deciso e chi ha la responsabilità di decidere quale
    evento deve occorrere, se alla componente di $b_1$ o a quella di $b_2$.\\
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.7]{img/confu2.jpg}
      \caption{Esempio di confusione simmetrica tra $e_1$ ed $e_2$} 
    \end{figure}
  \end{enumerate}
  Petri era convinto, nella sua visione deterministica e senza conflitti, che
  l'avere una situazione di confusione nel modello fosse dovuto al non aver
  esplicitato alcuni aspetti o di aver costruito male il modello, con
  informazioni parziali e non complete sul modello e sull'ambiente.\\
  Un altro studioso, Einar Smith, molto vicino a Petri ha invece dimostrato come la
  confusione sia inevitabile
\end{definizione}
% \newpage
Vediamo un esempio famoso, detto della \textbf{mutua esclusione}, portato da
Smith per spiegare come la confusione sia inevitabile nella realtà.
\begin{esempio}
  Si analizza il seguente sistema:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{img/mes.jpg} 
  \end{figure}
  In rosso e in Blu abbiamo specificate le due componenti del sistema, che
  condividono una risorsa, ovvero la condizione $b_4$, che rappresenta che la
  risorsa è libera e a disposizione. L'evento $e_1$ e evento $e_4$ rappresentano
  eventi di acquisizione della risorsa (rispettivamente per la prima e per la
  seconda componente) e quindi le loro precondizioni rappresentano la necessità
  di acquisirla. Tra questi due eventi c'è una \textbf{situazione di
    conflitto}. Le condizioni $b_2$ e $b_7$, ovvero le rispettive postcondizioni
  dei due eventi, rappresentano che la risorsa è in uso per la rispettiva
  componente mentre gli eventi $e_2$ ed $e_5$ rappresentano il rilascio della
  risorsa condivisa, sempre per la rispettiva componente, arrivando
  rispettivamente nella componente $b_3$ e $b_8$. Ovviamente la risorsa
  non può essere contemporaneamente in uso da entrambe le risorse, quindi $b_2$
  e $b_7$ non possono essere contemporaneamente marcate, ovvero vere, e per
  questo si parla di \textit{mutua esclusione} (se una delle due è vera l'altra
  deve essere necessariamente falsa). D'altro canto gli
  eventi $e_3$ ed $e_6$ possono invece occorrere in modo concorrente senza
  conflitti.\\
  Scrivendo formalmente si ha che, nel caso che la risorsa sia stata acquisita
  dalla componente rossa e successivamente rilasciata:
  \[\{b_3,b_4,b_6\}[\{e_3,e_4\}>\{b_1,b_7\}\]
  ma se scatta prima $e_3$ ho il conflitto tra $e_1$ ed $e_4$, se scatta prima
  $e_4$ non ho conflitti con $e_1$. Quindi non ho informazioni sulla risoluzione
  del conflitto.\\
  \textbf{Capire se è confusione simmetrica}
\end{esempio}
\subsection{Sottoreti}
Partiamo subito con una definizione formale:
\begin{definizione}
  Siano $N=(B,E,F)$ e $N_1=(B_1,E_1,F_1)$ due reti elementari.\\
  Si dice che $N_1$ è \textbf{sottorete} di $N$ sse:
  \begin{itemize}
    \item $B_1\subseteq B$, quindi l'insieme delle condizioni della rete $N_1$
    è sottoinsieme di quello della rete $N$
    \item $E_1\subseteq E$, quindi l'insieme degli eventi della rete $N_1$
    è sottoinsieme di quello della rete $N$
    \item $F_1=F\cap[(B_1\times E_1)\cup (E_1\times B_1)]$, ovvero la relazione
    di flusso di $N_1$ è definita come la restrizione della relazione di flusso
    di $N$ rispetto alle condizioni e $B_1$ e agli eventi $E_1$ (tengo quindi solo
    gli archi di $N$ che connettono eventi e condizioni di $N_1$)
  \end{itemize}
\end{definizione}
\begin{definizione}
  Siano $N=(B,E,F)$ e $N_1=(B_1,E_1,F_1)$ due reti elementari.\\
  Si dice che $N_1$ è \textbf{sottorete generata da} $B_1$ di $N$ (ovvero di
  sottorete generata da un insieme di condizioni) sse:
  \begin{itemize}
    \item $B_1\subseteq B$, quindi l'insieme delle condizioni della rete $N_1$
    è sottoinsieme di quello della rete $N$
    \item $E_1=\, ^\bullet B_1\cup B_1^\bullet$, ovvero come eventi si
    hanno tutti quegli eventi che sono collegati in $N$ alle condizioni incluse
    nell'insieme di condizioni $B_1$, prendendo quindi tutti i pre-eventi e i
    post-eventi delle condizioni dell'insieme $B_1$
    \item $F_1=F\cap[(B_1\times E_1)\cup (E_1\times B_1)]$, ovvero la relazione
    di flusso di $N_1$ è definita come la restrizione della relazione di flusso
    di $N$ rispetto alle condizioni $B_1$ e agli eventi $E_1$
  \end{itemize}
  Non ho quindi una sottorete generata da un insieme arbitrario di condizioni ed
  eventi ma questi ultimi sono direttamente presi in relazione all'insieme delle
  condizioni scelto
\end{definizione}
\begin{definizione}
  Siano $N=(B,E,F)$ e $N_1=(B_1,E_1,F_1)$ due reti elementari.\\
  Si dice che $N_1$ è \textbf{sottorete generata da} $E_1$ di $N$ (ovvero di
  sottorete generata da un insieme di condizioni) sse:
  \begin{itemize}
    \item $B_1=\,^\bullet E_1\cup E_1^\bullet$, ovvero come condizioni si
    hanno tutte quelle condizioni che sono collegati in $N$ agli eventi inclusi
    nell'insieme di eventi $E_1$, prendendo quindi tutte e precondizioni e le
    postcondizioni degli eventi dell'insieme $E_1$
    \item $E_1\subseteq E$, quindi l'insieme degli eventi della rete $N_1$
    è sottoinsieme di quello della rete $N$
    \item $F_1=F\cap[(B_1\times E_1)\cup (E_1\times B_1)]$, ovvero la relazione
    di flusso di $N_1$ è definita come la restrizione della relazione di flusso
    di $N$ rispetto alle condizioni $B_1$ e agli eventi $E_1$
  \end{itemize}
  Non ho quindi una sottorete generata da un insieme arbitrario di condizioni ed
  eventi ma le prime sono direttamente prese in relazione all'insieme degli
  eventi scelto
\end{definizione}
\newpage
\begin{esempio}
  Tornando all'esempio della mutua esclusione:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{img/mes.jpg} 
  \end{figure}
  si ha, per esempio:
  \begin{itemize}
    \item in rosso si ha la sottorete $N'=(\{b_1,b_2,b_3\},
    \{e_1,e_2,e_3\},F')$, che è la sottorete generata  dall'insieme di
    condizioni $B'=\{b_1,b_2,b_3\}$ 
    \item in blu si ha la sottorete $N'=(\{b_6,b_7,b_8\},
    \{e_4,e_5,e_6\},F')$, che è la sottorete generata dall'insieme di condizioni
    $B'=\{b_6,b_7,b_8\}$
    \item si ha la sottorete $N'=(\{b_2,b_4,b_7\},\{e_1,e_2,e_4,e_5\},F')$, che
    è la sottorete generata  dall'insieme di condizioni $B'=\{b_2,b_4,b_7\}$
    \item si ha la sottorete $N'=(\{b_1,b_2,b_3,b_4\},\{e_1,e_2,e_3\},F')$, che è la
    sottorete generata da  dall'insieme di eventi $E'=\{e_1,e_2,e_3\}$
  \end{itemize}
\end{esempio}
\subsection{Operazioni di Composizione per Reti di Petri}
Data una rete $N=(B,E,F,c_0)$ questa può essere ottenuta componendo altre reti
di Petri. Si hanno in letteratura 3 modi principali:
\begin{enumerate}
  \item la \textbf{composizione sincrona}
  \item la \textbf{composizione asincrona}
  \item la \textbf{composizione mista, tra sincrona e asincrona}
\end{enumerate}
\newpage
Iniziamo informalmente a vedere degli esempi pratici.
\begin{esempio}
  Supponiamo di avere i modelli di due componenti, $N_1$, con un evento che
  corrisponde ad un'azione di invio, ed $N_2$, con un evento che corrisponde ad
  un'azione di ricezione:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/sinc.jpg} 
  \end{figure}
  Supponiamo che invio e ricezione siano eventi corrispondenti all'handshacking,
  ovvero l'invio avviene solo se può avvenire la ricezione.\\
  Vado quindi a sincronizzare questi due eventi, che diventano quindi un'unico
  evento nella rete composta, che è abilitato se le due precondizioni, nelle due
  componenti sono entrambe vere: 
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/sinc2.jpg} 
  \end{figure}
  La composizione viene indicata con:
  \[N_1||N_2\]
  Lo scatto dell'evento, in maniera sincrona, rende vere le postcondizioni nelle
  due componenti e false le due precondizioni.\\
  Abbiamo appena visto un esempio di \textbf{composizione sincrona}
\end{esempio}
\newpage
\begin{esempio}
  Supponiamo di avere i modelli di due componenti, $N_1$, che invia in un canale
  un messaggio (per esempio in un buffer), e $N_2$, che riceverà il messaggio
  solo quando esso sarà disponibile:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/asinc.jpg} 
  \end{figure}
  In questo caso, a differenza dell'esempio precedente, non identifichiamo
  eventi ma condizioni. Identifico quindi il canale (le due condizioni) come uno
  solo, che avrà il pre-evento in una componente e il post-evento nell'altra.\\
  Quindi il pre-evento, nella componente $N_1$, può scattare solo se questa
  nuova condizione condivisa, il canale, è libera, indipendentemente dalla
  componente $N_2$. D'altro canto l'evento in $N_2$ può scattare solo se la
  condizione condivisa è marcata, indipendentemente dallo stato della prima
  componente, liberando il canale di comunicazione.\
  Avvio e ricezione (dopo che il messaggio è stato inviato può essere letto in
  un qualsiasi futuro) non sono sincronizzati e si ha quindi a che fare con
  un esempio di \textbf{composizione asincrona}
\end{esempio}
Sarà interessante studiare come la composizione di due componenti, per esempio,
senza deadlock non comporta, in generale, l'ottenimento di una rete priva di
deadlock.
\newpage
\section{Reti Posti e Transizioni}
Vediamo ora una nuova classe delle \textit{reti di Petri}, detta \textbf{reti
  Posti e Transizioni}, permette di rappresentare un sistema in modo più
compatto, sono una sorta di \textit{ripiegamento} dei sistemi
elementari. Partiamo quindi da un esempio:
\begin{esempio}
  prendiamo in studio sempre il sistema produttore-consumatore:
  Sia nella versione con contatto:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/pt.jpg} 
  \end{figure}
  che senza:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/pt2.jpg} 
  \end{figure}
  Suppongo di voler modellare il sistema in modo che il buffer possa avere un
  numero determinato di posizioni, per esempio 2 (il buffer può quindi
  depositare fino a due elementi). Si ottiene quindi:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/pt3.jpg} 
  \end{figure}
  Dove si hanno due posizioni del buffer (e quindi basta che una sia vuota per
  permettere al produttore di depositare) a disposizione del sistema. Si nota
  come l'aumento dei buffer complica drasticamente la modellazione del sistema.
  Si cerca quindi una soluzione più compatta, compattando gli eventi
  \textit{deposita} e \textit{preleva} e dando nuova notazione al buffer. Si
  ottiene:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/pt4.jpg} 
  \end{figure}
  con il buffer che diventa anche un \textbf{contatore} del numero di elementi
  presenti al suo interno. Un buffer a due posizioni diventa quindi una
  condizione non più booleana, detta \textbf{Posto}, con una \textbf{capacità}
  pari a 2. Il produttore non può produrre oltre la capacità.\\
  Con questa rappresentazione non ho più alcuna difficoltà nel rappresentare più
  posizioni del buffer in quanto basta aumentare la capacità. Ho però perso
  delle informazioni infatti non ho più una relazione di dipendenza tra dove si
  deposita (se nella prima posizione o nella seconda) e dove si preleva.\\
  Aggiungiamo un'altra complicanza: aggiungiamo un secondo consumatore:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/pt5.jpg} 
  \end{figure}
  Possiamo \textit{comprimere} nella stessa maniera, ottenendo, in quanto i due
  consumatori hanno lo stesso modello di comportamento:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/pt6.jpg} 
  \end{figure}
  Anche qui perdo l'informazione riguardo quale dei due consumatori ha
  effettivamente prelevato, riguardo quale dei due è pronto a \textit{prelevare}
  e quale a \textit{consumare}, continuando ad ignorare anche la posizione
  del buffer nei confronti della quale stanno agendo.\\
  Si può arrivare ad avere due componenti identiche che sono concorrenti tra
  loro. 
\end{esempio}
Le condizioni non sono più, in generale, booleane ma sono \textbf{posti}
dotati di \textbf{contatori}, gli elementi all'interno sono detti
\textbf{marche}. Ai posti si assegna anche una \textbf{capacità}. 
\\
Si ha inoltre che lo scatto di una transizione dipende dalla disponibilità
delle risorse, per esempio un produttore può produrre $n$ elementi alla volta
e il consumatore consumarne $m$. Si usano quindi archi pesati (se non indicato
ovviamente ha peso 1, quello del normale check booleano di condizione attiva)
tali che una transizione possa scattare sse i pesi vengono rispettati:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.7]{img/pt7.jpg}
  \caption{Esempi con la transizione $t_i$ non abilitata}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.7]{img/pt8.jpg}
  \caption{Esempio con la transizione $t_i$ abilitata, a sinistra il ``prima''
    e a destra il ``dopo'' lo scatto di $t_i$}
\end{figure}
\newpage
\begin{esempio}
  Petri aveva introdotto l'uso delle reti anche per le reazioni chimiche, per
  esempio:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.35]{img/pt9.jpg}
    \caption{Esempio con la modellazione della produzione di $H_2O$}
  \end{figure}
\end{esempio}
\subsection{I Filosofi a Cena}
Questo è un classico problema di sincronizzazione, introdotto da Dijkstra. Si ha
un tavolo rotondo con 5 filosofi ($p_i,\,\i=0,\ldots 4$), ciascuno ha davanti un
piatto di spaghetti e si hanno solo 5 forchette ($f_i,\,\i=0,\ldots 4$):
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/pt10.jpg}
  \caption{Rappresentazione schematica del problema dei 5 filosofi}
\end{figure}
Ogni filosofo ha lo stesso comportamento, un po' pensa e un po' mangia,
prendendo prima la forchetta alla sua destra e poi quella alla sua sinistra
(perché necessitano di due forchette per mangiare). Si ha quindi il
\textit{deadlock} se tutti vogliono mangiare, prendendo tutti in primis una
forchetta, impedendosi tutti a vicenda di mangiare (non potendo prendere due
forchette) o si ha la \textit{starvation} in quanto potrebbero accordarsi per
mangiare in quattro, impedendo a uno di mangiare.\\
Si vuole modellare tale schema. Ogni filosofo può essere modellato come una rete
elementare, che farà da componente al modello finale:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/pt11.jpg}
\end{figure}
con gli eventi \textit{upD} e \textit{upS}, dove il filosofo prende la forchetta
destra e sinistra, e rispettivamente \textit{downD} e \textit{downS}, dove le
mette giù. Si ha la condizione che specifica che il filosofo sta pensando e
quella che mi segnala l'azione del mangiare, oltre alle due condizioni
intermedie che separano le azioni tra forchetta sinistra e destra.\\
Si modella anche la componente della forchetta, con gli eventi che segnalano se
è depositata, \textit{down}, o meno, \textit{up}:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/pt12.jpg}
\end{figure}
\newpage
Combino quindi le due componenti per specificare il filosofo che prende la sua
forchetta destra e poi la sinistra, quindi per ogni componente \textit{filosofo}
si hanno due componenti \textit{forchetta}:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/pt13.jpg}
\end{figure}
con la \textit{forchetta 1 (fork 1)} che sarà la forchetta destra e la
\textit{forchetta 2 (fork 2)} che sarà la forchetta sinistra. Si hanno quindi
diverse transizioni di sincronizzazione tra le due componenti (ogni volta che il
filosofo interagisce con la forchetta).\\
Bisogna aggiungere che ogni forchetta può essere presa da due filosofi, ognuna a
destra di un filosofo e a sinistra di un altro:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/pt14.jpg}
\end{figure}
Ogni forchetta quindi si sincronizza alternativamente tra due filosofi, venendo
presa da uno dei due filosofi. Si è arrivati quindi ad una \textbf{situazione di
  conflitto}, o meglio ad una \textbf{situazione di confusione}.\\
Per ora la soluzione di questo problema viene lasciato da parte per valutare il
modello dello stesso.\\
Si ha quindi una possibile rappresentazione del modello completo:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/pt15.jpg}
\end{figure}
Ma ogni filosofo, come del resto ogni forchetta, si comporta nella stessa
maniera. Si tenda quindi di \textit{ripiegare} il modello, modellando un
un'unica componente filosofo con 5 marche distribuite nei vari stati locali del
filosofo. Stesso discorso per le forchette. Ottengo quindi il seguente modello:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.6]{img/pt16.jpg}
\end{figure}
Con le due componenti, \textit{filosofo} e \textit{forchetta}, entrambe
inizializzate con 5 marche ciascuno. Questo modello però perde informazione
sulla forchetta che un filosofo può prendere, si ha un radicale \textbf{cambio
  di protocollo}. Un filosofo può prendere una qualsiasi forchetta e non più
quella alla sua destra/sinistra. Non posso quindi usare le \textit{reti Posti e
  Transizioni} in quanto perdo troppe informazioni.\\
L'unica soluzione possibile è quindi quella di recuperare le informazioni perse
inserendole nelle marche, alle quali viene aggiunta una struttura dati. Si ha
così modo di distinguere i vari filosofi e le forchette. Per compattare il
sistema devo quindi rendere più complessa l'essenza della marca, viene
arricchita con una struttura dati.\\
Si arriva così ad avere una \textbf{Rete di Alto Livello}, come, per esempio,
una \textit{rete colorata}, a partire da un rete elementare.\\
Distinguo quindi filosofi e forchette nei due insiemi di strutture dati:
\begin{enumerate}
  \item $Phil=\{p_0,\ldots, p_4\}$, insieme dei filosofi
  \item $Fork=\{f_0,\ldots, f_4\}$, insieme delle forchette
\end{enumerate}
Si nota che per gli indici si ha una somma \textit{modulo 5}, ovvero gli indici
rispondono alla regola:
\[(i+1) \mbox{ mod } 5\]
In modo che gli indici siano ordinati avendo inoltre lo 0 che segue il 4, in
modo circolare.\\
Si ottiene quindi, mantenendo il protocollo iniziale:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.6]{img/pt17.jpg}
\end{figure}
Si mantiene quindi un modello simile a quello descritto sopra ma al posto di
marche non strutturate si ha nel posto delle 5 forchette le marche dell'insieme
\textit{Fork} e al posto dei 5 filosofi le marche dell'insieme \textit{Phil}:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.6]{img/pt18.jpg}
\end{figure}
Le transizioni scattano in determinate condizioni. Per esempio la prima
transizione, relativa al fatto che il filosofo prende la forchetta alla sua
destra, scatta sse con l'istanza filosofo $p_j$ e forchetta $f_i$ si ha che
$i=j$ (ho quindi almeno una forchetta, almeno un filosofo e la forchetta deve
essere quella alla sua destra, che, per come abbiamo modellato il problema, è
quella con lo stesso indice). Sugli archi si hanno quindi annotate determinate
variabili che denotano istanze. Per la forchetta a sinistra si usa il modulo
5. Si procede quindi per i vari filosofi.
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/pt19.jpg}
  \caption{Esempio con il filosofo $p_3$ che ha già preso la forchetta alla sua
    destra, $f_3$, e prende quella a sinistra, $f_4$}
\end{figure}
\textit{Si ha quindi un prezzo per rappresentare in maniera compatta un sistema
  elementare, mediante una rete di alto livello, ovvero l'arricchimento della
  struttura dati}.
\subsection{Formalizzazione delle Reti Posti e Transizioni}
Nonostante la sezione si occupi di formalismi ci appoggiamo ad un esempio per
avere un confronto diretto tra teoria e pratica. Questo esempio è un'n-sima
versione del sistema produttore-consumatore, con un produttore che deposita
a due elementi alla volta, in un buffer che ha capacità massima apri a cinque,
che vengono consumati da due consumatori:
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.65]{img/pt20.jpg}
\end{figure}
quindi al massimo ho cinque marche nel buffer e 2 in uno degli stati del
consumatore (in quanto rappresenta due consumatori).\\
Passiamo ora alla formalizzazione:
\begin{definizione}
  Si definisce un \textbf{sistema Posti e Transizioni (\textit{sistema P/T})} la
  sestupla: 
  \[\Sigma=(S,T,F,K,W;M_0)\]
  dove:
  \begin{itemize}
    \item $(S,T,F)$ è una rete con:
    \begin{itemize}
      \item $S$ che rappresenta l'insieme dei posti
      \item $T$ che rappresenta l'insieme delle transizioni
      \item $F$ che rappresenta la relazione di flusso che lega posti e
      transizioni tramite archi
    \end{itemize}
    \item $K:S\to\mathbb{N}^{+}\cup \{\infty\}$ che rappresenta una funzione che
    ad ogni posto in $S$ assegna un valore, che può essere un naturale
    strettamente positivo o anche infinito (ovvero in quel posto posso avere un
    numero qualsiasi di marche). Il valore zero non è ammesso in quanto
    implicherebbe che il posto non potrebbe mai essere occupato, rendendo la
    modellazione di un tale posto inutile. $K$ è detta \textbf{funzione
      capacità dei posti}.
    \item $W:F\to\mathbb{N}\cup \{\infty\}$  che rappresenta una funzione che
    assegna ad ogni arco un peso mediante un valore naturale che stavolta può
    essere, oltre che infinito, anche zero. $W$ è detta \textbf{funzione peso
      degli archi}
    \item $M_0:S\to \mathbb{N}\cup\{\infty\}:\,\forall s\in
    S\,\,t.c.\,\,M_0(s)\leq K(s)$ che rappresenta la \textbf{marcatura iniziale}
    del sistema, ovvero una funzione che assegna ad ogni posto un naturale,
    eventualmente nullo o infinito, minore o uguale alla capacità massima di
    tale posto (capacità espressa dalla funzione $K$) indicante il numero di
    marche allo stato iniziale. 
  \end{itemize}
  Bisogna ora definire la regola di scatto, ovvero il \textbf{gioco delle
    marche}, la regola per cui le marche si spostano sulla rete. Si ha quindi,
  dati:
  \[M:S\to\mathbb{N}\cup \{\infty\}\mbox{ e }t\in T\]
  ovvero data una marcatura e una qualsiasi transizione $t$ si ha che:
  \[M[t>\]
  ovvero una transizione è abilitata in una certa marcatura,
  \begin{center}
    sse:
  \end{center}
  \[\forall s\in S,\,\,M(s)\geq W(s,t)\wedge M(s)+W(t,s)\leq K(s)\]
  ovvero per ogni posto si ha che ci sono abbastanza marche nei posti perché
  possa scattare la transizione, ovvero c'è un arco di peso corretto che collega
  quel posto con la transizione, avendo peso dell'arco minore o uguale al numero
  di marche del posto, e, inoltre, si deve verificare che la transizione non
  metta troppe marche in quel posto, quindi la marcatura del posto (ovvero il
  numero di marche già presenti in esso) più il numero di marche che si
  aggiungono con lo scatto della transizione non deve superare la capacità del
  posto.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.75]{img/pt21.jpg}
    \caption{Nell'esempio si ha che la transizione $t_2$ può scattare sse nella
      posto precedente ho almeno due marche, in quanto l'arco tra i due ha peso
      due. Inoltre ho il posto che segue vuoto ma con capacità massima pari a
      tre, quindi la transizione può scattare in quanto l'arco tra i due pesa
      due, assicurandomi che dopo la transizione, nel posto che la segue, non
      verrà superata la capienza massima, arrivando infatti ad avere marcatura
      pari a 2}
  \end{figure}
  Lo scatto della transizione, se questa è abilitata nella marcatura, mi genera
  una nuova marcatura che viene ottenuta da quella precedente togliendo tante
  marche dal posto che è di input alla transizione quanto il peso dell'arco che
  connette tale posto alla transizione e aggiungendo tante marche al posto in
  output quante il peso dell'arco che connette la transizione a tale posto,
  ovvero, formalmente: 
  \[M[t>M'\]
  \begin{center}
    sse
  \end{center}
  \[M[t> \wedge\, \forall s\in S,\,\,M'(s)=M(s)-W(s,t)+W(t,s)\]
  quindi il nuovo posto avrà marcatura pari a quella precedente al più dei due
  contributi, il primo negativo e il secondo positivo, dei pesi dei due archi.
\end{definizione}
\begin{definizione}
  Dato un sistema P/T $\Sigma=(S, T , F , K , W;M_0)$ si definisce
  l'\textbf{insieme delle marcature raggiungibili}, dalla marcatura iniziale  di
  $\Sigma$ come:
  \[[M_0>\]
  ed esso è il più piccolo insieme tale che:
  \begin{itemize}
    \item $M_0\in [M_0>$, ovvero ola marcatura iniziale appartiene all'insieme
    delle marcature raggiungibili
    \item se $M\in [M_0> \wedge\,\exists t\in T:\,M[t>M'$ allora $M'\in [M_0$,
    ovvero se $M$ appartiene all'insieme delle marcature raggiungibili ed esiste
    una transizione una transizione tale per cui $M$ va in $M'$ allora, di
    conseguenza si ha che $M'$ appartiene all'insieme delle marcature
    raggiungibili dallo stato iniziale
  \end{itemize}
  \emph{Anche questa è una definizione per induzione}
\end{definizione}
\begin{esempio}
  Vediamo qualche esempio particolare (se la capacità non è specificata si ha
  che essa è infinita):
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/pt22.jpg}
    \caption{Lo scatto toglie due marche dall'input e ne mette una nell'output
      ma la seconda volta $t$ non sarebbe più abilitata in quanto il posto in
      output si satura alla prima transizione, è un caso di \textbf{contatto}}
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/pt23.jpg}
    \caption{La transizione $t$ è abilitata avendo i posti in ingresso col
      giusto numero di marche e in uscita ha un posto vuoto che può riempire a
      piacere e lo stesso porto che prima aveva in input che può riempire con
      una marca rispettando le regole di capacità}
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/pt24.jpg}
    \caption{La transizione $t$ non è abilitata, e non lo sarà mai in $M$ in
      quanto la regola di scatto prevede che ci sia a priori una capacità
      sufficiente nei posti di output, cosa che qui non accade avendo uno di
      essi capacità uno, non si ragiona in modo sequenziale ``prima tolgo e poi
      metto'', lo spazio deve essere disponibile a priori per far scattare la
      transizione} 
  \end{figure}
  \newpage
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/pt25.jpg}
    \caption{In questa rete si ha che $t_1$ e $t_2$ sono abilitate in $M$ ed
      esse sono \textbf{concorrenti} in quanto lo scatto dell'una non disabilita
      lo scatto dell'altra dovendo entrambe prelevare una marca da un posto che
      ne contiene due per riversarne una ciascuna in un posto libero. Si ha
      quindi concorrenza anche in corrispondenza di transizioni non
      indipendenti}  
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.55]{img/pt26.jpg}
    \caption{In questa rete si ha che $t_1$ e $t_2$ sono in \textbf{conflitto} e
      \textbf{non concorrenti} tra loro. Si ha infatti che lo scatto di una
      disabilita quella dell'altra, visto che da sola $t_2$ svuoterebbe il posto
      in input (se scatta prima $t_1$ poi ho una sola marca nello stato in input
      disabilitando $t_2$ che ne richiede due, viceversa lo scatto di $t_2$
      lascerebbe vuoto lo stato in input impedendo a $t_1$ di scattare). Il
      conflitto è dato non dalla struttura della rete ma dalla sua marcatura
      iniziale. Quindi in questa rete con questa marcatura può scattare una sola
      delle due transizioni, inoltre $t_1$ potrebbe occorrere due volte senza
      errori, essendo quindi, se occorre contemporaneamente per quelle due volte,
      \textbf{concorrente con se stessa}. La doppia occorrenza di $t_1$, indicatat
      con $2t_1$ è un \textbf{passo}}  
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/pt27.jpg}
    \caption{In questa rete si ha che $t_1$ è abilitata due volte, avendo
      quindi $2t_1$ e lo è anche $t_2$, che può scattare solo una volta. Si ha
      quindi che $t_1$ occorre concorrente con se stessa e contemporaneamente
      concorrente con $t_2$. Si ha quindi che il \emph{multiset} $2t_1+t_2$ è un
      \textbf{passo abilitato}} 
  \end{figure}
\end{esempio}
Si da ora la definizione formale di \textit{multiset di transizioni abilitate}
in un sistema:
\begin{definizione}
  Dato un sistema P/T $\Sigma=(S, T , F , K , W;M_0)$ si definisce un
  \textbf{multiset} $U:T\to\mathbb{N}$ come una funzione che assegna ad una
  transizione un numero naturale. Si ha inoltre che:
  \begin{itemize}
    \item un multiset è detto \textbf{concorrentemente abilitato} in
    $M\in[M_0>$, quindi con $M$ marcatura raggiungibile. Detto in maniera
    diversa $U$ è un passo $M[U>$. Un multiset è concorrentemente abilitato in
    $M$ sse:
    \[\forall s\in S,\,\,\sum_{t\in T}U(t)\cdot W(s,t)\leq M(s)\,\,\wedge\,\,
      M(s)+\sum_{t\in T}U(t)\cdot W(t,s)\leq K\]
    ovvero per ogni posto si devono avere un numero di marche nella marcatura
    iniziale superiore al numero che posso togliere mediante le varie
    transizioni nel multiset $U$. Inoltre le marche che devo aggiungere più il
    numero di marche già presenti nel posto in output deve essere inferiore alla
    capacità dell'output.\\
    Questa condizione può essere riscritta come:
    \[\sum_{t\in T}U(t)\cdot W(s,t)\leq M(s)\leq K(s)-\sum_{t\in T}U(t)\cdot
      W(t,s)\]
    \item un multiset $U$ abilitato in $M$ può occorrere generando $M'$, avendo
    quindi:
    \[M[U>M'\]
    sse:
    \[\forall s\in S\,\,M'(s)=M(s)-\sum_{t\in T}U(t)\cdot W(s,t)+\sum_{t\in
        T}U(t)\cdot W(t,s)\]
    quindi la marcatura $M'$ in un certo posto è uguale alla marcatura del posto
    presente prima del passo al più dei contributi, positivi e negativi, delle
    varie transizioni (positivi se si ha tale posto come output e negativi se lo
    si ha come input)
    \item $U_\Sigma$ è l'\textbf{insieme dei passi} di $\Sigma$ e viene definito
    come:
    \[U_\Sigma=\{U:T\to \mathbb{N}|\,\,\exists M,M'\in [M_0>:\,\,M[U>M'\}\]
    ovvero l'insieme dei passi possibili tali per cui passo da una marcatura
    raggiungibile ad un'altra attivando il passo
  \end{itemize}
\end{definizione}
\begin{definizione}
  Dato un sistema P/T $\Sigma=(S, T , F , K , W;M_0)$, tale che $\forall s\in S
  K(s)<\infty$, si definisce il \textbf{grafo di raggiungibilità}, indicato con
  $RG(\Sigma)$, costruito solo con posti di capacità finita in quanto avere
  capacità infinite comporterebbe avere un grafo infinito, come la quadrupla:
  \[RG(\Sigma)=([M_0 >, U_\Sigma , A, M_0 )\]
  dove, per rappresentare questo sistema di transizioni, si ha:
  \begin{itemize}
    \item $[M_0>$ che rappresenta le marcature raggiungibili dallo stato iniziale
    del sistema $\Sigma$. Sono così rappresentati gli stati del sistema di
    transizioni  
    \item $U_\Sigma$ rappresenta l'insieme dei passi che rappresenta l'alfabeto
    delle etichette degli archi
    del sistema di transizioni
    \item $A=\{(M,U,M'):\,M,M'\in [M_0>\,\wedge\, U\in U_\Sigma \,\wedge\,
    M[U>M'\}$ dove, quindi dalla marcatura $M$ ho un arco, etichettato $U$, alla
    marcatura $M'$ sse le due marcature sono raggiungibili dallo stato iniziale,
    ed esiste un passo, nell'insieme dei passi, abilitato in $M$ tale per cui
    posso passare dalla marcatura $M$ alla marcatura $M'$. È quindi la regola
    che definisce come connettere mediante archi i vari stati
    \item $M_0$ che rappresenta la marcatura iniziale del sistema nonché lo
    stato iniziale del sistema di transizioni
  \end{itemize}
  Se $U$ è una singola transizione si ha il grafo di raggiungibilità
  sequenziale, detto $SGR(\Sigma)$.\\
  \textbf{La diamond property non è più valida in questa classe di reti a causa
    dei \emph{self loop} che posso avere tra una transizione e un posto}
\end{definizione}
\begin{esempio}
  Si prenda il sistema P/T $\Sigma$:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/pt31.jpg} 
  \end{figure}
  \newpage
  Si ha che lo scatto di $t_1$ e lo scatto di $t_2$ non può essere contemporaneo
  in quanto si richiederebbero due marche nel posto $s$, devono quindi scattare
  in sequenza. SI ottiene che il grafo dei casi raggiungibili è uguale a quello
  dei casi raggiungibili sequenziale e quindi non vale la diamond property (non
  potendo aggiungere alcun arco tra $\{s_1,s,s_2\}$ e $\{s\}$):
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/pt32.jpg}
  \end{figure}
\end{esempio}
\begin{definizione}
  Dato un sistema P/T si ha che una transizione $t$ è in una \textbf{situazione
    di contatto} nella marcatura $M$ sse:
  \[\forall s\in S, W(s,t)\leq M(s)\,\wedge M(s)+W(t,s)>K(s)\]
  ovvero una transizione è in una situazione di contatto in una certa marcatura
  $M$ sse ha abbastanza marche nei posti in input ma non abbastnza nei posti in
  output. Formalmente si ha che l'arco tra il posto in input e la transizioni ha
  peso inferiore al numero di marche dell'input e la marcatura del posto output
  più le marche che vengono aggiunte è maggiore della capacità del posto di
  output stesso .\\
  Un esempio:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.65]{img/pt33.jpg}
    \caption{Esempio di situazione di contatto}
  \end{figure}
\end{definizione}
\newpage
Si cerca di capire se è possibile trasformare la rete da una con contatti ad una
senza modificarne il comportamento. Questo è possibile con la
\textbf{complementazione}. Preso un posto $s$ se ne fa il complemento
$\overline{s}$, ovvero si fa un altro posto che ha archi in output verso
transizioni che sono di input a quelle del posto considerato e viceversa per gli
archi in input. Il complemento è quindi collegato alle stesse transizioni del
posto considerato ma con archi direzionati all'inverso. Per quanto riguarda il
numero di marche del complemento bisogna avere che il numero di marche del posto
e del suo complemento siano di somma pari alla capacità del posto (che è pari a
quella del complemento), ovvero:
\[M_0(\overline{s})=K(s)-M_0(s)\]
Si può inoltre dimostrare che per ogni marcatura raggiungibile $M$ la somma
delle marcature tra un posto e il suo complemento è sempre la medesima, comunque
evolva il sistema:
\[M(\overline{s})+M(s)=K(s)\]
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.65]{img/pt35.jpg}
\end{figure}
\begin{definizione}
  Dato un sistema P/T $\Sigma=(S, T , F , K , W;M_0)$ è \textbf{senza contatti}
  sse:
  \[\forall M\in[M_0>,\,\forall t\in T,\forall s\in S\]
  si ha:
  \[M(s)\geq W(s,t)\Rightarrow M(s)+W(t,s)\leq K(s)\]
  ovvero ogni volta che ho abbastanza marche in un posto di input ad una
  transizione allora sicuramente non ne ho troppe nel posto di output
\end{definizione}
\begin{definizione}
  Dato un sistema P/T $\Sigma=(S, T , F , K , W;M_0)$ senza contatti allora si
  ha che una transizione $t\in T$ è abilitata in $M\in[M_0>$, che si indica
  con $M[t>$, sse: 
  \[\forall s\in S,\,\,M(s)\geq W(s,t)\]
  ovvero sse nei posti in input ho abbastanza marche (trascurando di studiare
  i posti in output essendo una rete senza contatti).\\
  Si ha quindi che in assenza di contatti la capacità dei posti non gioca più
  alcun ruolo nella regola di scatto in quanto controllo solo di avere
  sufficienti marche in input per far scattare una transizione
\end{definizione}
\subsection{Reti Marcate}
Si cerca di identificare i sistemi elementari come casi particolari dei sistemi
P/T.
\begin{definizione}
  Dato un sistema P/T $\Sigma=(S, T , F , K , W;M_0)$ si ha che $\Sigma$ è una
  \textbf{Rete Marcata} sse:
  \[\forall s\in S,\,\,M_0(s)\in\mathbb{N}\,\wedge \,K(s)=\infty
    \,\wedge\,\forall t\in T,\,\,W(s,t)\leq 1\,\wedge\, W(t,s)\leq 1 \]
  ovvero per ogni posto la marcatura iniziale assegna un valore finito al posto
  che però ha capacità infinita (che quindi non gioca nessun ruolo, non
  vincolando lo scatto della transizione). Inoltre il peso di ogni arco deve
  essere minore o uguale a uno, imponendo quindi che abbia o peso uno o peso
  nullo (che segnala l'assenza dell'arco).\\
  Questa sottoclasse delle reti P/T può essere denotata con la sola quadrupla:
  \[\Sigma=(S,T,F;M_0)\]
  in quanto la capacità, denotata dalla funzione $K$, e il peso degli archi,
  denotato dalla funzione $W$, diventano ridondanti ed eliminabili dallo studio
  della rete.
\end{definizione}
\begin{definizione}
  Una rete marcata è definita \textbf{safe (sicura)} sse per ogni marcatura
  raggiungibile, compresa quella iniziale, ogni posto contiene al massimo una
  marca (tornando quindi ad una definizione simil booleana dello stato del posto
  che o è vuoto o contiene una marca). Formalmente si ha che una rete marcata è
  safe sse: 
  \[\forall M\in[M_0>,\,\forall s\in S:\,\,M(s)\leq 1\]
  \textbf{In una rete marcata i self-loop possono essere abilitati}:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.75]{img/pt36.jpg}
    \caption{Esempio di loop (o cappio) su una rete marcata}
  \end{figure}
  Si ricorda che in una rete elementare in ogni caso il self-loop non è
  abilitato e questa è una delle differenze principale tra un sistema elementare
  e una rete marcata, differenza alle quali si aggiungono anche il fatto che in
  una rete marcata ho capacità infinita e non pari a uno e marcature di un posto
  intere e non booleane. Sia nei sistemi elementari che nelle reti marcate ho
  solo archi di peso 1. \\
  Si ha di conseguenza che un \emph{sistema elementare puro}, ovvero un sistema
  elementare che non presenta mai cappi, \emph{coincide}, $\cong$, con una
  \emph{rete marcata pura e safe}, quindi senza cappi e con al più una marca per
  posto (quindi o una marca o zero marche).
  \\
  \textit{Poter vedere un sistema elementare come sottoclasse delle reti P/T
    permette varie possibilità dal punto di vista pratico.}
\end{definizione}
\subsection{Proprietà di Comportamento}
Si considerano sistemi P/T del tipo $\Sigma=(S,T,F,K,W;M_0)$ tali che $\forall
s\in S$ si ha $K(s)=\infty$ quindi con capacità dei posti illimitata.
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.35]{img/ptc.jpg}
  \caption{Esempio di sistema P/T}
\end{figure}
Abbiamo già visto la teoria dietro il grafo di raggiungibilità ma a questa vanno
aggiunte le tecniche di rappresentazione. La differenza con i grafi di sistemi
elementari si ritrova nel fatto che le marcature possono essere rappresentate da
un vettore colonna, con tanti elementi quanti i posti della rete, contenente
nell'i-sima posizione il numero di marche dell'i-simo posto in una data 
marcatura.\\
Solitamente il grafo di raggiungibilità viene usato per indagare le proprietà di
comportamento.
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.65]{img/ptc2.jpg}
  \caption{Grafo di raggiungibilità del sistema precedente, si nota, per
    esempio, che con $(0,1,2)$ si indica che in quello stato
    $M(s_1)=0$,$M(s_2)=1$ e $M(s_3)=2$ } 
\end{figure}
\subsubsection{Limitatezza}
Vediamo innanzitutto un problema famoso, detto \textbf{problema di
  raggiungibilità}.\\
In tale problema si ha una rete e due marcature, $M$ e $M'$, e ci si domanda se
dalla prima marcatura, raggiungibile dalla marcatura iniziale $M_0$, si può
raggiungere la seconda. Formalmente si ha che:
\[RP= \{\langle(S, T , F , W ), M, M'\rangle | (S, T , F , W ) \mbox{ è una rete
    P/T},\,M,M':S\to \mathbb{N} \mbox{ e } M'\in[M>\}\]
Questo problema è stato dimostrato decidibile da Mayr, anche se esponenziale
nello spazio. È un problema hard.\\
Si ha quindi l'importanza del fatto che il grafo delle marcature sia limitato e
viene quindi studiato il \textbf{problema della limitatezza}.
\begin{definizione}
  Sia $\Sigma=(S,T,F,K,W;M_0)$ un sistema P/T con $K(s)=\infty,\forall s\in
  S$. Sia inoltre definito $n\in\mathbb{N},\,\,n\geq 1$. Si ha quindi che il
  sistema in analisi è:
  \begin{itemize}
    \item \textit{\textbf{n-bounded} (n-limitato)} sse $\forall s\in S$ e
    $\forall M\in 
    M_0[>$ si ha:
    \[M(s)\leq n\]
    ovvero sse per ogni posto e per ogni marcatura raggiungibile da quella
    iniziale non succede che su tale posto si accumulino più di $n$ marche
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.55]{img/ptc4.jpg}
      \caption{Esempio di sistema \emph{2-bounded} (ma non 1-bounded)}
    \end{figure}
    \item \textit{\textbf{bounded} (limitato)} sse $\exists n\in\mathbb{N}$ tale
    per cui $\forall s\in S$ e $\forall M\in M_0[>$ si ha:
    \[M(s)\leq n\]
    ovvero se esiste un limite per il numero di marche
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.55]{img/ptc5.jpg}
      \caption{Esempio di sistema \emph{non bounded}, si andrà in
        \emph{overflow}} 
    \end{figure}
    Si ha di conseguenza una proposizione:
    se $Sigma$ è \textbf{limitato} allora il numero delle marcature raggiungibili
    dalla marcatura iniziale è un insieme \textbf{finito}, ovvero $[M_0>$ è un
    insieme finito, e quindi il grafo di raggiungibilità (il grafo delle marcature)
    è finito sia nel caso sia standard che in quello sia sequenziale:
    \begin{itemize}
      \item $SG(\Sigma)$ è finito
      \item $SGR(\Sigma)$ è finito
    \end{itemize}
    \item \textit{\textbf{safe} (1-safe, sicuro)} sse $\forall s\in S$ e
    $\forall M\in 
    M_0[>$ si ha:
    \[M(s)\leq 1\]
    ovvero ho al più una marca per posto comunque evolva il sistema  
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.55]{img/ptc3.jpg}
      \caption{Esempio di sistema \emph{safe} (ovvero 1-bounded)}
    \end{figure}
    Un sistema safe con $n$ posti ha al più $2^n$ marcature raggiungibili
  \end{itemize}
\end{definizione}
\subsubsection{Terminazione}
Un'altra proprietà interessante è quella della \textbf{terminazione}, in quanto
è spesso necessario che un sistema sequenziale termini (a differenza di uno
concorrente che spesso non ha termine).
\begin{definizione}
  Sia $\Sigma=(S,T,F,K,W;M_0)$ un sistema P/T. Si ha che:
  \begin{itemize}
    \item $\Sigma$ è detto \textbf{terminante} sse non ammette sequenze
    infinite.\\
    Inoltre $M\in[M_0>$ è una \textbf{marcatura di deadlock} sse $\forall t\in
    T$ non si ha $M[t>$, ovvero una marcatura in cui non è abilitata alcuna
    transizione. \\
    Ne segue che un sistema è terminante se arriva in una marcatura di
    deadlock.\\
    \item $\Sigma$ è \textbf{deadlock-free} sse $\forall M\in[M_0>$:
    \[\exists t\in T:\,\, M[t>\]
    ovvero sse non esiste una marcatura di deadlock raggiungibile da quella
    iniziale. Scritto diversamente:
    \[\not\exists M\in[M_0>:\,\,M\mbox{ è una marcatura di deadlock}\]
    \item $\Sigma$ è \textit{\textbf{1-live} (1-vivo)} sse $\forall t\in T$:
    \[\exists M\in [M_0>:\,\,M[t>\]
    ovvero sse ogni transizione può essere abilitata almeno una volta (ho sempre
    una marcatura raggiungibile che abilita una transizione).\\
    \textit{Non avere un sistema 1-live implica un errore nella modellazione
      dello stesso in quanto si ha una transizione mai abilitata superflua}
    \item $\Sigma$ è \textit{\textbf{live} (vivo)} sse $\forall t\in T$ e
    $\forall M\in[M_0>$:
    \[\exists M'\in[M>:\,\,M'[t>\]
    ovvero sse per ogni transizione e per ogni marcatura raggiungibile esiste
    sempre un'altra marcatura raggiungibile che abilita la transizione.\\
    \textit{Si ha che un sistema \emph{live} comporta che tale sistema sia anche
      \emph{deadlock-free} mentre il fatto che un sistema sia
      \emph{deadlock-free} non implica necessariamente che sia anche
      \emph{live}} 
  \end{itemize}
\end{definizione}
\newpage
\begin{esempio}
  Vediamo qualche esempio:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.55]{img/ptc6.jpg}
    \caption{Esempio di sistema \emph{terminante} (quindi non deadlock-free), in
      quanto scatta al più due volte}
    \includegraphics[scale = 0.55]{img/ptc7.jpg}
    \caption{Esempio di sistema \emph{deadlock-free}, \emph{non terminante} e
      \emph{1-live} ma non live, in quanto può continuare a cicalare, ogni
      transizione scatta almeno una volta ma la prima può scattare solo una
      volta e non può più essere attivata} 
    \includegraphics[scale = 0.55]{img/ptc8.jpg}
    \caption{Esempio di sistema \textit{live}, \emph{non terminante} e
      \emph{deadlock-free}, in quanto si ha un ciclo infinito lungo tutto il
      sistema} 
    \includegraphics[scale = 0.55]{img/ptc9.jpg}
    \caption{Esempio di sistema \emph{1-live} ma non \emph{live}, \emph{non
        terminante} e \emph{non deadlock-free}, in quanto potrebbe sia terminare
      che non terminare non potendo permettere la riattivazione di tutte le
      transizioni}  
  \end{figure}
\end{esempio}
\newpage
\subsubsection{Reversibilità}
Un'altra proprietà di comportamento è quella della \textbf{reversibilità}, detta
anche della \textbf{ciclicità}
\begin{definizione}
  Sia $\Sigma=(S,T,F,K,W;M_0)$ un sistema P/T. Si ha che $\Sigma$ è
  \textit{\textbf{reversible} (reversibile} sse:
  \[\forall M\in[M_0>:\,\,M_0\in[M>\]
  ovvero se da una marcatura raggiungile da quella iniziale posso tornare alla
  marcatura iniziale stessa.\\
  \begin{figure}[H]
    \includegraphics[scale = 0.55]{img/ptc10.jpg}
    ~
    \includegraphics[scale = 0.55]{img/ptc1.jpg}
    \caption{A sinistra un esempio di sistema \emph{reversible} e a destra uno
      \emph{non reversible}} 
  \end{figure}
  \textit{Si ha che avere un sistema \emph{reversible} e \emph{1-live} implica
    avere un sistema \emph{live} ma non si ha per forza il contrario}
\end{definizione}
\subsubsection{Tecniche di Verifica delle Proprietà}
La prima tecnica usata per studiare le proprietà sopra descritte è quello
dell'analisi del grafo di raggiungibilità $RG(\Sigma)$ e di quello di
raggiungibilità sequenziale $(SGR(\Sigma))$.\\
Si ha innanzitutto che:
\textit{Se $RG(\Sigma)$ è\textbf{ finito}, allora esistono algoritmi per
  decidere le seguenti proprietà:}
\begin{itemize}
  \item un posto è \textit{safe, m-bounded, bounded}
  \item il sistema $\Sigma$ è \textit{safe, n-bounded, limitato}
  \item una transizione è \textit{\textbf{dead} (morta)},
  \textit{1-live},\textit{ live} (\textit{live} se per ogni marcatura
  raggiungibile ho un cammino che contiene tale transizione)
  \item il sistema $\Sigma$ va in \textit{deadlock}, è \textit{deadlock-free,
    1-live e live} 
  \item il sistema $\Sigma$ è \textit{reversible}
\end{itemize}
\newpage
Si hanno quindi due risultati interessati:
\begin{enumerate}
  \item \textit{un sistema $\Sigma$ \textbf{terminante}, ovvero che va in
    \textbf{deadlock}, implica che il grafo di raggiungibilità $RG(\Sigma)$ ha
    almeno un \textbf{nodo terminante}, ovvero un nodo da cui non escono archi,
    e viceversa}.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/ptc12a.jpg}
    \includegraphics[scale = 0.6]{img/ptc12b.jpg}
    \caption{Esempio di un sistema terminante, col suo grafo di raggiungibilità
      che presenta, appunto, un nodo terminante, il nodo etichettato con
      ``$0100$''}
  \end{figure}
  \item \textit{un sistema $\Sigma$ \textbf{reversible} implica che il grafo di
    raggiungibilità $RG(\Sigma)$, come del resto anche quello sequenziale
    $SRG(\Sigma)$, è \textbf{fortemente connesso}, e viceversa}.\\
  Si ricorda che un grafo orientato è fortemente connesso se per ogni coppia di
  nodi esiste un cammino orientato dal primo al secondo nodo.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/ptc13.jpg}
    \includegraphics[scale = 0.7]{img/ptc14.jpg}
    \caption{Esempio di sistema \emph{live}, \emph{1-safe} e \emph{non
        reversible} con il grafo di raggiungibilità, quindi, \emph{non
        fortemente connesso}} 
  \end{figure}
\end{enumerate}
\newpage
Oltre allo studio del grafo di raggiungibilità si hanno altre tecniche di
verifica delle proprietà, tra cui l'\textbf{analisi strutturale del grafo della
  rete} che sfrutta: 
\begin{itemize}
  \item \textbf{tecniche di algebra lineare}, che sfruttano la rappresentazione
  algebrica della rete, mediante:
  \begin{itemize}
    \item \textit{equazioni di stato} che descrivono la dinamica attraverso
    operazioni algebriche tra matrici
    \item \textit{S-invarianti} e \textit{T-invarianti}
  \end{itemize}
  \item \textbf{studio del grafo della rete, di particolari sottoinsiemi di
    nodi e altre caratteristiche}
  \item \textbf{condizioni necessarie e sufficienti per garantire proprietà di
    comportamento di particolari sottoclassi di reti} 
\end{itemize}
\subsubsection{Safety, Liveness e Fairness}
Le proprietà che specificano il comportamento dei \textbf{sistemi concorrenti
  e/o distribuiti} (detti anche \textbf{reattivi}) sono classificate in tre
principali categorie, a seconda del tipo di comportamento che descrivono:
\begin{enumerate}
  \item \textbf{proprietà di safety}, che descrivono proprietà che non devono
  mai accadere, dichiarano che ``mai accadranno comportamenti indesiderati''
  (ad esempio: mai un semaforo avrà accese contemporaneamente le luci
  verde e rossa; mai due semafori ad un incrocio saranno contemporaneamente
  verdi). \\\textit{Nota: questa proprietà non ha nulla a che vedere con la
    \emph{1-safeness} (la \emph{safeness}) delle reti P/T}
  \item \textbf{proprietà di liveness}, che descrivono proprietà che devono
  essere verificate da tutte le esecuzioni, dichiarano che ``prima o poi un
  certo fatto deve accadere'' (ad esempio, prima o poi il semaforo diventa
  verde) 
  \item \textbf{proprietà di fairness}, che descrivono proprietà che
  ``descrivono fatti che devono accadere infinitamente spesso'' (ad esempio, la
  luce verde si accende infinitamente spesso)
\end{enumerate}
\subsection{Analisi Strutturale}
Spesso studiare il grafo di raggiungibilità comporta un costo esponenziale nel
numero dei posti della rete, le marcature sono almeno dell'ordine di
$2^{posti}$. Si passa quindi dallo studio del grafo di raggiungibilità allo
studio della rete stessa mediante la cosiddetta \textbf{analisi strutturale},
che studia il \textbf{grafo della rete}
\begin{definizione}
  Sia $\Sigma=(S,T,F;M_0)$ una \textbf{rete marcata}, reti con capacità
  dei posti illimitata e peso degli archi $\leq$ 1 (quindi o 0 o 1). Possiamo
  quindi studiare alcune proprietà strutturali che danno indicazioni sulle
  proprietà di comportamento:
  \begin{itemize}
    \item se un sistema $\Sigma$ è \emph{safe} e \emph{bounded} allora:
    \[\forall x\in S\cup T,\,\,^\bullet x\neq \emptyset \wedge x^\bullet \neq
      \emptyset\]
    ovvero per ogni nodo del grafo della rete non si ha mai che tale nodo abbia
    un insieme di pre-elementi vuoto (una transizione senza archi entranti
    sarebbe sempre abilitata permettendole di scattare infinite volte, rendendo
    il sistema \emph{non bounded}) e un insieme di post-elementi vuoto (una
    transizione senza archi uscenti, al suo scatto, non svuota le marche prese
    in input comportando il \emph{non bounded} del sistema). Analogamente si
    ragiona per i posti.
    \\ Questa è una \textbf{condizione} \textbf{necessaria}
    \item se un sistema $\Sigma$ è \emph{safe} e \emph{bounded} allora
    sicuramente il grafo della rete $(S,T,F)$ è \textbf{strettamente connesso}
    (si ricorda che in un grafo strettamente connesso, presi due nodi, esiste
    sempre un cammino tra essi)
    \item se un sistema $\Sigma$ è \emph{safe} e \emph{bounded} allora:
    \[\exists M\in[M_o>, \,\exists \sigma \in T^*:\,M[\sigma>M\]
    \[\mbox{tale che tutte le transizioni in }T\mbox{ occorrono in }\sigma\]
    ovvero esiste una marcatura raggiungibile ed esiste una sequenza di
    transizioni tale che a partire da quella marcatura, con quella sequenza, si
    torna nella stessa marcatura e in questa sequenza compaiono tutte le
    transizioni del mio sistema.\\
    \emph{Questa proprietà può essere analizzata sul grafo di raggiungibilità
      e non sul grafo della rete}
  \end{itemize}
  Si hanno anche proprietà strutturali legate al fatto che il grafo della rete
  $N=(S,T,F)$ sia connesso. Tale rete infatti è:
  \begin{itemize}
    \item \textbf{debolmente connessa} (detto anche solo \textbf{connessa}) sse:
    \[\forall x,y\in S\cup T,\,\,(x,y)\in(F\cup F^{-1})^*\]
    ovvero presi due nodi qualunque tra gli insiemi dei posti e delle
    transizioni esiste un cammino non orientato ($(F\cup F^{-1})^*$, dove lo
    star indica la chiusura transitiva) tra i due nodi
    \item \textbf{strettamente connessa} sse:
    \[\forall x,y\in S\cup T,\,\,(x,y)\in F^*\]
    ovvero presi due nodi qualunque tra gli insiemi dei posti e delle
    transizioni esiste un cammino orientato ($F^*$) tra i due nodi 
  \end{itemize}
  Inoltre si ha che:
  \begin{itemize}
    \item un \textbf{cammino semplice} di $N$ è una sequenza
    \[x_1f_1x_2f_2\ldots f_{n-1}x_n\]
    che non passa mai due volte per uno stesso nodo e per lo stesso arco,
    ovvero, formalmente: 
    \[x_i\in S\cup T \mbox{ e } f_i\in F\]
    \item un \textbf{ciclo semplice} è un cammino semplice tale che $x_1 = x_n$,
    ovvero il primo e l'ultimo nodo coincidono
    \item una rete $N = (S, T , F )$ è \textbf{coperta da cicli} sse, $\forall
    f\in F$, ovvero ogni arco, appartiene a qualche ciclo
  \end{itemize}
\end{definizione}
Si ha la seguente proposizione:
\begin{center}
  Data una rete $N=(S,T,F)$ si ha che essa è \textbf{strettamente connessa} sse
  $N$ è \textbf{debolmente connessa} e \textbf{coperta da cicli}
\end{center}
\newpage
Si hanno poi delle proprietà strutturali per reti senza marcature:
\begin{definizione}
  Data una rete $N=(P,T,F)$ senza marcature (con capacità illimitata e peso
  degli archi $\leq 1$) si ha che può essere definita:
  \begin{itemize}
    \item \textbf{strutturalmente limitata} se:
    \[\forall M_0:\,\,(P,T,F;M_0) \mbox{ è limitata}\]
    ovvero per ogni possibile marcatura iniziale il sistema è limitato
    \item \textbf{strutturalmente viva} se:
    \[\exists M_0:\,\,(P,T,F;M_0) \mbox{ è viva}\]
    ovvero se esiste almeno una marcatura iniziale tale per cui il sistema è
    vivo 
    \item \textbf{ben formata (\textit{WF, well formed})} se:
    \[\exists M_0:\,\,(N,M_0) \mbox{ è viva e limitata}\]
    ovvero esiste almeno una marcatura tale per cui il sistema risulta essere
    sia vivo che limitato
  \end{itemize}
\end{definizione}
\begin{esempio}
  Vediamo degli esempi per chiarire come la struttura della rete possa dare
  indicazioni sul comportamento.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.55]{img/as.jpg}
    \caption{Esempio di rete \emph{strutturalmente limitata} \emph{non
        strutturalmente viva} e \emph{non WF}, infatti $t_3$
      scatta fino a svuotare i due pre-posti ma dopo lo scatto di una delle due
      transizioni tra $t_1$ e $t_2$ impedisce lo scatto dell'altra, portando ad
      uno stato di \emph{deadlock}}
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.55]{img/as2.jpg}
    \caption{Esempio di rete \emph{non strutturalmente limitata},
      \emph{strutturalmente viva} e \emph{non WF}, infatti facendo scattare
      $t_1$ accumulo una marca in $s_1$ e una in $s_2$. Facendo poi scattare le
      altre due transizioni accumulo marche in $s_2$, in modo
      illimitato. D'altro canto la rete è \emph{viva} in quanto può sempre
      scattare una transizione} 
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.55]{img/as3.jpg}
    \caption{Esempio di rete \emph{strutturalmente limitata},
      \emph{strutturalmente viva}, \emph{WF} e \emph{non reversible}, infatti in
      ogni posto si ha sempre o una o zero marche e può sempre scattare almeno una
      transizione}
  \end{figure}
\end{esempio}
Si hanno anche altre proposizioni interessanti:
\begin{itemize}
  \item se una rete $N$ senza marcatura è \textbf{well formed} allora si ha che
  $N$ è \textbf{coperta di cicli}.\\
  \textit{È una condizione necessaria}
  \item \textit{ricordando che $N$ è \textbf{strettamente connessa} sse
    \textbf{debolmente connessa} e \textbf{coperta da cicli},} si ha che se $N$ è
  \textbf{well formed} e \textbf{debolmente connessa} allora $N$ è
  \textbf{strettamente connessa}.\\
  \textit{È una condizione necessaria}
\end{itemize}
\textit{quindi se una rete\emph{ debolmente connessa} non è anche
  \emph{strettamente connessa}, o \emph{coperta da cicli}, non esiste una
  marcatura iniziale tale che il sistema così ottenuto sia \emph{vivo} e
  \emph{limitato}}
\subsubsection{Rappresentazione Algebrica}
Per poter fare analisi strutturale sulla rete è utile avere una
\textbf{rappresentazione algebrica} del grafo della rete.
\begin{definizione}
  Sia $\Sigma = (S, T , F , K , W ; M_0 )$ un sistema P/T tale che $\forall s\in
  S,\,\,K(S)=\infty$ (quindi con capacità illimitata).\\
  $\Sigma$ può essere rappresentato da\textbf{ due matrici} con $|S|$ (numero
  dei posti) righe e $|T|$ (numero delle transizioni) colonne:
  \begin{enumerate}
    \item una \textbf{matrice backward}, $\underline{B}:S\times T\to
    \mathbb{N}$, che contiene in posizione $i,j$ il peso dell'arco che collega
    il posto $i$-simo alla transizione $j$-sima
    \item una \textbf{matrice forward}, $\underline{F}:S\times T\to \mathbb{N}$,
    che contiene in posizione $i,j$ il peso dell'arco che collega 
    la transizione $j$-sima al posto $i$-simo
  \end{enumerate}
  La marcatura $M_0$ può essere rappresentata da un vettore colonna $M_0$ di
  $|S|$ elementi.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/al.jpg}
    \caption{Esempio di una rete che comporta:
      $\underline{B}_{i,j}=W(s_i,t_j)=h$,  $\underline{F}_{i,j}=W(t_i,s_j)=k$ e
      $M_{0_i}=M_0(s_i)=2$} 
  \end{figure}
  Con questa rappresentazione doppia si può facilmente rappresentare una rete a
  livello computazionale
\end{definizione}
\newpage
Vediamo ora come rappresentare la regola di scatto usando le matrici.
\begin{definizione}
  Sia $M:S\to\mathbb{N}$ una marcatura e sia $t\in T$ una certa transizione. Si
  ha che la transizione è abilitata in tale marcatura ($M[t>$) sse:
  \[\forall s\in S,\,M(s)\geq W(s,t)\]
  (ovvero se il numero di marche in ogni posto che funge da pre-posto alla
  transizione è maggiore o uguale al peso ddell'arco che collega il posto alla
  transizione) \\
  ovvero sse:
  \[\underline{M}\geq \underline{B}(t)\]
  (ovvero nella matrice backward, nella colonna di tale transizione, ho pesi
  degli archi minori o uguali al numero di marche di ogni posto che precede tale
  transizione. Posso quindi confrontare la colonna che descrive la marcatura e
  la colonna nella matrice backward relativa alla transizione)\\
  Inoltre si ha che tale transizione, se abilitata mi porta nella marcatura $M'$
  ($M[t>M'$) sse:
  \[M[t > \wedge \,\,\forall s \in S, M'(s) = M(s) − W (s, t) + W (t, s)\]
  (ovvero ottengo la nuova marcatura partendo dalla marcatura precedente 
  sommando/sottraendo i contributi di archi entranti e uscenti)
  ovvero sse:
  \[M[t > \wedge\,\, \underline{M'} = \underline{M} − \underline{B}(t) +
    \underline{F} (t) = \underline{M} + \underline{F} (t) − \underline{B}(t)\]
  (ovvero usando le due matrici posso dire che se una transizione è abilitata
  allora lo scatto mi porta in una marcatura $M'$ calcolabile prendendo la
  colonna $M$, togliendo i valori della colonna di $t$ nella matrice backward e
  aggiungendo quelli della matrice forward (ovviamente nell'ordine che si
  preferisce essendo operazioni commutative)) 
\end{definizione}
\newpage
\begin{esempio}
  Vediamo un esempio più completo sulla rete:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/al2.jpg}
  \end{figure}
  dove si ha (i posti vuoti equivalgono a 0):
  \begin{itemize}
    \item la matrice backward:
    \begin{center}
      \begin{tabular}{|c|c|c|c|}
        \hline
        $\underline{B}$ & $t_1$ & $t_2$ & $t_3$ \\
        \hline 
        $s_1$ & 1 & & \\
        \hline
        $s_2$ & & 1 & \\
        \hline
        $s_3$ & & & 1 \\
        \hline
      \end{tabular}
    \end{center}
    \item la matrice forward:
    \begin{center}
      \begin{tabular}{|c|c|c|c|}
        \hline
        $\underline{F}$ & $t_1$ & $t_2$ & $t_3$ \\
        \hline 
        $s_1$ & 1 & & 1 \\
        \hline
        $s_2$ & &  & \\
        \hline
        $s_3$ & 1 & 2 & \\
        \hline
      \end{tabular}
    \end{center}
    \item la marcatura iniziale:
    \begin{center}
      \begin{tabular}{|c|c|}
        \hline
        & $M_0$\\
        \hline
        $s_1$ & \\
        \hline
        $s_2$ & 1 \\
        \hline
        $s_3$ & 2 \\
        \hline
      \end{tabular}
    \end{center}
    \item $M_0[t_2>M$:
    \[
      \begin{tabular}{|c|c|}
        \hline
        & $M$\\
        \hline
        $s_1$ & \\
        \hline
        $s_2$ &  \\
        \hline
        $s_3$ & 4 \\
        \hline
      \end{tabular}=
      \begin{tabular}{|c|c|}
        \hline
        & $M_0$\\
        \hline
        $s_1$ & \\
        \hline
        $s_2$ & 1 \\
        \hline
        $s_3$ & 2 \\
        \hline
      \end{tabular}-
      \begin{tabular}{|c|c|}
        \hline
        & $B_{t_2}$\\
        \hline
        $s_1$ & \\
        \hline
        $s_2$ & 1 \\
        \hline
        $s_3$ & \\
        \hline
      \end{tabular}+
      \begin{tabular}{|c|c|}
        \hline
        & $F_{t_2}$\\
        \hline
        $s_1$ & \\
        \hline
        $s_2$ & \\
        \hline
        $s_3$ & 2 \\
        \hline
      \end{tabular}
    \]
  \end{itemize}
\end{esempio}
\newpage
\begin{definizione}
  Sia $\Sigma = (S, T , F , K , W ; M_0 )$ un sistema P/T tale che: $\forall s
  \in S\,\, K (s) = \infty$ e $N= (S, T , F)$ sia \textbf{senza cappi}. Allora
  il sistema può essere rappresentato da un’unica matrice $\underline{N} : S
  \times T \to\mathbb{N}$ chiamata \textbf{matrice di incidenza}. Tale matrice
  ha $|S|$ righe e $|T|$ colonne e ha valore della posizione $i,j$ è data dalla
  differenza tra la matrice forward e quella backward in tale posizione:
  \[\underline{N_{i,j}}=\underline{F_{i,j}}-\underline{B_{i,j}}\]
  \begin{esempio}
    \textbf{Vedo quindi ogni transizione che effetto ha su un dato posto:}
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/al3.jpg}
    \end{figure}
    dove, per esempio, la transizione $t_1$ toglie una marca a $s_1$ (-1) e
    aggiunge una marca in $s_4$ (1).\\
    Vediamo anche il passaggio tra la marcatura iniziale $M_0$ (quella
    nellì'immagine sopra) e la marcatura $M_1$ (rappresentata nell'immagine
    seguente):
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/al4.jpg}
    \end{figure}
    Aggiungiamo quindi le colonne delle marcature:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/al5.jpg}
    \end{figure}
    In $M_0$ è abilitata $t_1$ quindi:
    \[M_0[t_1> \Longleftrightarrow \underline{M_0}+\underline{t_1}\geq 0
      \Longleftrightarrow  \underline{M_0}+\underline{N_{t_1}}\geq
      0\]
    e si ottiene che:
    \[M_0[t_1>M_1 \Longleftrightarrow
      \underline{M_0}+\underline{t_1}=\underline{M_1}\]
    ovvero il modo in cui calcolare $M_1$.\\
    Vediamo un altro esempio:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/al6.jpg}
    \end{figure}
    dove si vede, per esempio, che in $M_0$ la transizione $t_2$ non è
    abilitata, mentre, d'altro canto:
    \[M_0[t_1>M_1[t_2>M_2\Longleftrightarrow
      \underline{M_0}+\underline{t_1}+\underline{t_2}=\underline{M_2}\] 
  \end{esempio}
  A partire dall'ultimo esempio si osserva che la colonna della transizione
  $t_1$ la posso ottenere dalla matrice moltiplicando tale matrice per il
  \textbf{vettore caratteristico} di $t_1$, ovvero:
  \[\underline{t_1}=\underline{N_{t_1}}=\underline{N}\cdot
    \underline{c_{t_1}}\]
  dove:
  \[
    \underline{c}_{t_1}=\left[\begin{matrix}
        1\\
        0
      \end{matrix}\right]
  \]
  per cui si ha che:
  \[\underline{M_0}+\underline{N}\cdot \underline{c_{t_1}} =\underline{M_1}\]
\end{definizione}
\begin{definizione}
  Sia $\sigma \in T^*$ una sequenza di transizioni.\\
  Il \textbf{vettore di Parikh} (nome del ricercatore che lo ha introdotto) di
  $\sigma$ è il vettore colonna di $|T|$ elementi, $\underline{c_\sigma}$, tale
  che $\underline{c_\sigma}(\underline{t_i})$ è il numero di occorrenze di $t_i$
  in $\sigma$.
  \begin{esempio}
    Dato:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/al4.jpg}
    \end{figure}
    \    con:
    \begin{figure}[H]
      \centering
      \      \includegraphics[scale = 0.6]{img/al5.jpg}
    \end{figure}
    Si prenda per esempio $M_0[t_1t_3t_2t_3t_2>M_1$.\\
    Si ha che:
    \begin{itemize}
      \item $\sigma=t_1t_3t_2t_3t_2$
      \item $\underline{c_\sigma}=\left[\begin{matrix}
          2\\
          1\\
          2
        \end{matrix}\right]$
      ovvero ho due transizioni $t_1$, una $t_2$ e due $t_3$
    \end{itemize}
    Si ha quindi che:
    \[M_0[\sigma >M_1\Longrightarrow
      \underline{M_0}+\underline{N}\cdot\underline{c_\sigma}=\underline{M_1}\]
    Quest'ultima equazione è detta \textbf{equazione di stato} o \textbf{firing
      lemma} e descrive lo scatto di una sequenza, permettendo di simulare
    algebricamente il comportamento di una rete.
  \end{esempio}
  \textbf{La validità dell’equazione di stato è condizione necessaria, non
    sufficiente, affinché una sequenza di transizioni generi una marcatura in un
    sistema.} Infatti:
  \begin{itemize}
    \item se l’equazione non è soddisfatta, per un certo vettore di Parikh,
    allora non c’è una sequenza di transizioni, con quel vettore di Parikh, che
    faccia raggiungere quella marcatura
    \item se l’equazione è soddisfatta, con $\underline{c_\sigma}\geq 0$, non è
    detto che ci sia una sequenza $\sigma$ di transizioni tale che $M_0 [\sigma
    > M_1$ e in tal caso si dice che il vettore $\underline{c_\sigma}$
    \textbf{non è realizzabile}
  \end{itemize}
  \begin{esempio}
    Vediamo un esempio:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/al7.jpg}
      \caption{Esempio di una rete con la marcatura
        $\underline{M_0}=(0,0)^T$. Si ha che vale l'equazione
        $(0,0)^T+\underline{N}\cdot \underline{c_\sigma}$, con $\sigma =t_1t_2$,
        ma tale sequenza $\sigma$ non può scattare in $M_0$, quindi
        $\underline{c_\sigma}$ non è realizzabile}
    \end{figure}
  \end{esempio}
\end{definizione}
\subsection{Invarianti}
\emph{In questa sezione si considerano sistemi P/T $\Sigma=(S,T,F,W;M_0)$ con
  \textbf{capacità illimitata}, \textbf{senza cappi} (ovvero un pre-elemento non
  è anche, contemporaneamente, post-elementi) e con \textbf{matrice di
    incidenza} $\underline{N}$ (che grazie all'assenza di cappi permette una
  rappresentazione univoca del grafo della rete). La presenza di cappi può
  essere eliminata aggiungendo una transizione (per scomporre la vecchia
  transizione in due diverse) e un posto tra le due transizioni:}
\begin{figure}[H]
  \centering
  \includegraphics[scale = 0.5]{img/cap.jpg}
\end{figure}
Si continua lo studio, mediante l'algebra lineare, del grafo della rete, in
particolare mediante l'uso delle matrici di incidenza.
\newpage
\begin{definizione}
  Si definiscono \textbf{invarianti} delle componenti strutturali ottenute come
  soluzione di sistemi di equazioni lineari omogenei (sfruttando la
  rappresentazione matriciale della rete). Si hanno due tipi:
  \begin{itemize}
    \item \textbf{S-invarianti}, per i posti
    \item \textbf{T-invarianti}, per le transizioni
  \end{itemize}
\end{definizione}
\begin{definizione}
  Si dice che una marcatura $M\in[M_0>$ è \textbf{riproducibile} sse:
  \[\exists w\in T^*:M[w>M\]
  ovvero se esiste una sequenza di transizioni tale per cui $M$ sia abilitata in
  tale sequenza e tale per cui lo scatto mi riporta nella stessa marcatura $M$.
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/ti.jpg}
    \caption{Esempio dove $M_0$ non è riproducibile mentre $M=\{b\}$ e\\
      $M'=\{c\}$ sono riproducibili (facendo scattare $t_2t_3$ e $t_3t_2$
      rispettivamente)} 
  \end{figure}
\end{definizione}
\subsubsection{T-Invarianti}
Partiamo studiando i \textit{T-invarianti}.
\begin{definizione}
  Si parte dallo studio dell'equazione di stato e si ha che i
  \textbf{T-invarianti} indicano quante volte ogni transizione della rete deve
  scattare per poter riprodurre una certa marcatura.\\
  Per avere la riproducibilità, mediante la sequenza
  $w$, della marcatura $M$ si avrebbe:
  \[M[w>M\]
  che scritto sotto forma di \textbf{equazione di stato} diventa:
  \[\underline{M}+\underline{N}\cdot\underline{c_w}=\underline{M}\]
  con $\underline{c_w}$ che è il vettore di Parikh della sequenza $w$.\\
  Si nota subito che $\underline{N}\cdot\underline{c_w}$ deve essere il
  \textbf{vettore nullo} per permettere l'ottenimento del risultato
  $\underline{M}$ quindi $\underline{x}=\underline{c_w}$ è soluzione di:
  \[\underline{N}\cdot\underline{x}=\underline{0}\]
  e quindi $\underline{c_w}$ viene detto \textbf{T-invariante}
  \begin{esempio}
    preso un sistema con la sua matrice di adiacenza:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/ti.jpg}
      \centering
      \includegraphics[scale = 0.5]{img/t2.jpg}
    \end{figure}
    Si ha che $\underline{J^T}$ permette di avere $\underline{N}\cdot
    \underline{J}=\underline{0}$, dimostrando che $M[t_2t_2>M$ e
    $M'[t_3t_2>M'$ sono riproducibili
  \end{esempio}
\end{definizione}
Vediamo ora di formalizzare questa definizione:
\begin{definizione}
  Dato un sistema P/T $\Sigma=(S,T,F,W;M_0)$ con matrice di incidenza
  $\underline{N}$ si ha che:
  \begin{itemize}
    \item un vettore $\underline{J}:|T|\to\mathbb{Z}$ è un \textbf{T-invariante}
    sse $\underline{N}\cdot\underline{J}=\underline{0}$. Ovvero dato un vettore
    colonna di cardinalità pari al numero di transizioni della rete che assegna
    un valore intero è un T-invariante sse moltiplicato alla matrice di
    incidenza da il vettore nullo
    \item se $\underline{J_1}$ e $\underline{J_2}$ sono T-invarianti si ha che
    sono T-invarianti anche: 
    \begin{itemize}
      \item $\underline{J_1}+\underline{J_2}$
      \item $z\cdot\underline{J_1},\,\,\,z\in \mathbb{Z}$
    \end{itemize}
    ovvero \textbf{ogni combinazione lineare di T-invarianti è un T-invariante}
    \item un T-invariante $\underline{J}$ non negativo e non nullo
    ($\underline{J}\neq 0$) è detto \textbf{minimale} sse non esiste un altro
    T-invariante $\underline{J'}$ tale che:
    \[0\leq \underline{J'}\leq \underline{J}\]
    ovvero $\underline{J}$ è una \emph{base} a partire dalla quale posso
    ottenere tutti i possibili T-invarianti
  \end{itemize}
\end{definizione}
\begin{esempio}
  Vediamo un esempio pratico. Dato un sistema con la sua matrice di incidenza:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 2.2]{img/ti2.jpg}
  \end{figure}
  vedo come calcolare il T-invariante (con $y_i$ variabili di indice compreso
  tra 1 e $|T|$):
  \[\underline{J}=\left[
      \begin{matrix}
        y_1\\
        y_2\\
        y_3
      \end{matrix}
    \right]\]
  Procedo scrivendo il sistema di equazioni lineari omogenee corrispondente al
  prodotto riga per colonna tra la matrice di adiacenza e $\underline{J}$:
  \[
    \begin{cases}
      -y_1-y_2+y_3=0\\
      -y_1+y_2=0\\
      y_1-y_2=0\\
      y_1+y_2-y_3=0
    \end{cases}
  \]
  risolvendo si ottiene un numero illimitato di soluzioni dalle quali si
  estrae quella minimale, ottenendo la base:
  \[
    \begin{cases}
      y_3=2y_2\\
      y_1=y_2\\
      y_1=y_2=\\
      y_3=2y_1
    \end{cases}\to\underline{J}=\left[
      \begin{matrix}
        1\\
        1\\
        2
      \end{matrix}
    \right]
  \]
  si nota che la \textbf{somma pesata delle colonne} cosı̀ come indicato dal
  vettore $\underline{J}$ è uguale al vettore nullo. Nell'esempio sarebbe:
  \[
    \left[
      \begin{matrix}
        -1\\
        -1\\
        1\\
        1
      \end{matrix}
    \right]+\left[
      \begin{matrix}
        -1\\
        1\\
        -1\\
        1
      \end{matrix}
    \right]+2\cdot\left[
      \begin{matrix}
        1\\
        0\\
        0\\
        -1
      \end{matrix}
    \right]=\left[
      \begin{matrix}
        0\\
        0\\
        0\\
        0
      \end{matrix}
    \right]\]
\end{esempio}
\begin{definizione}
  Dato un sistema P/T $\Sigma=(S,T,F,W;M_0)$ con capacità illimitata e dato un
  T-invariante non negativo $\underline{J}$ si ha che la \textbf{sottorete} $N_J
  = (S_J , T_J , F_J , W_J)$ è la rappresentazione grafica di $\underline{J}$
  sse $N_J$ è la sottorete di $N$ generata dalle transizioni identificate da
  $\underline{J}$ dove:
  \begin{itemize}
    \item $T_J=\{t\in T|\,\underline{J}(t)\neq 0\}$, ovvero ha come transizioni
    quelle in cui il T-invariante è positivo (essendo non negativo e non nullo)
    \item $S_J=^\bulletT_J\cup T_J^\bullet$, ovvero ha come posti ha i pre-posti
    e i post-posti delle transizioni in $T_J$
    \item $F_J=F\cap[(S_J\times T_J)\cup(T_J\tims S_J)]$, ovvero gli archi sono
    quelli della rete di partenza costruiti però tra i nuovi posti e le nuove
    transizioni 
    \item $W_J=W_{|F_J|}$, ovvero il peso degli archi è identico a quello di
    partenza al più di dare peso sono agli archi presenti nella sottorete
  \end{itemize}
  \begin{esempio}
    preso il sistema:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/ti.jpg}
    \end{figure}
    e il T-invariante $J=[0\,\,\,1\,\,\,1]^T$ si ottiene la sottorete $N_J$:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.5]{img/ti3.jpg}
    \end{figure}
  \end{esempio}
\end{definizione}
\begin{esempio}
  Vediamo anche un esempio più complesso sulla mutua esclusione:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{img/ti4.jpg}
  \end{figure}
  dove anche la combinazione lineare (2 volte il primo più il secondo)
  $[2\,\,\,2\,\,\,1\,\,\,1]$ dei due T-invarianti è un T-invariante.
\end{esempio}

\begin{definizione}
  Dato un sistema P/T $\Sigma=(S,T,F,W;M_0)$ e dato un T-invariante non negativo
  $\underline{J}$ si ha che esso è un \textbf{T-invariante realizzabile} sse:
  \[\exists M\in[M_0>\mbox{ e }\exists w\in T^*:\,\,M[w>\land\,
    \underline{J}=\underline{c_w}\]
  ovvero sse esiste una marcatura $M$ raggiungibile da quella iniziale ed esiste
  una sequenza di transizioni tale per cui tale sequenza sia attivabile in $M$
  e tale per cui il T-invariante sia il vettore di Parikh. In altre parole sse:
  \[M[w>M\]
  \textbf{Non tutti i T-invarianti non negativi sono realizzabili} (per esempio
  nel caso in cui $M_0$ sia il vettore nullo o in diversi altri casi)
  \begin{esempio}
    Dato il sistema:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.5]{img/ti5.jpg}
    \end{figure}
    e il T-invariante $\underline{J}=[0\,\,\,0\,\,\,1\,\,\,1]^T$
    si ha che al variare delle marcature iniziali:
    \begin{itemize}
      \item se $M_0=[1\,\,\,0\,\,\,0\,\,\,0]^T$ allora $\underline{J}$ non è
      realizzabile
      \item se $M_0=[2\,\,\,0\,\,\,0\,\,\,0]^T$ allora $\underline{J}$ è
      realizzabile infatti $M=[0\,\,\,1\,\,\,1\,\,\,0]^T$ ̀ raggiungibile da $M_0$
      e $M$ è riproducibile tramite la sequenza $t_3t_4$
    \end{itemize}
  \end{esempio}
\end{definizione}
\begin{definizione}
  Dalla definizione di T-invariante segue che se $M\in[M_0>$ è una
  \textbf{marcatura riproducibile} di $\Sigma$, allora le transizioni per la
  riproduzione di $M$ sono  le transizioni di una\textbf{ rappresentazione
    grafica} di un T-invariante.
  Si ha inoltre la seguente proprietà:
  Sia $N = (S, T , F , W )$ senza marcatura una rete P/T e sia $\underline{J}$
  un suo T-invariante non negativo. Allora $\exists M:S\to\mathbb{N}$ tale $M$ è
  una \textbf{marcatura riproducibile} tramite $\underline{J}\in N$, cioè tale
  che: 
  \[\underline{M}+\underline{N}\cdot \underline{J}=\underline{M}\]
  Posso quindi stabilire una marcatura riproducibile per la quale si ottiene un
  T-invariante realizzabile
\end{definizione}
Si studia ora la relazione tra il concetto di T-invariante e la proprietà di
comportamento di una rete.
\begin{definizione}
  Una rete $N = (S, T , F , W )$ è detta \textbf{coperta da T-invarianti} sse:
  \[\forall t\in T,\,\exists \underline{J} \mbox{ T-invariante di
    }N:\,\,\underline{J}(t)>0\]
  ovvero per ogni transizione esiste almeno un T-invariante che assegna a quella
  transizione un valore positivo
\end{definizione}
\begin{teorema}
  Vediamo il teorema detto \textbf{condizione necessaria per la vivezza e
    limitatezza}.\\
  Si ha che se $\Sigma=(N;M_0)$ è \textbf{finito, vivo e limitato} allora $N$ è
  \textbf{coperta da T-invarianti}.\\
  \textit{\textbf{Non vale l'inverso}}
  \newpage
  \begin{esempio}
    Vediamo un esempio per dimostrare che non vale l'inverso:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/ti7.jpg}
      \caption{Esempio di rete viva, non limitata ma comunque coperta da
        T-invarianti} 
    \end{figure}
  \end{esempio}
  inoltre si ha che: \\
  \textbf{una rete viva è coperta da T-invarianti, ma non
    necessariamente vale il viceversa, infatti la rete data può andare in
    deadlock.}
  \begin{esempio}
    Vediamo un esempio per dimostrare l'affermazione:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.5]{img/ti9.jpg}
      \caption{Esempio di rete non viva, limitato ma comunque coperta da
        T-invarianti (le transizioni viola e blu formano dei
        T-invarianti). Nell'immagine si ha una marcatura di deadlock} 
    \end{figure}
  \end{esempio}
\end{teorema}
\begin{teorema}
  Sia $N=(P,T,F)$ (non marcata) una \textbf{rete ben formata} (per cui ammette
  una marcatura per cui è \emph{viva} e \emph{limitata}) allora sicuramente è
  \textbf{coperta da T-invarianti}. Anche questa è una \emph{condizione
    necessaria ma non sufficiente}.\\
  Si nota che lo studio dei T-invarianti è indipendente dalla marcatura in
  quanto vengono calcolati indipendentemente dalla marcatura ma in dipendenza
  del solo grafo della rete
\end{teorema}
\subsubsection{S-Invarianti}
Passiamo allo studio degli S-invarianti che pongono al centro del loro studio i
posti e non le transizioni.\\
\begin{definizione}
  Gli \textbf{S-invarianti} sono le componenti strutturali che individuano
  insiemi di posti tali che, comunque evolva il sistema in analisi, mantengono
  un numero di marche \textbf{costante}, marche che eventualmente possono essere
  opportunamente pesate.\\
  Nella pratica sommando le righe corrispondenti presunti valori di un insieme
  di posti che identifica un S-invariante si ottiene il vettore nullo.\\
  Un S-invariante viene quindi visualizzato da un vettore colonna con $1$ in
  corrispondenza dei posti che fanno parte dell'insieme che identifica
  l'S-invariante e $0$ altrimenti
  \begin{esempio}
    Preso il sistema, con la sua matrice di incidenza:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.4]{img/si.jpg} 
    \end{figure}
    \begin{center}
      \begin{tabular}{|c|c|c|}
        \hline
        & $t_1$ & $t_2$\\
        \hline
        $a$ & -1 &\\
        \hline
        $b$ & 1 & -1\\
        \hline
        $c$ & & 1\\
        \hline
        $d$ & -1 & \\
        \hline
        $e$ & & 1\\
        \hline
      \end{tabular}
    \end{center}
    Si nota, per esempio, che si avrà sempre e solo una marca tra i posti
    $a$, $b$ e $c$. Nella matrice possiamo prendere e sommare le 3 righe in
    corrispondenza dei 3 stati e sommarli:
    \[
      \left[
        \begin{matrix}
          -1\\
          0
        \end{matrix}
      \right]+\left[
        \begin{matrix}
          1\\
          -1
        \end{matrix}
      \right]+\left[
        \begin{matrix}
          0\\
          1
        \end{matrix}
      \right]=\left[
        \begin{matrix}
          0\\
          0
        \end{matrix}
      \right]
    \]
    Quindi l'effetto delle due transizioni su questo insieme di posti non
    modifica il numero di marche complessivo in esso contenuto. Si ha quindi che
    l'insieme $\{a,b,c\}$ è un S-invariante, come del resto lo sarebbe anche
    l'insieme $\{d,b,e\}$:
    \[SI_1=\left[
        \begin{matrix}
          1\\
          1\\
          1\\
          0\\
          0\\
        \end{matrix}
      \right]\,\,\,\,\,SI_2=\left[
        \begin{matrix}
          0\\
          1\\
          0\\
          1\\
          1\\
        \end{matrix}
      \right]
    \]
    ovviamente lo sono anche $\{a,b,e\}$ o anche $\{d,b,c\}$
  \end{esempio}
\end{definizione}
Vediamo ora di formalizzare meglio:
\begin{definizione}
  Preso $S'$ insieme di stati che definiscono un S-invariante ($S'\subseteq S$)
  e dato il numero di transizioni $|T|$ si ha che:
  \[\sum_{s\in S'}t_i(s)=0,\,\,\,i=1,\ldots,|T|\]
  Sia quindi $\underline{c}_{S'}$ il vettore caratteristico di $S'$ (che
  presenta 1 in corrispondenza delle righe degli stati in $S'$ e 0 altrimenti),
  si ha che:
  \[\sum_{s\in S}\underline{c}_{S'}^T\cdot
    \underline{t_i}(s)=0,\,\,\,i=1,\ldots,|T|\] 
  e quindi (elevazione $^T$ per indicare il trasposto):
  \[\underline{c}_{S'}^T\cdot \underline{t_i}=0,\,\,\,i=1,\ldots,|T|\]
  e se il numero di marche rimane costante per tutte le transizioni si ha che:
  \[\underline{c}_{S'}^T\cdot \underline{N}=\underline{0}\]
  Viceversa se ho un vettore $\underline{x}$, che presenta un numero di elementi
  pari al numero di posti del sistema in analisi, tale che:
  \[\underline{x}\cdot \underline{N}=\underline{0}\]
  e ho una soluzione in $\{0,1\}$ allora $\underline{x}$ è il \textbf{vettore
    caratteristico di un insieme di posti con marcatura costante} 

  \newpage
  Vediamo quindi un esempio:
  \begin{esempio}
    Dato un sistema con la sua matrice di incidenza:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/si2.jpg} 
    \end{figure}
    risolviamo il sistema di equazioni lineari omogeneo:
    \[
      \underline{x}\cdot \underline{N}=0\to\begin{cases}
        -x_1-x_2+x_3+x_4=0\\
        -x_1+x_2-x_3+x_4=0\\
        x_1-x_4=0
      \end{cases}\to
      \begin{cases}
        x_1=x_4\\
        x_2=x_3
      \end{cases}
    \]
    Si hanno quindi infinite soluzioni, tra cui, per esempio,
    $[0\,\,\,1\,\,\,1\,\,\,0]^T$ e $[1\,\,\,0\,\,\,0\,\,\,1]^T$, che sono
    S-invarianti (e sono basi)
  \end{esempio}
  Quindi, dato un sistema P/T $\Sigma=(S,T,F,W:M_0)$ senza cappi e con matrice
  di incidenza $N$, possiamo dare la definizione formale di S-invariante:
  \begin{itemize}
    \item un vettore $\underline{I}:S\to \mathbb{Z}$ è un S-invariante sse:
    \[\underline{I}\cdot \underline{N}=\underline{0}\]
    \item dati due invarianti $\underline{I_1}$ e $\underline{I_2}$ si ha che
    sono S-invarianti anche:
    \begin{itemize}
      \item $\underline{I_1}+\underline{I_2}$
      \item $z\cdot \underline{I_1},\,z\in \mathbb{Z}$
    \end{itemize}
    Si ha quindi che \textbf{ogni combinazione lineare di S-invarianti è un
      S-invariante}  
    \item un S-invariante non negativo $\underline{I}\neq \underline{0}$ è
    \textbf{minimale} sse non esiste un altro S-invariante non-negativo
    $\underline{I'}$ tale per cui:
    \[\underline{0}\leq \underline{I'}\leq \underline{I}\]
    ovvero $\underline{I}$ è una base a partire dalla quale posso ottenere tutti
    i possibili S-invarianti
  \end{itemize}
\end{definizione}
\begin{esempio}
  Vediamo quindi anche un esempio in cui non si ha una soluzione
  in $\{0,1\}$.\\
  Sia dato il seguente sistema, con la sua matrice di incidenza:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/si3.jpg}
    \includegraphics[scale = 0.5]{img/si4.jpg}
  \end{figure}
  Si calcola la soluzione di $\underline{x}\cdot \underline{N}=\underline{0}$:
  \[
    \begin{cases}
      -2x_1+x_2=0\\
      -x_2+2x_3=0
    \end{cases}\to
    \begin{cases}
      x_2=2x_1\\
      x_2=2x_3
    \end{cases}
  \]
  ottenendo, per esempio, la soluzione di base:
  \[
    \underline{I}=\left[
      \begin{matrix}
        1\\
        2\\
        1
      \end{matrix}
    \right]
  \]
  Quindi ho ottenuto un S-invariante che \textbf{non è un vettore
    caratteristico}. Bisogna quindi interpretare questo risultato dicendo che
  una marca in $b$ \textbf{conta/pesa} quanto due marche in $a$ o due marche in
  $c$.  
\end{esempio}
Bisogna quindi studiare formalmente per questa situazione.
\begin{teorema}
  Questo teorema viene detto \textbf{primo teorema principale della
    conservazione delle marche}.\\ 
  Dato un sistema P/T $\Sigma=(S,T,F,W;M_0)$ senza cappi e con matrice
  di incidenza $N$ si ha che, $\forall M\in[M_0>$ e $\forall\underline{I}$ tale
  per cui $\underline{I}\cdot \underline{N}=\underline{0}$ (quindi per ogni
  S-invariante):
  \[\underline{I}\cdot \underline{M}=\underline{I}\cdot \underline{M_0}\]
  si ha quindi che la somma delle marche rimane costante tra le marcature
\end{teorema}
\begin{proof}
  Si parte dall'equazione di stato:
  \[\underline{M_0}+\underline{N}\cdot \underline{c_\sigma}=\underline{M}\]
  si moltiplica da entrambe le parti per $\underline{I}$, ottenendo:
  \[\underline{I}\cdot\underline{M_0}+\underline{I}\cdot\underline{N}\cdot
    \underline{c_\sigma}=\underline{I}\cdot\underline{M}\]
  ed essendo $\underline{I}$ S-invariante segue la tesi in quanto
  $\underline{I}\cdot \underline{N}=\underline{0}$ comportando:
  \[\underline{I}\cdot \underline{M}+\underline{0}=\underline{I}\cdot
    \underline{M_0}\] 
  \[\Downarrow\]
  \[\underline{I}\cdot \underline{M}=\underline{I}\cdot \underline{M_0}\]  
\end{proof}
Questo teorema viene usato per discutere la \textbf{raggiungibilità delle
  marcature}, infatti se si ha che, date due marcature $M$ e $M'$,
$\underline{I}\cdot \underline{M}\neq\underline{I}\cdot \underline{M'}$ allora
si può dedurre che $M'\not\in M[>$.\\
Inoltre, se si ha una marcatura $M'$ solo parzialmente conosciuta allora
l'equazione $\underline{I}\cdot \underline{M}=\underline{I}\cdot \underline{M'}$
fornisce una \textbf{condizione necessaria} per poter completare la marcatura
parziale $M'$ in modo che essa sia raggiungibile dalla marcatura $M$.\\
\begin{teorema}
  Questo teorema viene detto \textbf{secondo teorema principale della
    conservazione delle marche} e garantisce che l'inverso del \textbf{primo
    teorema} è vero solo se ogni transizione può scattare almeno una volta a
  partire da $M_0$, cioè se il sistema è \textbf{1-vivo}.\\ 
  Sia dato un sistema P/T $\Sigma=(S,T,F,W;M_0)$ \textbf{1-vivo} e sia
  $\underline{I}:S\to\mathbb{Z}$ tale $\forall M\in[M_0>: \underline{I}\cdot
  \underline{M}=\underline{I}\cdot \underline{M_0}$ allora:
  \[\underline{I}\cdot \underline{N}=\underline{0}\]
  ovvero $\underline{I}$ è un S-invariante.\\
  Quindi se vale il primo teorema non è detto che $\underline{I}$ sia un
  S-invariante ma per esserlo devo avere un sistema 1-vivo.
  \newpage
  \begin{esempio}
    Vediamo, infatti, un esempio con un sistema non 1-vivo (con la sua matrice
    di incidenza):
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.8]{img/si5.jpg}
      \includegraphics[scale = 0.8]{img/si6.jpg}
    \end{figure}
    Si ha che $\underline{I}=[1\,\,\,1\,\,\,1]$ è tale che, $\forall M\in[M_0>$:
    \[\underline{I}\cdot \underline{M}=\underline{I}\cdot\underline{M_0}=1\]
    ma $\underline{I}=[1\,\,\,1\,\,\,1]$ non è soluzione di $\underline{I}\cdot
    \underline{N}=\underline{0}$, infatti:
    \[[1\,\,\,1\,\,\,1]\cdot\underline{N}=[0\,\,\,0\,\,\,-2]\]
    e quindi $\underline{I}$ non è un S-invariante
  \end{esempio}
\end{teorema}
Definiamo ora formalmente la rappresentazione grafica di un S-invariante:
\begin{definizione}
  Sia dato un sistema P/T $\Sigma=(S,T,F,W;M_0)$ e sia $\underline{I}$ un
  S-invariante allora la rappresentazione grafica di $\underline{I}$ è la
  sottorete:
  \[N_I=(S_I,T_I,F_I,W_I)\]
  generata dai posti identificati da $\underline{I}$, ovvero tale che:
  \begin{itemize}
    \item $S_I=\{s\in S|\,\underline{I}(s)\neq 0\}$, ovvero gli stati della rete
    sono gli stati per i quali l'S-invariante assegna valore non nullo
    \item $T_I=\,^\bullet S_I\cup S_I^\bullet$, ovvero le transizioni sono le
    pre-transizioni e le post-transizioni di dei posti in $S_I$
    \item $F_I=F\cap [(S_I\times T_I)\cup (T_I\times S_I)]$, ovvero gli archi
    sono quelli che connettono posti e transizioni appena definiti nel sistema
    originale 
    \item $W_I=W_{|F_I|}$, ovvero il peso degli è il medesimo che avrebbero nel
    sistema originale
  \end{itemize}
  \newpage
  \begin{esempio}
    Per esempio, dato il sistema:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/si7.jpg}
    \end{figure}
    con l'S-invariante:
    \[\underline{I}=[1\,\,\,0\,\,\,0\,\,\,1]\]
    si ottiene la sottorete:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.6]{img/si8.jpg}
    \end{figure}
    che è la rappresentazione grafica di $\underline{I}$
  \end{esempio}
\end{definizione}
Anche per gli S-invarianti vediamo un esempio complesso:
\begin{esempio}
  Prendiamo il sistema tipico che modella la mutua esclusione di due processi
  ciclici che condividono una risorsa: 
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{img/si9.jpg}
  \end{figure}
  \newpage
  con la matrice di incidenza e i tre S-invarianti di base:
   \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{img/si10.jpg}
  \end{figure}
  Il primo e il secondo S-invariante rappresentano rispettivamente le due
  sottoreti dei due processi che ciclano mentre la sottorete del terzo
  invariante rappresenta l'uso della risorsa tra i due processi. Si possono fare
  diverse considerazioni, sempre considerando $\forallM\in[M_0>$,
  $\forall\underline{I}$ S-invariante e
  $\underline{I}\cdot\underline{M}=\underline{I}\cdot \underline{M_o}$:
  \begin{itemize}
    \item dai tre S-invarianti si deduce che il sistema è \textbf{limitato},
    infatti, per esempio, da
    $\underline{I_1}\cdot\underline{M}=\underline{I_1}\cdot\underline{M_0}$ si
    ottiene:
    \[M(s_1)+M(s_2)=M_0(s_1)+M_0(s_2)=1\]
    ovvero in $s_1$ e $s_2$ c'è sempre al più una marca (o è marcato uno o
    l'altro) e quindi $s_1$ e $s_2$ sono limitati. Espandendo lo stesso
    ragionamento anche agli altri due S-invarianti si ottiene che la
      rete è \textbf{coperta da S-invarianti} e quindi il sistema è limitato e
      non si ha accumulo di marche in alcun posto
    \item da $\underline{I_3}$ si deduce la mutua esclusione, infatti:
    \[M(s_2)+M(s_3)+M(s_4)=M_0(s_2)+M_0(s_3)+M_0(s_4)=1\]
    quindi se uno dei tre stati è marcato sicuramente non lo saranno gli altri
    due, in quanto la somma delle loro marche sarà sempre pari ad uno
    \item da $\underline{I_3}-\underline{I_2}=[0\,\,\,1\,\,\,1\,\,\,0\,\,\,-1]$,
    si deduce che se $s_2$ è marcato allora lo è anche $s_5$ (idem per $s_3$
    sempre rispetto ad $s_5$), infatti:
    \[M(s_2)+M(s_3)-M(s_5)=M_0(s_2)+M_0(s_3)-M_0(s_5)=0\]
    e quindi posso dire che:
    \[M(s_2)+M(s_3)=M(s_5)\]
    ovviamente se $s_5$ è marcato non posso avere sia $s_2$ che $s_4$ marcati
  \end{itemize}
\end{esempio}
\newpage
\begin{definizione}
  Una rete $N = (S, T , F , W )$ è\textbf{ coperta da S-invarianti} sse:
  \[\forall s\in S,\,\,\,\exists\, \underline{I} \mbox{ S-invariante tale che }
    \underline{I}(s)>0\] 
\end{definizione}
\begin{teorema}
  Si ha il \textbf{teorema della condizione sufficiente per la limitatezza}.\\
  Dato un sistema P/T $\Sigma=(S,T,F,W;M_0)$ se $(S,T,F,W)$ è \textbf{coperto da
    S-invarianti} allora il sistema $\Sigma$ è \textbf{strutturalmente limitato}
  (ovvero per qualsiasi marcatura iniziale venga fornita nessun posto accumulerà
  più di un certo numero di marche).\\
  L’inverso non è vero in generale, possono esistere infatti sistemi limitati o
  strutturalmente limitati che non sono coperti da S-invarianti, è quindi una
  condizione \textbf{sufficiente ma non necessaria}.
\end{teorema}
\begin{teorema}
  Si ha il \textbf{teorema della condizione necessaria per la vivezza}.\\
  Dato un sistema P/T $\Sigma=(S,T,F,W;M_0)$ se esso è \textbf{vivo} allora
  necessariamente ogni S-invariante $\underline{I}$ non negativo deve essere
  marcato nella marcatura iniziale $M_0$, ovvero:
  \[\underline{I}\cdot \underline{M_0}>0\]
  in quanto altrimenti le transizioni in $^\bullet S_I\cup S_)^\bullet$
  sarebbero \textbf{morte}, le transizioni connesse a questi posti infatti non
  scatterebbero mai.
\end{teorema}
\begin{esempio}
  Vediamo qualche esempio:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/si11.jpg}
    \caption{Esempio con un sistema P/T è vivo, limitato (1-safe, avendo al
      massimo una marca per posto), ma con la rete non coperta da S-invarianti
      infatti $s_5$ non appartiene a nessun S-invariante (i due S-invarianti
      sono visualizzabili in blue e rosso)}
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.7]{img/si12.jpg}
    \caption{rete è coperta da 2 S-invarianti (i 3 posti blu a sinistra e i tre
      posti rossi a destra) entrambi monomarcati, il sistema è quindi limitato;
      non è invece vivo, può infatti andare in deadlock}
    \includegraphics[scale = 0.7]{img/si13.jpg}
    \caption{Possibile situazione di deadlock }
  \end{figure}
\end{esempio}
Completiamo ora quanto detto per i T-invarianti:
\begin{teorema}
  Sia $N = (P, T, F )$ (non marcata) essa è \textbf{ben formata (WF)} sse
  $\exists\,M_0:(N,M_0)$ è \textbf{viva e limitata}, quindi:
  \begin{itemize}
    \item $N = (P, T , F )$ è WF sse $N$ è 
  \end{itemize}
\end{teorema}














\chapter{Logica PLTL}
\section{Ripasso Logica Proposizionale}
\emph{Si ringrazia
  \MYhref{https://github.com/bigboss98/appunti/tree/master/Fondamenti}{Marco
  Natali} per questo ripasso}\\ 
La logica è lo studio del ragionamento e dell’argomentazione e, in particolare,
dei procedimenti inferenziali, rivolti a chiarire quali	procedimenti di pensiero
siano validi e quali no. Vi sono molteplici tipologie di logiche, come ad
esempio la logica classica e le logiche costruttive, tutte accomunate di essere
composte da 3 elementi: 
% Elementi di una Logica
\begin{itemize}
  \item \textbf{Linguaggio}: insieme di simboli utilizzati nella Logica per
  definire le cose 
  \item \textbf{Sintassi}: insieme di regole che determina quali elementi
  appartengono o meno al linguaggio 
  \item \textbf{Semantica}: permette di dare un significato alle formule del
  linguaggio e determinare se rappresentano o meno la verità.
\end{itemize}

Noi ci occupiamo della logica Classica che si compone in \textit{logica
  proposizionale} e \textit{logica predicativa}.

La logica proposizionale è un tipo di logica classica che presenta come
caratteristica quella di essere un linguaggio limitato in quanto si possono
esprimere soltanto proposizioni senza avere la possibilità di estenderla a una
classe di persone.
\newpage
\subsection{Sintassi}
Il linguaggio di una logica proposizionale è composto dai seguenti elementi:

% Elementi linguaggio logica proposizionale
\begin{itemize}
  \item Variabili Proposizionali: $P,Q,R \dots$
  \item Connettivi Proposizionali: $\land, \lor, \neg, \rightarrow, \iff$
  \item Simboli Ausiliari: (,)
  \item Costanti: $T,F$
\end{itemize}

La sintassi di un linguaggio è composta da una serie di formule ben
formate ($FBF$) definite induttivamente nel seguente modo:
% definizione formule ben formate
\begin{enumerate}
  \item Le costanti e le variabili proposizionali $\in FBF$.

  \item Se $A$ e $B \in FBF$ allora $(A \land B)$,$(A \lor B)$,$(\neg A)$,$(A
  \rightarrow B)$, $(A \iff B)$, $TA$ e $FA$ sono delle formule ben formate.
  \item nient'altro è una formula
\end{enumerate}

\begin{esempio}
  Vediamo degli esempi:
  \begin{itemize}
    \item $(P \land Q) \in Fbf$  è una formula ben formata\newline
    \item $(PQ \land R) \not \in Fbf$ in quanto non si rispetta la sintassi del
    linguaggio 
    definita. 
  \end{itemize}
\end{esempio}
% Definizione delle sottoformule
\begin{definizione}

  Sia $A \in FBF$, l'insieme delle sottoformule di $A$ è definito come segue:
  \begin{enumerate}
    \item Se $A$ è una costante o variabile proposizionale allora A stessa è la
    sua 
    sottoformula 
    \item Se $A$ è una formula del tipo $(\neg A')$ allora le sottoformule di A
    sono 
    A stessa e le sottoformule di $A'$; 
    $\neg$ è detto connettivo principale e $A'$ sottoformula immediata di A.
    \item Se $A$ è una formula del tipo $B \circ C$, allora le sottoformule di A
    sono A stessa 
    e le sottoformule di B e C; $\circ$ è il connettivo principale e B e C sono
    le due sottoformule immediate di A. 
  \end{enumerate}

\end{definizione}
È possibile ridurre ed eliminare delle parentesi attraverso l'introduzione della
precedenza tra gli operatori, che è definita come segue:\newline
$\neg, \land, \lor, \rightarrow,\iff$.

In assenza di parentesi una formula va parentizzata privileggiando le
sottoformule 
i cui connettivi principali hanno la precedenza più alta.\newline
In caso di parità di precedenza vi è la convenzione di associare da destra a
sinistra. 

Esempio:\newline
$\neg A \land (\neg B \rightarrow C) \lor D$ diventa
$((\neg A) \land ((\neg B) \rightarrow C) \lor D)$.

% Definizione di albero sintattico
\begin{definizione}
  Un albero sintattico $T$ è un albero binario coi nodi etichettati da simboli
  di $L$, che rappresenta la scomposizione di una formula ben formata $X$
  definita 
  come segue: 
\end{definizione}
\begin{enumerate}
  \item Se $X$ è una formula atomica,l'albero binario che la rappresenta è
  composto 
  soltanto dal nodo etichettato con $X$
  \item Se $X = A \circ B$, $X$ è rappresentata da un albero binario che ha la
  radice 
  etichettata con $\circ$, i cui figli sinistri e destri sono la
  rappresentazione di $A$ e $B$ 
  \item Se $X = \neg A$,$X$ è rappresentato dall'albero binario con radice
  etichettata 
  con $\neg$, il cui figlio è la rappresentazione di $A$
\end{enumerate}

Poiché una formula è definita mediante un albero sintattico, le proprietà di una
formula 
possono essere dimostrate mediante induzione strutturale sulla formula, ossia
dimostrare 
che la proprietà di una formula soddisfi i seguenti 3 casi:
\begin{itemize}
  \item è verificata la proprietà per tutte le formule atomo $A$
  \item supposta verifica la proprietà per $A$, si verifica che la proprietà è
  verificata per $\neg A$ 
  \item supposta la proprietà verificata per $A_1$ e $A_2$, si verifica che la
  proprietà è verifica per $A_1 \circ A_2$, per ogni connettivo $\circ$.
\end{itemize}
\newpage
\subsection{Semantica}
La semantica di una logica consente di dare un significato e un interpretazione
alle formule del Linguaggio.\newline

\begin{definizione}
  Sia data una formula proposizionale $P$ e sia ${P_1,\dots,P_n}$, l'insieme
  degli 
  atomi che compaiono nella formula $A$.Si definisce come \emph{interpretazione}
  una 
  funzione $v:\{P_1,\dots,P_n\} \mapsto \{T,F\}$ che attribuisce un valore di
  verità 
  a ciascun atomo della formula $A$.
\end{definizione}

I connettivi della Logica Proposizionale hanno i seguenti valori di verità:
% Tabella di Verità degli operatori
\[
  \begin{array}{ccccccc}
    \toprule
    \text{A} & \text{B} & A \land B & A \lor B & \neg A & A \Rightarrow B & A \iff B \\
    \midrule
    F & F & F & F & T & T & T \\
    F & T & F & T & T & T & F \\
    T & F & F & T & F & F & F \\
    T & T & T & T & F & T & T \\
    \bottomrule
  \end{array}
\]

Essendo ogni formula $A$ definita mediante un unico albero sintattico,
l'interpretazione $v$ 
è ben definito e ciò comporta che data una formula $A$ e un interpretazione
$v$, 
eseguita la definizione induttiva dei valori di verità, si ottiene un unico
$v(A)$. 

% Tipologie di formule
\begin{definizione}
  Una formula nella logica proposizionale può essere di diversi tipi:
  \begin{itemize}
    \item \textbf{valida o tautologica:} la formula è soddisfatta da qualsiasi
    valutazione della Formula 
    \item \textbf{Soddisfacibile non Tautologica:} la formula è soddisfatta da
    qualche valutazione 
    della formula ma non da tutte.
    \item \textbf{falsificabile:} la formula non è soddisfatta da qualche
    valutazione della formula. 
    \item \textbf{Contraddizione:} la formula non viene mai soddisfatta
  \end{itemize}
\end{definizione}

\begin{teorema}
  Si ha che:
  \begin{itemize}
    \item $A$ è una formula valida se e solo se $\neg A$ è insoddisfacibile.
    \item $A$ è soddisfacibile se e solo se $\neg A$ è falsificabile
  \end{itemize}
\end{teorema}


\subsubsection{Modelli e decidibilità}
Si definisce \emph{modello}, indicato con $M \models A$, tutte le valutazioni
booleane 
che rendono vera la formula $A$.
Si definisce \emph{contromodello}, indicato con , tutte le valutazioni booleane
che rendono falsa la formula $A$.

La logica proposizionale è decidibile (posso sempre verificare il significato di
una formula). 
Esiste infatti una procedura effettiva che stabilisce la validità o no di una
formula, o se questa 
ad esempio è una tautologia.
In particolare il verificare se una proposizione è tautologica o meno è
l’operazione di decidibilità principale che si svolge nel calcolo
proposizionale. 

\begin{definizione}
  Se $M \models A$ per tutti gli $M$, allora $A$ è una tautologia e si indica
  $\models A$ 
\end{definizione}

\begin{definizione}
  Se $M \models A$ per qualche $M$, allora $A$ è soddisfacibile
\end{definizione}

\begin{definizione}
  Se $M \models A$ non è soddisfatta da nessun $M$, allora $A$ è
  insoddisfacibile 
\end{definizione}
\subsection{Equivalenze Logiche}
\begin{defi}
  Date due formule $A$ e $B$, si dice che $A$ è \emph{logicamente equivalente} a
  $B$, 
  indicato con $A \equiv B$, se e solo se per ogni interpretazione $v$ risulta
  $v(A) = v(B)$. 
\end{defi}

Nella logica proposizionale sono definite le seguenti equivalenze logiche,
indicate con $\equiv$,: 
\begin{enumerate}
  \item \textbf{Idempotenza:}
  \begin{align*}
    A \lor A  \equiv  A \\
    A \land A  \equiv  A \\
  \end{align*}
  \item \textbf{Associatività:}
  \begin{align*}
    A \lor (B \lor C) \equiv  (A \lor B) \lor C \\
    A \land (B \land C)  \equiv  (A \land B) \land C
  \end{align*}
  \item \textbf{Commutatività:}
  \begin{align*}
    A \lor B  \equiv  B \lor A \\
    A \land B  \equiv  B \land A
  \end{align*}
  \item \textbf{Distributività:}
  \begin{align*}
    A \lor (B \land C)  \equiv & (A \lor B) \land (A \lor C)\\
    A \land (B \lor C)  \equiv & (A \land B \lor (A \land C)
  \end{align*}
  \item \textbf{Assorbimento:}
  \begin{align*}
    A \lor (A \land B)  \equiv  A
    A \land (A \lor B)  \equiv  A
  \end{align*}
  \item \textbf{Doppia negazione:}
  \begin{equation*}
    \neg \neg A \equiv A
  \end{equation*}
  \item\textbf{ Leggi di De Morgan:}
  \begin{align*}
    \neg (A \lor B)  \equiv  \neg A \land \neg B \\
    \neg(A \land B)  \equiv  \neg A \lor \neg B
  \end{align*}
  \item \textbf{Terzo escluso:}
  \begin{equation*}
    A \lor \neg A \equiv T
  \end{equation*}
  \item \textbf{Contrapposizione:}
  \begin{equation*}
    A \rightarrow B \equiv \neg B \rightarrow \neg A
  \end{equation*}
  \item \textbf{Contraddizione}
  \begin{equation*}
    A \land \neg A \equiv F
  \end{equation*}
\end{enumerate}
\subsubsection{Completezza di insiemi di Connettivi}
Un insieme di connettivi logici è completo se mediante i suoi connettivi si può
esprimere un qualunque altro connettivo.
Nella logica proposizionale valgono anche le seguenti equivalenze, utili per
ridurre il linguaggio: 

\[(A \rightarrow B)  \equiv  (\neg A \lor B) \]
\[(A \lor B)  \equiv  \neg(\neg A \land \neg B) \]
\[(A \land B)  \equiv  \neg(\neg A \lor \neg B) \]
\[(A \iff B) \equiv  (A \rightarrow B) \land (B \rightarrow A) \]



L'insieme dei connettivi $\{ \neg,\lor,\land \}$, $\{ \neg,\land \}$ e $\{
\neg,\lor \}$ sono completi 
e ciò è facilmente dimostrabile utilizzando le seguenti equivalenze logiche.
\newpage
\section{Logica PLTL}
La semplice logica proposizionale ha però dei limiti, si introduce quindi la
\textbf{logica PLTL (\textit{Propositional Linear Time Logic})} che introduce il
concetto di \textbf{tempo}, in ottica \textbf{lineare}, nel processo logico.\\
La logica PLTL viene usata per \textbf{model checking} e lo scopo generale che
ci si propone è quello di presentare un approccio formale alla progettazione e
all'implementazione di sistemi basato su un linguaggio formale che permetta di
specificarli e ragionare sulle loro proprietà. Si vogliono studiare sia sistemi
grandi, come software o addirittura sistemi operativi, che piccoli, come per
esempio una coppia di semafori. Tutti questi sistemi hanno però la medesima
caratteristica, ovvero assumono in ogni \textbf{istante di tempo} un determinato
\textbf{stato} definito dalle \textbf{variabili} del sistema stesso. Si hanno
quindi nel sistema, in ogni istante di tempo:
\begin{itemize}
  \item uno \textbf{stato}, univocamente definito dai valori delle variabili
  \item una \textbf{transizione} che segnala un passaggio da uno stato ad un
  altro
  \item una \textbf{computazione} che rappresenta una sequenza di stati di cui
  ogni coppia forma una transizione. Si introduce quindi il concetto
  \textbf{temporale} 
\end{itemize}
Il \textbf{tempo} viene pensato come un oggetto discreto, cadenzato dalle
transizioni, su cui può essere definita anche una relazione d'ordine (potrebbe
servire che un processo termini prima, dopo o nello stesso tempo di un altro).
\begin{definizione}
  Si definisce \textbf{sistema reattivo} una componente:
  \begin{itemize}
    \item non terminante e interattivo
    \item che può leggere il proprio input non solo all'inizio della
    computazione e che può produrre output non solo alla fine della sua
    computazione (si possono quindi avere multipli input e multipli output)
    \item che interagisce con altri componenti distribuiti o concorrenti
  \end{itemize}
\end{definizione}
Si hanno però sistemi non terminanti che usano un numero \textbf{finito} di
variabili e di conseguenza si ha un numero di stati, in cui transita il sistema,
\textbf{finito}. Si hanno infatti sistemi di transizione finiti che
\textit{``comprimono''} sistemi non terminanti, con computazioni di lunghezza
infinita, in una rappresentazione finita.
\newpage
Si ricorda che si possono inoltre categorizzare le proprietà che vogliamo
specificare come: 
\begin{itemize}
  \item \textbf{proprietà di safety}
  \item \textbf{proprietà di liveness}
  \item \textbf{proprietà di fairness}
\end{itemize}
Si può quindi già intuire che:
\begin{center}
  \textit{le logiche temporali sono frammenti della logica del primo ordine}
\end{center}
Infatti con le logiche temporali, come la \textit{logica PLTL}, si supera il
limite di rappresentazione temporale della logica proposizionale, permettendo,
per esempio, di rappresentare proposizioni del tipo \textit{``prima o poi'',
  ``accade sempre che'' etc$\ldots$}.\\
Si ha un \textbf{tempo lineare e discreto}, che si può pensare di rappresentare
nella logica classica proposizionale (per esempio come indice) ma questo
comporta il doversi dotare di infinite variabili proposizionali (corrispondenti
ad ogni \textit{``step''} temporale) e comporta l'ottenimento di una formula
proposizionale di lunghezza infinita. Si può pensare di passare allo studio di
questi casi con la \textit{logica predicativa} ma sarebbe ben più del
necessario, in quanto le procedure sarebbero di complessità molto elevata, a
causa della grande espressività della logica predicativa. Inoltre la logica dei
predicati è indecidibile. \\
Si cerca quindi la via di mezzo cercando logiche specializzate, meno espressive
della logica predicativa ma decidibili rispetto ai problemi presi in
considerazione. Si cerca una logica con procedure efficienti rispetto
all'ampiezza della descrizione del problema in analisi (ovvero tipicamente
l'ampiezza del sistema di transizioni), che è direttamente proporzionale al
numero di stati del sistema (lineare rispetto al numero degli stati), e rispetto
alla lunghezza della formula che esprime la proprietà da testare.
\newpage
\subsection{Sintassi}
Vediamo innanzitutto la \textbf{sintassi} della logica PLTL.\\
Si ha a che fare con un \textit{vocabolario} più ampio rispetto a quelli della
logica proposizionale, vengono infatti aggiunti dei \textbf{connettivi} utili
alla rappresentazione temporale richiesta.
\begin{definizione}
  Nella logica PLTL, oltre ai connettivi logici classici della logica
  proporzionale, si definiscono i seguenti connettivi:
  \begin{itemize}
    \item \textbf{X}, detto anche \textbf{next} o \textbf{tomorrow}. È un
    connettivo unario e viene rappresentato con $\circ$
    \item \textbf{F}, detto anche \textbf{sometime} o \textbf{future}. È un
    connettivo unario e viene rappresentato con $\diamond$
    \item \textbf{G}, detto anche \textbf{globally} o \textbf{always}. È un
    connettivo unario e viene rappresentato con $\square$
    \item \textbf{U}, detto anche \textbf{until}. È un connettivo binario
  \end{itemize}
  Viene inoltre indicato con $V$ l'\textbf{insieme delle variabili
    proposizionali}, variabili che hanno lo stesso significato della logica
  proposizionale, quindi ogni variabile corrisponde ad una singola proposizione
  del linguaggio naturale. 
\end{definizione}
\begin{definizione}
  Diamo ora una definizione, formale, procedendo per induzione di
  \textbf{formula} del linguaggio PLTL (definendo così l'aspetto sintattico
  della logica PLTL): 
  \begin{itemize}
    \item $\forall p\in V$ vale che $p$ è una formula del linguaggio PLTL,
    ovvero le variabili proposizionali della logica classica sono formule del
    linguaggio PLTL. Questa è una \emph{formula atomica}
    \item i simboli $\top$ (detto anche \emph{true} che semanticamente semplifica
    una tautologia) e $\bot$ (detto anche \textit{false} o \emph{bottom} che
    semanticamente specifica una formula sempre falsa, ovvero una
    contraddizione) sono formule del linguaggio PLTL. Anche questa è una
    \emph{formula atomica}
    \item se $A$ è una formula del linguaggio PLTL allora lo sono anche:
    \begin{itemize}[label=$\ast$]
      \item $\neg A$
      \item $\mathbf{X}A$
      \item $\mathbf{F}A$
      \item $\mathbf{G}A$
    \end{itemize}
    \newpage
    \item  se $A$ e $B$ sono formule del linguaggio PLTL allora lo sono anche:
    \begin{itemize}[label=$\ast$]
      \item $A\to B$
      \item $A\land B$
      \item $A\lor B$
      \item $A\mathbf{U}B$
    \end{itemize}
    \item nient'altro appartiene all'insieme delle formule del linguaggio PLTL
  \end{itemize}
  Quindi si nota come \textbf{l'insieme delle formule PLTL contiene quello delle
    formule classiche della logica proporzionale}. Si ha quindi che l'insieme
  delle formule PLTL non è altro che un'estensione di quello delle formule
  classiche della logica proposizionale.\\
\end{definizione}
\begin{definizione}
  La sintassi delle formule del linguaggio PLTL possono essere definite usando
  anche la notazione \textbf{BNF (Backus-Naur Form o Backus Normal Form)},
  ovvero, $\forall p\in V$:
  \[A::=p\,|\,\top\,|\,\bot\,|\,(\neg A)\,|\,(A\land A)\,|\,(A\lor A)\,|\,(A\to
    A)\,|\,(\mathbf{X}A)\,|\,\mathbf{F}A\,|\,\mathbf{G}A\,|\,(A\mathbf{U}A)\]
  Non si è usata quindi una definizione ricorsiva ma viene invece usata una
  \textbf{grammatica}\\
  \begin{shaded}
    La BNF (Backus-Naur Form o Backus Normal Form) è una metasintassi, ovvero un
    formalismo attraverso cui è possibile descrivere la sintassi di linguaggi
    formali (il prefisso meta ha proprio a che vedere con la natura circolare di
    questa definizione). Si tratta di uno strumento molto usato per descrivere
    in modo preciso e non ambiguo la sintassi dei linguaggi di programmazione,
    dei protocolli di rete e così via, benché non manchino in letteratura esempi
    di sue applicazioni a contesti anche non informatici e addirittura non
    tecnologici. La BNF viene usata nella maggior parte dei testi sulla teoria
    dei linguaggi di programmazione e in molti testi introduttivi su specifici
    linguaggi. \\
    In termini formali, la BNF può essere vista come un formalismo per descrivere
    grammatiche libere dal contesto.  \\
    Una specifica BNF è un insieme di regole di derivazione ciascuna espressa
    nella forma: 
    \begin{center}
      \textit{<simbolo> ::= \_espressione\_}
    \end{center}
  \end{shaded}
\end{definizione}
\newpage
\subsection{Semantica}
\begin{definizione}
  La \textbf{semantica}, ovvero il significato dei connettivi della logica PLTL,
  è data usando i cosiddetti \textbf{modelli lineari} (si ricorda l'uso di un
  tempo lineare).\\
  Si consideri una struttura algebrica di questo tipo:
  \[M=\langle S,\,\rho, \,\to, \,\Vdash\rangle\]
  dove:
  \begin{itemize}
    \item $S$ è un \textbf{insieme infinito di stati}, detti anche
    \textbf{mondi}
    \item $\rho \in S$ è uno stato del modello detto anche \textbf{root} o
    \textbf{radice}. In tale stato viene codificato il \textbf{tempo zero}
    \item $\to$ è una relazione binaria su $S$ detta \textbf{relazione di
      transizione} la quale introduce un \textbf{ordinamento lineare} sugli
    elementi di $S$. Si ha quindi che:
    \[\to\,\subseteq S\times S\]
    e, $\forall \alpha \in S$, \textbf{esiste ed è unico} $\beta\in S$ tale che
    vale:
    \[\alpha\to\beta\]
    Preso quindi uno stato qualsiasi ho un solo modo per passare ad un altro
    stato (esiste quindi un ``prima'' e un ``dopo'').\\
    Inoltre vale che, $\forall \alpha\in S$, $\alpha\not\to \rho$, dove $\rho\in
    S$ è l'unico elemento di $S$ a godere di questa proprietà (in quanto $\rho$
    codifica il tempo zero). 
    \item $\Vdash$ è una relazione binaria, inclusa in $S\times V$, detta
    \textbf{relazione di soddisfacibilità}. Solitamente con $\alpha\Vdash\p$
    si indica che $(\alpha,\p)\in\,\Vdash$ è valido, ovvero che $alpha$
    \textbf{soddisfa} $p$, con $\alpha\in S$ e $p\in V$
  \end{itemize}
  La struttura $M$ viene detta \textbf{modello per PLTL}.\\
  Si nota come questi modelli lineari seguano un ordinamento simile a quello che
  si ha tra i numeri in $\mathbb{N}$.\\ 
  Si nota come la semantica della logica PLTL sia drasticamente più complessa
  di quella della logica classica proposizionale 
\end{definizione}
\newpage
Bisogna dare ora significato ai connettivi PLTL. Per i connettivi della logica
proposizionale si usano tavole di verità e induzione ma per la logica PLTL le
cose sono un po' diverse.\\
Per dare un significato alle formule del linguaggio PLTL bisogna basarsi sui
modelli per PLTL
\begin{definizione}
  Siano:
  \begin{itemize}
    \item $M=\langle S,\,\rho, \,\to, \,\Vdash\rangle$ un modello per PLTL
    \item $\alpha\in S$ uno stato
    \item $A$ una formula del linguaggio PLTL
  \end{itemize}
  si ha che la \textbf{soddisfacibilità} di $A$ nello stato $\alpha$ di $M$
  viene indicata con:
  \[(M,\alpha)\Vdash A\]
  e tale soddisfacibilità (o verità) della formula $A$ viene definita per
  induzione sulla struttura di $A$ nel seguente modo:
  \begin{enumerate}
    \item se $A$ è una variabile proposizionale allora vale $(M,\alpha)\Vdash A$
    sse $\alpha\Vdash A$ per la relazione $\Vdash$ definita da $M$. Questo è un
    \textbf{caso base}
    \item se $A$ è $\top$ allora vale $(M,\alpha)\Vdash\top$, ovvero il true è
    \textbf{soddisfatto} o \textbf{forzato} ovunque. Il simbolo \emph{true} è
    vero in ogni stato e in ogni modello (siamo nel concetto di tautologia).
    Questo è un \textbf{caso base}
    \item se $A$ è $\bot$ allora vale $(M,\alpha)\nVdash\bot$, ovvero il false
    non è mai soddisfatto qualsiasi sia lo stato. Questo è un \textbf{caso base}
    \item se $A$ è $B\land C$ (con $B$ e $C$ formule PLTL) allora vale
    $(M,\alpha)\Vdash B\land C$ sse $(M,\alpha)\Vdash B$ e $(M,\alpha)\Vdash C$
    \item se $A$ è $B\lor C$ (con $B$ e $C$ formule PLTL) allora vale
    $(M,\alpha)\Vdash B\lor C$ sse $(M,\alpha)\Vdash B$ oppure $(M,\alpha)\Vdash
    C$ 
    \item se $A$ è $B\to C$ (con $B$ e $C$ formule PLTL) allora vale\\
    $(M,\alpha)\Vdash B\to C$ sse $(M,\alpha)\nVdash B$ oppure $(M,\alpha)\Vdash
    C$ 
    \item se $A$ è $\neg B$ (con $B$ formula PLTL) allora vale
    $(M,\alpha)\Vdash\neg B$ sse vale $(M,\alpha)\nVdash B$
    \newpage
    \item se $A$ è del tipo $\mathbf{X}\,B$ allora vale $(M,\alpha)\Vdash
    \mathbf{X}\,B$ sse vale $(M,\beta)\Vdash B$ (ovvero quando $B$ è soddisfatto
    nello stato $\beta$ di M), dove $\beta\in S$ è lo stato
    successivo (nonché unico) ad $\alpha$, ovvero si ha $\alpha\to\beta$
    \item se $A$ è del tipo $\mathbf{F}\,B$ allora vale $(M,\alpha)\Vdash
    \mathbf{F}\,B$ sse esiste una sequenza finita
    $\alpha_0\to\alpha_1\to\ldots\to\alpha_n$, con $\alpha=\alpha_0$ e $n\geq
    0$, tale che $(M,\alpha_n)\Vdash B$ (ovvero sse esiste un punto ``nel
    futuro'', ovvero $alpha_n$, che però potrebbe anche essere $\alpha_0$, dove
    la formula $B$ è soddisfatta)
    \item se $A$ è del tipo $B\,\mathbf{U}\,B$ allora vale $(M,\alpha)\Vdash
    B\,\mathbf{U}\,C$ sse esiste una sequenza finita
    $\alpha_0\to\alpha_1\to\ldots\to\alpha_n$, con $\alpha=\alpha_0$ e $n\geq
    0$, tale che $(M,\alpha_n)\Vdash C$ e tale che $(M,\alpha_i)\Vdash
    B,\,\,\,i=0,1,\ldots,n-1$ (ovvero deve esistere uno stato ``nel futuro'' di
    $\alpha$ che soddisfa $C$ e uno stato intermedio a quello che soddisfa $C$
    che soddisfi $B$) 
    \item se $A$ è del tipo $\mathbf{G}\,B$ allora vale $(M,\alpha)\Vdash
    \mathbf{G}\,B$ sse $(M,\alpha)\Vdash B$ e $(M,\beta)\Vdash \mathbf{G}\,B$
    dove $\beta\in S$ è lo stato successivo (nonché unico) ad $\alpha$, ovvero
    si ha $\alpha\to\beta$ (ovvero sse nello stato $\alpha$ è soddisfatto $B$ e
    nello stato $\beta$ è soddisfatto $\mathbf{G}\,B$). Questa è una definizione
    ricorsiva (anche se uso $\mathbf{G}\,B$ su uno stato successivo ad
    $\alpha$)
  \end{enumerate}
  I primi 3 punti sono i casi base, quelli successivi sono i passi induttivi.\\
  I primi 7 punti sono simili a quelli della logica proposizionale, se ci
  limitiamo ad un solo mondo
\end{definizione}
\begin{esempio}
  Graficamente si ha qualcosa del tipo:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.263]{img/pltl.png}
  \end{figure}
  \textbf{che prosegue infinitamente}. Si nota che in ogni stato si specifica se
  una variabile proposizionale (che sono infinite) è soddisfatta e, per ciascuna
  variabile, si definisce una sorta di \textit{tabella di verità}, che
  specificare la soddisfacibilità di una variabile in un determinato stato
\end{esempio}
Si nota che la definizione di \textbf{modello PLTL} non esclude che due stati
diversi possano soddisfare le stesse variabili, ovvero che si possano comportare
nello stesso modo \textit{(come se nulla cambiasse nel tempo)}.
\begin{esempio}
  Possiamo anche, per esempio, dare una rappresentazione grafica dei nuovi
  connettivi, per esempio del \textbf{next}:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{img/pltl2.png}
  \end{figure}
  Ovvero in $\alpha$ è soddisfatto $B$, $\alpha\Vdash\mathbf{X}B$, sse nello
  stato successivo $\beta$ è soddisfatto $B$.\\
  Vediamo anche il connettivo \textbf{future}:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.27]{img/pltl3.png}
  \end{figure}
  Si nota che se posso affermare che nello stato $\alpha$ si possa affermare
  $\mathbf{F}\,B$ allora tutti gli elementi della sequenza finita possono
  affermare $\mathbf{F}B$ (compreso l'ultimo stato stesso e compresa l'ipotetica
  radice).
  \newpage
  Questa proprietà è detta \textbf{persistenza all'indietro}, ovvero se
  un certo stato soddisfa una formula nel futuro allora tutti gli stati
  nell'intervallo possono fare la medesima affermazione (estremi inclusi).\\
  Vediamo ora il connettivo \textbf{until}:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.27]{img/pltl4.png}
  \end{figure}
  Se però si avesse un lasso di tempo nullo non avrei bisogno della
  soddisfacibilità di $B$ (infatti si ha che $(M,\alpha_i)\Vdash
  B,\,\,\,i=0,1,\ldots,n-1$).\\ 
  Vediamo infine il connettivo \textbf{globally}:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.35]{img/pltl5.png}
  \end{figure}
  si può letteralmente visualizzare la definizione ricorsiva del connettivo. Si
  crea una catena di stati che soddisfano sempre $B$ e anche $\mathbf{G}\,B$. Si
  dice che $\mathbf{G}\,B$ è \textbf{persistente in avanti}, ovvero dal tempo
  iniziale in cui si afferma un \emph{globally} si avrà che in ogni stato sarà
  affermato il \emph{globally}.\\
  \newpage
  Vediamo un esempio di modello infinito:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{img/pltl6.png}
    \caption{Esempio pratico di un modello PLTL con la rappresentazione dei vari
    connettivi logici. \textbf{Errata corrige}: nella prima riga, a destra, non
    è $\alpha\Vdash \square \, t$ bensì $\rho\Vdash \square \, t$}
  \end{figure}
  In questo modello la variabile proporzionale $t$ è sempre soddisfatta, in ogni
  stato, e quindi si ha che vale la relazione:
  \[\theta\Vdash t,\,\,\forall \theta \in S\]
  Si usa, in generale, la stessa tecnica di analisi della logica proporzionale,
  partendo dallo studio delle variabili e arrivando gradualmente allo studio
  della formula finale più complessa
\end{esempio}
\begin{definizione}
  Quando $(M,\alpha)\Vdash A$ vale diremo che $M$ \textbf{soddisfa} $A$
  \textbf{nello stato} $\alpha$ o che $A$ \textbf{è vera nello stato} $\alpha$
  \textbf{di} $M$. Quando non dovesse esserci ambiguità circa il modello di cui
  si parla scriveremo semplicemente $\alpha\Vdash A$ invece di $(M,\alpha)\Vdash
  A$.  
\end{definizione}
\newpage
Si nota come sia possibile stabilire un ordinamento lineare tra i vari stati
del sistema, avendo una corrispondenza biunivoca tra il modello PLTL è la
rappresentazione dei numeri naturali. Si ha quindi che $\rho$ rappresenta lo
$0$ (il tempo zero) mentre gli archi $\to$ rappresentano il $+1$, ovvero il
\textbf{successore}.\\ 
Possiamo definire i modelli per PLTL in modo più compatto come una coppia
$M=\langle \mathbb{N},\Vdash\rangle$, con $ \Vdash \subseteq \mathbb{N}\times
V$. In questo modo $\mathbb{N}$ diventa l’insieme degli stati e, per ogni stato,
specifica quali formule atomiche sono soddisfatte. \\
Dato che implicitamente assumiamo che gli elementi di $\mathbb{N}$ si 
susseguano esattamente nel modo standard, la nozione di soddisfacibilità di
formule per il linguaggio PLTL può essere data in modo compatto.
\begin{definizione}
  Siano $M=\langle \mathbb{N},\Vdash\rangle$ un modello per PLTL, $t\in
  \mathbb{N}$ e $A$ una formula del linguaggio PLTL.\\
  La \textbf{soddisfacibilità di} $A$ \textbf{nello stato (o, detto meglio,
    \emph{al tempo})} $t$ \textbf{di} $M$, indicata con:
  \[(M,t)\Vdash A\]
  \begin{center}
    o più brevemente con:
  \end{center}
  \[t\Vdash A\]
  viene definita per induzione sulla struttura di $A$ come segue:
  \begin{enumerate}
    \item nel caso $A$ sia una variabile proposizionale allora vale
    $(M,t)\Vdash A$ sse vale $t\Vdash A$ per la relazione $\Vdash$ di $M$
    \item nel caso $A$ sia $\top$ allora vale $(M,t)\Vdash \top$
    \item nel caso $A$ sia $\bot$ allora vale $(M,t)\nVdash \bot$
    \item nel caso $A$ sia $B\land C$ allora vale $(M,t)\Vdash B\land C$ sse
    valgono $(M,t)\Vdash B$ e $(M,t)\Vdash C$
    \item nel caso $A$ sia $B\lor C$ allora vale $(M,t)\Vdash B\lor C$ sse
    valgono $(M,t)\Vdash B$ oppure $(M,t)\Vdash C$
    \item nel caso $A$ sia $B\to C$ allora vale $(M,t)\Vdash B\to C$ sse
    valgono $(M,t)\nVdash B$ oppure $(M,t)\Vdash C$
    \item nel caso $A$ sia $\neg B$ allora vale $(M,t)\Vdash \neg B$ sse
    $(M,t)\nVdash B$
    \item nel caso $A$ sia del tipo $\mathbf{X}\,B$ allora vale $(M,t)\Vdash
    \mathbf{X}\,B$ sse vale $(M,t+1)\Vdash B$
    \item nel caso $A$ sia del tipo $\mathbf{F}\,B$ allora vale $(M,t)\Vdash
    \mathbf{F}\,B$ sse $\exists\, t'\geq t$ tale che vale $(M,t')\Vdash B$
    \item nel caso $A$ sia del tipo $B\mathbf{U}\,C$ allora vale $(M,t)\Vdash
    B\mathbf{U}\,C$ sse $\exists\, t'\geq t$ tale che vale $(M,t')\Vdash C$ e,
    per $t''=t,\ldots,t'$, vale $(M,t'')\Vdash B$
    \item nel caso $A$ sia del tipo $\mathbf{G}\,A$ allora vale $(M,t)\Vdash
    \mathbf{G}\,A$ sse $\forall\, t'\geq t$ vale $(M,t')\Vdash A$
  \end{enumerate}
  \textbf{I connettivi temporali vengono rappresentati in una forma
    drasticamente più compatta grazie alla rappresentazione mediante numeri
    naturali.}\\ 
  Quindi, in questa \emph{scrittura alternativa}, ad ogni stato si associa un
  numero intero e viceversa ad ogni intero viene associato uno stato. Questa
  scrittura però non risulta comoda per la rappresentazione dei sistemi P/T,
  dove non si ha una struttura obbligatoriamente lineare rappresentabile tramite
  una sequenza di numeri naturali.
\end{definizione}
\begin{definizione}
  Si definiscono nella logica PLTL i seguenti concetti:
  \begin{itemize}
    \item una formula $A$ è \textbf{soddisfacibile} nella logica PLTL se
    esistono un modello $M=\langle \mathbb{N},\Vdash\rangle$ ed un tempo
    $t\in\mathbb{N}$, tale che $(M,t)\Vdash A$
    \item una formula $A$ è una \textbf{tautologia} nella logica PLTL se per
    ogni modello $M=\langle \mathbb{N},\Vdash\rangle$ e per ogni tempo
    $t\in\mathbb{N}$, vale che$ (M,t)\Vdash A$
    \item una formula $A$ è una \textbf{contraddizione} nella logica PLTL se
    $\neg A$ è una tautologia
  \end{itemize}
  Si nota l'analogia di questi concetti tra la logica PLTL e la logica classica
  proporzionale.\\
  Si nota che la soddisfacibilità di una formula al tempo $t$ dipende solo dal
  tempo $t$ stesso e dai tempi seguenti: non dipende dal tempo \emph{passato}.\\
  Se una formula è soddisfacibile al tempo $t$ lo è anche nel sotto-modello
  ottenuto rimuovendo al modello iniziale tutti gli stati precedenti al tempo
  $t$.\\
  Si dice che: \textbf{lo stato presente ci dice tutto}
\end{definizione}
\subsection{Proprietà dei Connettivi Temporali}
\begin{definizione}
  Vengono quindi definite alcune proprietà dei connettivi temporali nella logica
  PLTL:
  \begin{itemize}
    \item se vale $(M,t)\Vdash \mathbf{F}\,A$ allora $\forall t'\leq t$ vale
    $(M,t')\Vdash \mathbf{F}\,A$, quindi il \emph{future} è \textbf{persistente
      all'indietro}
    \item se vale $(M,t)\Vdash \mathbf{G}\,A$ allora $\forall t'\geq t$ vale
    $(M,t')\Vdash \mathbf{G}\,A$, quindi il \emph{globally} è
    \textbf{persistente in avanti}
    \newpage
    \item se vale $(M,t)\nVdash A\,\mathbf{U}\,B$ allora per definizione del
    significato di \textbf{U} ci sono due casi:
    \begin{enumerate}
      \item non esiste alcuno stato che renda vera $B$, ovvero $\forall t'\geq
      t$ vale che $(M,t')\nVdash B$. Detto in maniera equivalente
      $\forall t'\geq t$ vale $(M,t')\Vdash\neg B$, ovvero $(M,t')\Vdash
      \mathbf{G}\neg B$
      \begin{figure}[H]
        \centering
        \includegraphics[scale = 0.35]{img/pltl7.png}
        \caption{Rappresentazione grafica del punto 1}
      \end{figure}
      \item si ha che il precedente è falso, ovvero $\exists\,t'\geq t$ tale che
      $(M,t')\Vdash B$, quindi $\exist\,\,t''$ tale che $t\leq t''< t$ tale che
      che $(M,t'')\nVdash A$
      \begin{figure}[H]
        \centering
        \includegraphics[scale = 0.35]{img/pltl8.png}
        \caption{Rappresentazione grafica del punto 2}
      \end{figure}
    \end{enumerate}
    \newpage
    \item se vale $(M,t)\Vdash B\,\mathbf{U}\,C$ allora $\exists\, t'\geq t$
    tale che $(M,t')\Vdash C$ e tale che, per $t''=t,\ldots,t'-1$,
    $(M,t'')\Vdash B$. Questo implica che, per $t''=t,\ldots,t'$, $(M,t'')\Vdash
    B\,\mathbf{U}\,C$. Graficamente:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.3]{img/pltl9.png}
    \end{figure}
    \item si ha che $\mathbf{G}\,A\to A$ è una \textbf{tautologia} in quanto se
    si ha che $(M,t)\Vdash \mathbf{G}\,A$ allora si ha che $(M,t)\Vdash A$ per
    definizione del connettivo \emph{globally} e per il significato di
    \emph{implicazione}
    \item si ha che $\mathbf{G}\,A$ è \emph{equivalente} a
    $\overbrace{\mathbf{G}\ldots\mathbf{G} }^{\mbox{2 o
        più}}A$. Ovvero scrivere $n$ volte $\mathbf{G}$ davanti ad una formula
    data non ha alcun senso. Si nota che la logica PLTL possa essere vista, in
    questo caso, come frammento della logica predicativa. \emph{Dimostrazione
      nel file delle esercitazioni} 
    \item si ha che $A\to \mathbf{F}\,A$ è una
    \emph{tautologia} in quanto se si ha che $(M,t)\Vdash A$ allora
    si ha che $(M,t)\Vdash\mathbf{F} A$ per definizione del connettivo
    \emph{future} e per il significato di \emph{implicazione}
    \item si ha che $\mathbf{F}\,A$ è \emph{equivalente} a
    $\overbrace{\mathbf{F}\ldots\mathbf{F} }^{\mbox{2 o
        più}}A$. Ovvero scrivere $n$ volte $\mathbf{F}$ davanti ad una formula
    data non ha alcun senso. Si nota che la logica PLTL possa essere vista, in
    questo caso, come frammento della logica predicativa. \emph{Dimostrazione
      nel file delle esercitazioni} 
    \item si ha che $B\to A\,\mathbf{U}B$ è una \textbf{tautologia} in quanto se
    si ha che $(M,t)\Vdash B$ allora si ha che $(M,t)\Vdash A\,\mathbf{U}\,B$
    per definizione del connettivo \emph{until} e per il significato di
    \emph{implicazione}
    \item si ha che $(A\,\mathbf{U}\,B)\to\mathbf{F}\,B$ è una \emph{tautologia}.
    \emph{Dimostrazione nel file delle esercitazioni}
    \item si ha che $\mathbf{F}\,A\to \top\,\mathbf{U}A$ è una
    \textbf{tautologia}. \emph{Dimostrazione nel file delle esercitazioni}
    \item si ha che $\neg\mathbf{G}\,A$ è \emph{equivalente} a
    $\neg\mathbf{F}\,A$. Infatti si dimostra che vale $(M,t)\Vdash \neg
    \mathbf{G}\,A$ sse vale $(M,t)\nVdash \neg \mathbf{F}\,A$, infatti, per il
    significato del connettivo $\neg$, $(M,t)\nVdash\mathbf{G}\,A$ vale sse
    vale anche $(M,t)\Vdash\mathbf{F}\neg A$. Inoltre, per definizione di
    $\mathbf{G}$, si ha che $(M,t)\nVdash\mathbf{G}\,A$ sse $\exists t'\geq t$
    tale che vale $(M,t')\Vdash A$ sse, sempre per il significato del connettivo
    $\neg$, vale $(M,t')\Vdash \neg A$, che a sua volta vale, per il significato
    di $\mathbf{F}$, sse $(M,t)\Vdash\mathbf{F} \neg A$. Graficamente:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.35]{img/pltl10.png}
    \end{figure}
    \item si ha che $\neg\mathbf{F}\,A$ è \emph{equivalente} a
    $\neg\mathbf{G}\,A$. Infatti si dimostra che vale $(M,t)\Vdash \neg
    \mathbf{F}\,A$ sse vale $(M,t)\nVdash \neg \mathbf{G}\,A$, infatti, per il
    significato del connettivo $\neg$, $(M,t)\nVdash\mathbf{F}\,A$ vale sse
    vale anche $(M,t)\Vdash\mathbf{G}\neg A$. Inoltre, per definizione di
    $\mathbf{G}$, si ha che $(M,t)\nVdash\mathbf{F}\,A$ sse $\forall t'\geq t$
    vale $(M,t')\Vdash A$ sse, sempre per il significato del connettivo
    $\neg$, vale $(M,t')\Vdash \neg A$, che a sua volta vale, per il significato
    di $\mathbf{G}$, sse $(M,t)\Vdash\mathbf{G} \neg A$. Graficamente:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.35]{img/pltl11.png}
    \end{figure}
    \item si ha che $\mathbf{G}\,A$ è \emph{equivalente} a
    $\neg(\top\mathbf{U}\neg A)$. Bisogna dimostrare che vale
    $(M,t)\Vdash\mathbf{G}\,A$ sse vale $(M,t)\Vdash\neg(\top\mathbf{U}\neg A)$.
    Per il significato di $\neg$ si che $(M,t)\Vdash\mathbf{G}\,A$ vale sse
    vale $(M,t)\nVdash\top\mathbf{U}\neg A$. Se si suppone che
    $(M,t)\Vdash\mathbf{G}\,A$ vale allora si ha che, $\forall t'\geq t$,
    valgono:
    \begin{itemize}
      \item $(M,t')\Vdash A$
      \item $(M,t')\nVdash \neg A$
    \end{itemize}
    Dato che non si può esibire un tempo $t'\geq t$ tale che $(M,t')\Vdash
    \neg A$ per definizione del connettivo $\mathbf{U}$ si che val
    $(M,t)\Vdash\neg(\top\mathbf{U}\neg A)$ e quindi vale $(M,t)\nVdash
    (\top\mathbf{U}\,\neg A)$.\\
    Dato che qui l’operando di sinistra di $\mathbf{B}$ è la formula $\top$, la
    quale è soddisfatta in ogni possibile stato, segue che se vale $(M,t)\nVdash
    (\top\mathbf{U}\,\neg A)$ allora, $\forall t'\geq t$, vale $(M,t')\nVdash
    \neg A$ e quindi, $\forall t'\geq t$, valgono:
    \begin{itemize}
      \item $(M,t')\Vdash\neg\neg A$
      \item $(M,t')\Vdash A$, che è la definizione di $\mathbf{G}$
    \end{itemize}
    Per cui si è dimostrato che $(M,t)\Vdash \mathbf{G}\,A$. Graficamente:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.5]{img/pltl12.png}
      \includegraphics[scale = 0.423]{img/pltl13.png}
    \end{figure}
    \newpage
    \item si ha che le formule $\mathbf{X}(A\lor B)$ e
    $\mathbf{X}\,A\lor\mathbf{X}\,B$ sono \emph{equivalenti}.
    \emph{Dimostrazione nel file delle esercitazioni}
    \item le formule $\mathbf{X}\,\mathbf{G}\,A$ e $\mathbf{G}\,\mathbf{X}\, A$
    sono \emph{equivalenti}. \emph{Dimostrazione nel file delle esercitazioni}
    \item le formule $\mathbf{X}\,\mathbf{F}\,A$ e $\mathbf{F}\,\mathbf{X}\, A$
    sono \emph{equivalenti}. \emph{Dimostrazione nel file delle esercitazioni}
    \item le formule $\mathbf{G}\,\mathbf{F}\,\mathbf{G}\,A$ e
    $\mathbf{F}\,\mathbf{G}\, A$  sono \emph{equivalenti}. Ovvero è inutile
    alternare in questa maniera $\mathbf{G}$ e $\mathbf{F}$.\\
    Si vuole dimostrare che:
    \[(M, t)\Vdash \mathbf{GFG}\,A \mbox{ se e solo se }(M, t)\Vdash \mathbf{F
        G}\.A\]
    Dalla definizione di $\mathbf{G}$ si ha che:
    \[\mbox{se } (M, t)\Vdash\mathbf{G F G}\,A,\mbox{ allora }(M,
      t)\Vdash\mathbf{F G}\,A\]
    inoltre, viceversa, si ha che:
    \[\mbox{se vale }(M, t)\Vdash\mathbf{F G}\,A, \mbox{ allora vale } (M,
      t)\Vdash\mathbf{G F G}\,A\]
    Inoltre, per il significato di $\mathbf{F}$ se vale $(M, t)\Vdash\mathbf{F
      G}\,A$ allora $\exists\,t'\geq t$ tale che $(M,
    t')\Vdash\mathbf{G}\,A$. Inoltre, per la persistenza in avanti del
    \emph{globally} si ha che:
    \[\forall\,t''\geq t',\,(M,t'')\Vdash \mathbf{G}\,A\]
    e per il significato di $\mathbf{F}$:
    \[\forall\,t''\geq t',\,(M,t'')\Vdash \mathbf{FG}\,A\]
    Ne segue che, dato che il \emph{future} è persistente all'indietro:
    \[\forall\,t'''\geq t',\,(M,t''')\Vdash \mathbf{FG}\,A\]
    e quindi, per definizione di $\mathbf{G}$, segue che $(M,t)\Vdash
    \mathbf{GFG}\,A$.
    \newpage
    Graficamente:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.35]{img/pltl14.png}
      \includegraphics[scale = 0.354]{img/pltl15.png}
    \end{figure}
    \item le formule $\mathbf{F}\,\mathbf{G}\,\mathbf{F}\,A$ e
    $\mathbf{G}\,\mathbf{F}\, A$  sono \emph{equivalenti}.
    Si vuole dimostrare che:
    \[(M, t)\Vdash \mathbf{FGF}\,A \mbox{ se e solo se }(M, t)\Vdash \mathbf{G
        F}\.A\]
    Dalla definizione di $\mathbf{F}$ si ha che:
    \[\mbox{se } (M, t)\Vdash\mathbf{G F}\,A,\mbox{ allora }(M,
      t)\Vdash\mathbf{F GF}\,A\]
    inoltre, viceversa, si ha che:
    \[\mbox{se vale }(M, t)\Vdash\mathbf{F GF}\,A, \mbox{ allora vale } (M,
      t)\Vdash\mathbf{G F}\,A\]
    Inoltre, per il significato di $\mathbf{F}$ se vale $(M, t)\Vdash\mathbf{F
      GF}\,A$ allora $\exists\,t'\geq t$ tale che $(M,
    t')\Vdash\mathbf{GF}\,A$. Inoltre, per il significato di $\mathbf{G}$ si ha
    che: 
    \[\forall\,t''\geq t',\,(M,t'')\Vdash \mathbf{F}\,A\]
    Ne segue che, dato che il \emph{future} è persistente all'indietro:
    \[\forall\,t'''\geq t',\,(M,t''')\Vdash \mathbf{F}\,A\]
    quindi a partire da $t$ vale sempre $\mathbf{F}\,A$
    e quindi, per definizione di $\mathbf{G}$, segue che $(M,t)\Vdash
    \mathbf{GF}\,A$.
    \newpage
    Graficamente:
    \begin{figure}[H]
      \centering
      \includegraphics[scale = 0.47]{img/pltl16.png}
      \includegraphics[scale = 0.526]{img/pltl17.png}
    \end{figure}
  \end{itemize}
\end{definizione}
\subsection{Concetti e Proprietà in Logica PLTL}
Vediamo quindi diversi concetti esprimibili in logica PLTL.\\
Un primo concetto è quello di espriremere \textit{un evento che deve avvenire un
  numero infinito di volte, non necessariamente di seguito}. Si parla di
\textit{fairness}.\\
Data una proprietà $A$ si vuole che diventi vera infinitamente spesso, ovvero è
sempre vero che prima o poi $A$ diventi vera:
\[\mathbf{GF}\,A\]
\textit{Si nota che comunque non si previene che possa essere vero $\neg A$ in
  un certo momento}. Infatti si ha un esempio semplificato dove $A$ diventa una
semplice variabile proporzionale $p$ (che quindi è sicuramente soddisfacibile):
\begin{esempio}
  La formula $\mathbf{GF}\,p\land \mathbf{GF}\neg p$ è soddisfacibile anche se
  $p\land\neg p$ è una contraddizione.\\
  Si definisce quindi un modello $M=\langle\mathbb{N},\Vdash\rangle$ dove la
  relazione di soddisfacibilità per la variabile $p$ è così definita:
  \[\Vdash\,\subseteq\{(i, p) |\,\, i \bmod 2 == 0\}\]
  dove $p$ è vera negli stati/tempi $i$ pari e falsa in quelli dispari (dove è
  vero $\neg p$).\\
  Graficamente:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/pltl18.png}
  \end{figure}
  Si nota che ovunque in questa catena di stati si può affermare $\mathbf{F}\,p$
  e anche $\mathbf{F}\neg p$ in quanto al più basta aspettare uno stato per
  ottenere quella condizione. Inoltre essendo i due \emph{future} sempre
  soddisfatti posso anche affermare che sono soddisfatti i \emph{globally} dei
  due \emph{future} e quindi $\mathbf{GF}\,p\land \mathbf{GF}\neg p$ è
  soddisfatta ovunque.
\end{esempio}
Si è quindi esibito uno degli infiniti modelli per $\mathbf{GF}\,A$ (banalmente,
per esempio, si potevano usare i moduli di un qualsiasi altro numero diverso da
due etc$\ldots$).\\
È possibile anche complicare la situazione, usando per esempio due variabili:
\begin{esempio}
  Si ha che la formula:
  \[\mathbf{GF}(p\land q)\land\mathbf{GF}(p\land \neg q)\land \mathbf{GF}(\neg
    p\land q)\land \mathbf{GF}(\neg p\land \neg q)\]
  è soddisfacibile anche se ogni coppia do formule rappresenta una
  contraddizione.\\
  Per esempio definiamo un modello $M=\langle\mathbb{N},\Vdash\rangle$ dove la
  relazione di soddisfacibilità è definita come:
  \[\Vdash\, \subseteq\{(i,p)|\,\,i\bmod 2 == 0\}\cup \{(i,q)|\,\,i\bmod 4 ==
    0\mbox{ oppure }i\bmod 4 == 1\}\]
  \newpage
  Si hanno così i seguenti casi:
  \begin{itemize}
    \item $p\land q$ è soddisfatta nei tempi/stati multipli di 4 (0 incluso)
    \item $p\land \neg q$ è soddisfatta nei tempi che diviso 4 danno resto 2
    \item $\neg p\land q$ è soddisfatta nei tempi che diviso 4 danno resto 1
    \item $\neg p\land\neg q$ è soddisfatta nei tempi che diviso 4 danno resto 3
  \end{itemize}
  Graficamente:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.5]{img/pltl19.jpg}
  \end{figure}
  Quindi per esempio $p\land q$ è soddisfatta in 0 (e lo sarà in 4 etc$\ldots$).
  E visto che ogni coppia di formule è soddisfatta a stati alterni infinitamente
  si ha, in ogni stato, la validità del \emph{globally} e del \emph{future}.\\
  Si potrebbe anche dire che vale il \emph{globally} totale della formula
  iniziale. 
\end{esempio}
Un secondo concetto è quello di espriremere un \textit{evento che prima o poi
  diventa vero e, una volta diventato tale, resta vero per sempre}, ovvero, data
una proprietà $A$, si ha che:
\[\mathbf{FG}\,A\]
Si nota però fin da subito che la formula:
\[\mathbf{FG}\,p\land \mathbf{FG}\neg p\]
\textbf{non è soddisfacibile}, in quanto se assumo che da un certo tempo in poi
una formula è sempre vera non posso asserire anche che da un certo tempo in poi
quella formula sia falsa.\\
Procedendo in modo formale argomentiamo \textbf{per assurdo}.
Si supponga che esista un modello $M=\langle \mathbb{N}m\Vdash\rangle$ che
soddisfi la formula data. Mi serve quindi, al tempo 0:
\[(M,0)\Vdash \mathbf{FG}\,p\land \mathbf{FG}\neg p\to (M,0)\Vdash
  \mathbf{FG}\,p\land (M,0)\Vdash \mathbf{FG}\neg p\]
e so che, per il significato del connettivo \emph{future}:
\begin{itemize}
  \item da $\mathbf{FG}\,p$ segue che esiste un tempo $t'$ tale che
  $(M,t')\Vdash\mathbf{G}\,p$
  \item da $\mathbf{FG}\neg p$ segue che esiste un tempo $t''$ tale che
  $(M,t'')\Vdash\mathbf{G}\neg p$
\end{itemize}
ma, sia per $t'\geq t''$ che per $t'\leq t''$ si ha una contraddizione:
\begin{itemize}
  \item se $t'\geq t''$ allora $(M,t')\Vdash p$ e $(M,t')\Vdash \neg p$
  \item se $t'\leq t''$ allora $(M,t'')\Vdash p$ e $(M,t'')\Vdash \neg p$
\end{itemize}
che sono entrambe \textbf{contraddizioni} in quanto una formula non può essere
contemporaneamente sia vera che falsa in un determinato tempo.\\
\textit{Siccome nei modelli PLTL i tempi sono comparabili, segue che è assurda
  l’ipotesi che la formula data sia soddisfacibile in qualche modello PLTL}.\\
Si poteva arrivare al medesimo risultato studiando i tableau.\\\\
Vediamo un ulteriore concetto, quello in cui \textit{prima o poi un evento
  diventa vero e da quel punto è garantito che l'evento sia vero con
  cadenza/periodo pari a $n$}. Data quindi una formula $A$ si ha che:
\[\mathbf{F}\,A\land\mathbf{G}(A\to\underbrace{
    \mathbf{X}\ldots\mathbf{X}}_{n\mbox{ volte, } n\geq 1}\,A)\]
Quindi bisogna attendere al massimo $n$ stati perché $A$ diventi vera, dove $n$
è in numero di $\mathbf{X}$ innestati. Si garantisce la frequenza minima con cui
$A$ diventa vera.
\begin{esempio}
  Vediamo un esempio.\\
  Possiamo osservare la seguente formula \textbf{soddisfacibile}:
  \[\mathbf{F}\,p\land \mathbf{G}(p\to\mathbf{XX}\,p)\land\mathbf{F}\neg p\land
    \mathbf{G}(\neg p\to\mathbf{XX}\neg p)\]
  dove se $p$ diventa vera si ha che poi essa lo diventa a cadenza di due stati
  e, di conseguenza, anche $\negn p$ seguirà la stessa cadenza.\\ 
  Possiamo infatti definire, per esempio, un modello $M=\langle
  \mathbb{N},\Vdash\rangle$ con:
  \[\Vdash=\{(i,p)|\,i\bmod 2==0\}\]
  $p$ sarà vero nei tempi pari e $\neg p$ sarà vero nei tempi dispari.
  formalmente si ha quindi che:
  \begin{itemize}
    \item $\forall t\in\mathbb{N}$ (con $t$ stato) tale che $t\bmod 2 == 0$
    valgono:
    \[(M,t)\Vdash p \mbox{ e } (M,t)\Vdash\mathbf{XX}\,p\]
    \item $\forall t\in\mathbb{N}$ (con $t$ stato) tale che $t\bmod 2 == 1$
    valgono:
    \[(M,t)\Vdash \neg p \mbox{ e } (M,t)\Vdash\mathbf{XX}\neg p\]
    \item $\forall t\in\mathbb{N}$ (con $t$ stato) valgono:
    \[(M,t)\Vdash p \to \mathbf{XX}\,p \mbox{ e }(M,t)\Vdash \neg p\to
      \mathbf{XX}\neg p\]
    e quindi, sempre $\forall t\in\mathbb{N}$, valgono:
    \[(M,t)\Vdash \mathbf{G}(p\to\mathbf{XX}\,p)\mbox{ e
      }(M,t)\Vdash\mathbf{G}(p\to\mathbf{XX}\neg p)\]
  \end{itemize}
  \emph{Si nota quindi come la formula sia soddisfa nel modello} $M$.\\
  Graficamente, specificando completamente solo le formule dei tempi pari:
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{img/pltl20.jpg}
  \end{figure}
\end{esempio}
\begin{esempio}
  Vediamo ora la dimostrazione della \textbf{non soddisfacibilità} della
  formula:
  \[\mathbf{F}\,p\land \mathbf{G}(p\to \mathbf{X}\,p)\land \mathbf{F}\neg
    p\land \mathbf{}(\neg p\to \mathbf{X}\neg p)\]
  Si procede \textbf{per assurdo}. Si supponga di avere un modello $M=\langle
  mathbb{N},\Vdash\rangle$ in cui la formula è soddisfacibile. Si ha quindi che
  $\exists\,t\in\mathbb{N}$ (con $t$ tempo) che soddisfa la formula. Di
  conseguenza esistono due tempo, $t',t''\in \mathbb{N}$ tali che:
  \[t'\geq t,\,t''\geq t,\,(M,t')\Vdash p \mbox{ e }(M,t'')\Vdash \neg p\]
  ne segue che:
  \[t'\neq t'',\,(M,t')\Vdash\mathbf{X}\,p,\,(M,t'+1)\Vdash
    p,\,(M,t'')\Vdash\mathbf{X}\neg p\mbox{ e }(M,t''+1)\Vdash \neg p\]
  quindi $\exists\,t''\in\mathbb{N}$ tale che:
  \[t'''\geq t',\,t'''\geq t''\mbox{ e }(M,t''')\Vdash p\mbox{ e
    }(M,t''')\Vdash \neg p\]
  \emph{Questo è impossibile per un modello, quindi è un assurdo assumere che
    esista una modello per la formula data. Si è arrivati ad una
    \textbf{contraddizione}}.
\end{esempio}
\newpage
Vediamo un ultimo concetto come esempio, quello i cui \textit{prima o poi un
  evento diventa vero e da quel momento in poi, ciclicamente, diventa vero
  quell'evento insieme ad altri due eventi.} \\
Si ha quindi la formula, per esempio con periodo complessivo nove:
\[\mathbf{F}\,A\land \mathbf{G}(A\to \mathbf{XXX}\,B)\land \mathbf{G}(B\to
  \mathbf{XXX}\,C)\land \mathbf{G}(C\to\mathbf{XXX}\,A)\]
Viene quindi generato un \textbf{ciclo} (dopo che ho visto $A$ aspetto tre stati
per vedere $B$, a questo punto altri tre stati per vedere $C$ e infine, dopo
altri tre stati torno a vedere $A$).
\begin{esempio}
  Vediamo un esempio semplificato con periodo pari a due.\\
  Si ha la formula:
  \[\mathbf{F}\,p\land \mathbf{G}(p\to \mathbf{X}\neg p)\land\mathbf{G}(\neg
    p\to\mathbf{X}\,p)\]
  Questa formula è \textbf{soddisfacibile} ed afferma che prima o poi $p$
  diventa vera e da tal punto in poi $p$ diventerà vera con esattamente periodo
  due (ogni due stati). 
\end{esempio}
\begin{esempio}
  Vediamo un esempio più complesso del precedente, con due variabili
  proposizionale: 
  \[\mathbf{F}\,p\]
  \[\land\]
  \[\mathbf{G}(p\to(\neg q\land \mathbf{X}\neg(p\lor q)\land
    \mathbf{XX}\neg(p\lor q)\land \mathbf{XXX}\,q))\]
  \[\land\]
  \[\mathbf{G}(q\to (\neg p\land \mathbf{X}\neg(p\lor q)\land \mathbf{XX}\neg
    (p\lor q)\land\mathbf{XXX}\,p))\] 
  la quale afferma che prima poi $p$ è soddisfatta ed inoltre $p$ è soddisfatta
  infinitamente spesso con periodo 6, $q$ è soddisfatta infinitamente spesso con
  periodo 6, se $p$ è vera, allora bisogna attendere esattamente 3 stati
  affinchè $q$ diventi vera e se $q$ è vera, allora bisogna attendere
  esattamente 3 stati affinchè $p$ diventi vera. Un modello per la formula
  soddisfa $p$ nei tempi multipli di 6 e soddisfa $q$ nei tempi che diviso per 6
  danno resto 3 (quindi 3, 9, 15,$\ldots$). Anche in questo caso si ha un
  \textbf{ciclo} che parte nel momento in cui soddisfo $p$ la prima volta.
\end{esempio}
Nel file delle esercitazioni si trovano ulteriori esempi, oltre alla citazione
di testi con vari esempi:
\begin{itemize}
  \item contatore modulo quattro
  \item semaforo stradale
\end{itemize}
\chapter{Model Checking}






\chapter{Seminario sul Teorema di Incompletezza}
\textit{\textbf{Nota: queste note, prese durante il seminario del prof
    Bernardinello tenuto il 1 Aprile 2020 sono incomplete, parziali e
    probabilmente scorrette in determinati punti.}}\\\\
Si parla del \textbf{teorema di incompletezza di G\"{o}del}. Come punto di
partenza \textit{storico} si parla di Hilbert, che pensava alla matematica come
un sistema puramente formale, introducendo i concetti di \textbf{coerenza e
  completezza della matematica}. Con coerenza si intende che non si possono
dimostrare contraddizioni mentre con completezza si indica che si può dimostrare
la correttezza o meno di un enunciato. Ci si basa anche sull'\textbf{aritmetica
  di Peano}, ovvero un insieme di assiomi da cui si derivano tutte le proprietà
dei numeri. \\
G\"{o}del cerca di dimostrare coerenza e completezza dell'aritmetica.
\begin{definizione}
  Un \textbf{sistema formale} è definito da un linguaggio in cui si possono
  scrivere \textbf{formule e assiomi}, si possono definire \textbf{regole di
    inferenza} che portano all'ottenimento di \textbf{teoremi
    (\textit{enunciati})}. Un enunciato interpretato in un modello può essere
  vero o falso
\end{definizione}
\begin{definizione}
  Un sistema formale $S$ espresso nel linguaggio $L$ è \textbf{sintatticamente
    coerente} se non esiste una formula $\alpha$ tale che siano dimostrabili sia
  $\alpha$ che $\neg \alpha$.\\
  Un sistema formale $S$ espresso nel linguaggio $L$ è \textbf{semanticamente
    coerente (correttezza)} se non è in grado di dimostrare nessuna formula che
  sia falsa.
\end{definizione}
\begin{definizione}
  Un sistema formale $S$ espresso nel linguaggio $L$ è \textbf{sintatticamente
    completo} se data una formula $\alpha$ o è dimostrabile $\alpha$ o
  $\neg\alpha$.\\
  Un sistema formale $S$ espresso nel linguaggio $L$ è \textbf{semanticamente
    completo} se è in grado di dimostrare tutte le formule vere.
\end{definizione}
Il linguaggio naturale permette l'\textbf{autoriferimento} delle frasi, quando
una frase predica qualcosa di se stessa, comportando spesso contraddizioni.
\begin{esempio}
  per esempio:
  \begin{center}
    \textit{questa frase si compone di sette parole}
    \\o\\
    \textit{questa frase è falsa}
  \end{center}
\end{esempio}
Una soluzione a questo problema classificando questo tipo di frasi in:
\begin{itemize}
  \item \textbf{vere}
  \item \textbf{false}
  \item \textbf{né vere né false} (come il secondo esempio)
\end{itemize}
ma una frase come:
\begin{center}
  \textit{questa frase non è vera}
\end{center}
rompe questa divisione tripartita. \\
I teoremi di G\"{o}del parlano di sistemi formali.
\begin{definizione}
  Un sistema formale è definito da:
  \begin{itemize}
    \item un alfabeto ($=,\land,\lor, \neg \to, \exists, \leftrightarrow,
    \forall, x_i, (,),0,',+,\times$, l'apice specifica il successore)
    \item un linguaggio con formule, enunciati e variabili libere ($a[x],
    a[x/n]$, dove $x$ è variabile libera e $x/n$ indica che tutte le variabili
    $x$ sono state sostituite con $n$) 
    \item degli assiomi
    \item delle regole di inferenza per costruire nuovi enunciati a partire da
    altri enunciati
  \end{itemize}
  Si ha quindi una grammatica per costruire e riconoscere frasi corrette.\\
  Si ha che una \textbf{dimostrazione} è una sequenza di formule derivate a
  vicenda con delle regole di inferenza.
  \begin{esempio}
    posso riscrivere, per esempio, il principio di induzione:
    \[\alpha[x/0]\to(\forall x(\alpha[x]\to \alpha[x'])\to \forall x\alpha[x])\]
  \end{esempio}  
\end{definizione}
\newpage
G\"{o}del si chiede se è possibile dimostrare se la correttezza e la completezza
dell'aritmetica.
\begin{teorema}
  Dato un enunciato $G$ nel sistema formale $S$ che, interpretato dice:
  \begin{center}
    G non è dimostrabile in S
  \end{center}
  allora:
  \begin{itemize}
    \item se $G$ fosse dimostrabile in $S$ sarebbe \textbf{semanticamente
      incoerente} 
    \item se $G$ non fosse dimostrabile in $S$ sarebbe un \textbf{enunciato
      vero} e $S$ sarebbe \textbf{semanticamente incompleto}
  \end{itemize}
  G\"{o}del trovò nell'aritmetica un enunciato di questo tipo
\end{teorema}
Dato che una formula dell'aritmetica parla di numeri si cerca un modo per
associare alle formule del sistema formale dell'aritmetica un numero. Si parte
associando un numero ad ogni simbolo dell'alfabeto del sistema (per esempio,
associando solo numeri dispari $g(0)=3,\,g(')=5,\,g(x)=31,\ldots$). Un alfabeto
è infinitamente numerabile. G\"{o}del asserisce che:
\begin{definizione}
  Una formula $\alpha$ è una sequenza di simboli $k_1,\ldots,k_M$, allora:
  \[g(\alpha)=2^{g(k_1)}3^{g(k_2)}5^{g(k_3)}\ldots\]
  ottenendo quindi numeri pari associati delle formule (grazie al
  $2^x$ iniziale, inoltre tutti gli esponenti sono dispari).\\
  Una dimostrazione $\Delta$ è una sequenza di formule
  $\alpha_1,\ldots,\alpha_k$, allora si ha che:
  \[g(\Delta)=2^{g(\alpha_1)}3^{g(\alpha_2)}5^{g(\alpha_3)}\ldots\]
  che è iniettiva.\\
  Il numero, detto \textbf{numero di G\"{o}del} sarà sempre diverso da quello di
  una formula in quanto ho esponenti pari
\end{definizione}
Una formula $\alpha[x,y]$ con due variabili libere corrisponde ad una relazione
binaria $R(x,y)$ sul dominio in cui interpretiamo il linguaggio e si ha:
\[R(m,n)\to\alpha[x/m,y/n]\]
Se ho una formula con una sola variabile libera $\alpha[x]$ corrisponde a un
predicato $P(x)$ sul dominio:
\[P(m)\leftrightarrow \vdash\alpha[x/m]\]
e in questo caso $\alpha[x]$ determina un sottoinsieme del dominio
\begin{definizione}
  Preso un enunciato $F$ con una dimostrazione $\Delta=F_1,\ldots,F$ e presi i
  numeri di G\"{o}del di $\Delta$ e $F$, considerandoli come coppia ordinata di
  numeri si può asserire che l'insieme delle coppie formate in questo modo
  prendendo tutte le dimostrazioni in $S$ definisce una relazione binaria sul
  dominio di $\mathbb{N}$ $Dim$.\\
  La relazione $Dim$ è rappresenta in $S$ da una formula con due variabili
  libere.\\
  Si ha che $Dim$ è \textbf{decidibile} in $S$
\end{definizione}
Nell'aritmetica di Peano esiste una formula decidibile $Dim(x,y)$ tale che
$\vdash Dim(m,n)$ sse $M$ è il numero di G\"{o}del di una dimostrazione della
formula il cui numero di G\"{o}del è $n$.\\
Si costruisce quindi una formula:
\[\exists x(Dim(x,y))\]
che dice che $y$ è un teorema (detto altrimenti che $y$ è dimostrabile) e si
indica con:
\[Teor(y)=\exists x(Dim(x,y))\]
Si ha che $Teor(y)$ \textbf{non è decidibile}.
\begin{definizione}
  Si ha la definizione del \textbf{Lemma Diagonale}.\\
  Sia $\alpha[x]$ una formula con una sola variabile libera, allora esiste un
  enunciato $\beta$ tale che:
  \[\vdash\beta\leftrightarrow\alpha[x/g(\beta)]\]
  ovvero il numero associato a $\beta$ ha la proprietà $\alpha$, detto
  altrimenti $\beta$ afferma di avere la proprietà $\alpha$.\\
  
  In aritmetica quindi esistono formule che dicono di se stesse di non essere
  dimostrabili quindi l'\textbf{aritmetica è incompleta}
\end{definizione}
Diamo ora i 2 \textbf{teoremi di G\"{o}del}:
\begin{teorema}
  Se $S$ è un sistema formale corretto in grado di esprimere una certa porzione
  di aritmetica allora esiste un enunciato $G_S$, formulato nel linguaggio del
  sistema formale, tale che $G_S$ è \textbf{indecidibile} in $S$, ossia non è né
  \textbf{dimostrabile} né \textbf{inconfutabile} 
\end{teorema}
\begin{teorema}
  Se $S$ è un sistema formale \textbf{corretto} in grado di esprimere una certa
  porzione di aritmetica allora $S$ non può provare la propria coerenza  
\end{teorema}
\end{document}

% LocalWords:  postcondizioni LocalWords estensionalità jpg img sse pre sem seq
% LocalWords:  side condition prod interleaving step semantics Petri deadlock
% LocalWords:  Property diamond property sottografo diam postcondizione seqq pt
% LocalWords:  conc conf backward confu mes Smith Einar Sottoreti sottorete upD
% LocalWords:  sinc all'handshacking all'handshacking handshaking asinc primis
% LocalWords:  Dijkstra starvation upS downD downS fork Phil mod sima multiset
% LocalWords:  concorrentemente self abbastnza safe simil ptc Mayr bounded live
% LocalWords:  overflow reversible dead Safety Liveness Fairness safety well WF
% LocalWords:  liveness fairness safeness formed as forward Parikh firing PLTL
% LocalWords:  affinchè sottoformula insoddisfacibile Morgan Idempotenza Linear
% LocalWords:  all'handshacking sottoformule Propositional Logic checking next
% LocalWords:  tomorrow sometime globally always until true bottom label BNF
% LocalWords:  Form root soddisfacibilità pltl png slide Godel Peano sull cap
% LocalWords:  autoriferimento iniettiva G\"{o}del Bernardinello espriremere
% LocalWords:  tableau github ciclano

\message{ !name(metodi.tex) !offset(-4381) }
