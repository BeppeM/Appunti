\chapter{Regressione Lineare}
\begin{comment}
In molte situazioni c'è una variabile $Y$, chiamata \emph{variabili dipendente} da un insieme di valori di input, chiamati \emph{indipendenti}, $X_1, X_2, \dots, X_r$.\newline
La più semplice relazione tra la variabile dipendente $Y$ e le variabili di input $X_1, X_2, \dots, X_r$ è la relazione lineare, in cui per alcune costanti $\beta_0, \dots, \beta_r$ si ha
\[ Y = \beta_0 + \beta_1x_1 + \dots + \beta_r x_r \]
Se esite una relazione tra $Y$ e $X_i$ allora dovrebbe essere possibile predirre esattamente la risposta per ogni insieme di valori ma in pratica comunque ogni previsione non è sempre
attendibile per cui la relazione più corretta per rappresentare una relazione lineare consiste nell'equazione
\[ Y = \beta_0 + \beta_1x_1 + \dots + \beta_rx_r + e \]
dove $e$ indica l'errore, assunto con media $0$ e varianza $\sigma^2$.\newline
Un altro modo di rappresentare l'equazione precedente è la seguente
\[ E[Y | x] = \beta_0 + \beta_1x_1 + \dots + \beta_rx_r \]
dove $x = (x_1, x_2, \dots, x_r)$ è l'insieme delle variabili indipendenti e $E[Y|x]$ è la risposta attesa per l'input $x$.\newline
L'equazione presentata si chiama \emph{equazione di regressione lineare} e il vettore $(\beta_0, \beta_1, \dots, \beta_r)$ è il vettore dei coefficienti di regressione che devono 
essere stimati dall'insieme dei dati di input.\newline
In caso di $r = 1$ si ha la presenza di una \emph{regressione singola} altrimenti si ha una \emph{regressione multipla} e noi studiamo principalmente la regressione singola in quanto
è molto utile dal punto di vista didattico.\newline
Un modello di regressione singola suppone una relazione lineare tra la media e il valore della variabile indipendente singola che può essere espressa come 
\[ Y = \alpha + \beta x + e \]
dove $X$ è il valore della variabile indipendente, $e$ è l'errore commesso nella stima mentre $Y$ è la risposta data in risposta a $X$.

\begin{prop}
Lo stimatore least squared di $\alpha$ e $\beta$, corrispondente all'insieme dei dati $(x_i, Y_i i = 1 \dots n$, sono rispettivamente
\[ A = \mean{Y} - B \mean{X} \]
\[ B = \frac{\sum _{i = 1}^n x_iY_i - \mean{x}\sum _{i = 1}^n Y_i}{\sum x_i^2 - n \mean{x}^2} \]
$A + Bx$ viene chiamata la linea di regressione stimata
\end{prop}
\begin{proof}
Supponiamo che la risposta $Y_i$ corrispondente ai valori d'input $x_i i = 1 \dots n$ viene osservata e usata per stimare $\alpha$ e $\beta$ in un modello 
di regressione lineare singola in cui se $A$ è lo stimatore per $\alpha$ e $B$ è lo stimatore di $\beta$ allora lo stimatore per la risposta corrispondente 
alla variabile $x_i$ dovrebbe essere $A + Bx_i$.\newline
Dato che la risposta attuale è $Y_i$, la differenza quadratica è $(Y_i - A - Bx_i)^2$ e se $A$ e $B$ sono gli stimatori di $\alpha$ e $\beta$ allora per stimare
le risposte e la risposta attuale usiamo la statistica
\[ SS = \sum _{i = 1}^n (Y_i - A - Bx_i)^2 \]
il metodo dei residui quadratici prevedi di usare come stimatori di $\alpha$ e $\beta$, i valori A e B che minimizzano la funzione SS e per determinare questi stimatori 
differenziamo SS come segue
\[ \frac{\partial SS}{\partial A} = -2 \sum _{i = 1}^n (Y_i - A - Bx_i) \]
\[ \frac{\partial SS}{\partial B} = -2 \sum _{i = 1}^n x_i(Y_i - A - Bx_i) \]
Ponendo queste derivate parziali uguali a 0 otteniamo le seguenti equazioni, per minimizzare i valori di A e B
\[ \sum _{i = 1}^n Y_i = nA + B \sum x_i \]
\[ \sum ^n x_iY_i = A \sum^n x_i + B \sum^n x_i^2 \]
Ponendo $\mean{Y}$ come media campionaria dei valori di risposta $Y_i$ e $\mean{x]$ come media campionaria dei valori di input $x_i$ otteniamo $A = \mean{Y} - B \mean{x}$.\newline
Sostituendo questo valore di A nella seconda equazione otteniamo 
\[ B = \frac{\sum x_iY_i - \mean{x}\sum^n Y_i}{\sum^n x_i^2 - n\mean{x}^2} \]
Usando le equazioni appena trovate di A e B e sfruttando il fatto che $n \mean{Y} = \sum^n Y_i$ si dimostra la proposizione.
\end{proof}
\end{comment}
